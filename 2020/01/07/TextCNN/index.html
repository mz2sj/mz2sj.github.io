<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/0JOFhyF5bT.jpeg">
  <link rel="icon" type="image/png" href="/img/0JOFhyF5bT.jpeg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="">
  <meta name="author" content="mz2sj">
  <meta name="keywords" content="">
  <title>TextCNN ~ 微风和暖</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>微风和暖</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/default.png')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期二, 一月 7日 2020, 7:50 晚上
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    2.5k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      11 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>学了点pytorch，最近在知乎刷到了一个repo主要是介绍文本分类的，感觉还不错，打算用来入门，自己将会比较细致的实现代码，分析代码的结构。天下大事必做于易，天下大事必做于细。寒假来了，自己基础较差，只能多花点时间了。</p>
<h2 id="基础网络"><a class="markdownIt-Anchor" href="#基础网络"></a> 基础网络</h2>
<p>作者自己没有直接调用torch的api，而是在其基础上进行实现。一些模块继承自<code>nn.Module</code>层，并且实现了初始化和前向计算。嘿呀，等我熟练了，我也要自己做一个工具箱。不多说上代码</p>
<h3 id="conv1d"><a class="markdownIt-Anchor" href="#conv1d"></a> Conv1d</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch.nn.Functional <span class="hljs-keyword">as</span> F

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Conv1d</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,in_channels,out_channels,filter_sizes)</span>:</span>
        super(Conv1d,self).__init__()
        self.convs=nn.ModuleList([
            nn.Conv1d(in_channels,out_channels,kernel_size=fs)
            <span class="hljs-keyword">for</span> fs <span class="hljs-keyword">in</span> filter_sizes
        ])
        self.init_params()
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_params</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.convs:
            nn.init.xavier_uniform_(m.weight.data)
            nn.init.constant_(m.bias.data,<span class="hljs-number">0.1</span>)
            
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
        <span class="hljs-keyword">return</span> [F.relu(conv(x))<span class="hljs-keyword">for</span> conv <span class="hljs-keyword">in</span> self.convs]
</code></pre>
<p>让我们来细细品味这段代码，继承自 <code>nn.Module</code> 这不用说了，表明这个模块可以和torch自带的模块混用。<code>__init__</code>函数的参数有讲究，除了必传的<code>self</code>外，还传入了一些指定卷积层形状的参数，例如通道数：<code>in_channels、out_channels</code>,还有卷积核集合。这里传入的是卷积核集合，而不是单单一个卷积核需要注意。接下来是继承自父类的初始化，<code>super()</code>第一个参数一般为函数名，第二个是self,后面再接<code>__init__()</code>。在<code>__init__()</code>函数中放置要用到的模块或参数等，不涉及到具体的操作，在初始化层调用了后面定义的参数 初始化函数。</p>
<p>第二部分是初始化层，<code>init_params()</code>。初始化层比较好理解，初始化<code>init</code>层的参数就好。前面要初始化的只有<code>ModuleList</code>中的卷积层，卷积的函数改天专门总结一下。卷积层参数分为weight和bias两部分,bias部分初始化为0。</p>
<p>第三部分是前向传播，<code>forward()</code>。参数有输入x，由于有多个卷积核，将多个卷积核的卷积结果放到列表中。再调用<code>F.relu()</code>activation。不得不说<code>torch.nn.Functional</code> 超好用。</p>
<h2 id="linear"><a class="markdownIt-Anchor" href="#linear"></a> Linear</h2>
<p>Linear层和Conv1d结构上差不多，不过Linear层的初始化参数将卷积层的channel换成了feature，并且少了卷积核。代码如下：</p>
<pre class="highlight"><code class="python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,in_features,out_features)</span>:</span>
        super(Linear,self).__init__()
        self.linear=nn.Linear(in_features=in_features,out_features=out_features)
        self.init_params()
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_parmas</span><span class="hljs-params">(self)</span>:</span>
        nn.init.kaiming_normal_(self.linear.weight)
        nn.init.constant_(self,linear.bias,<span class="hljs-number">0</span>)
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
        x=self.linear(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<h2 id="textcnn"><a class="markdownIt-Anchor" href="#textcnn"></a> TextCNN</h2>
<p>到了最最关键的环节了，TextCNN层:</p>
<pre class="highlight"><code class="python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TextCNN</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,embedding_dim,n_filters,filter_sizes,output_dim,dropout,
                 pretrained_embeddings)</span>:</span>
        super(TextCNN,self).__init__()
        <span class="hljs-comment">#导入预训练的embedding frezee指定权重是否更新</span>
        self.embedding=nn.Embedding.from_pretrained(pretrained_embeddings,freeze=<span class="hljs-literal">False</span>)
        <span class="hljs-comment">#卷积核参数 embedding维数，卷积核个数，卷积核大小</span>
        self.convs=Cov1d(embedding_dim,n_filters,filter_sizes)
        <span class="hljs-comment">#前向运算 输入维度 预测输出</span>
        self.fc=Linear(len(filter_sizes)*fliters,output_dim)
        self.dropout=nn.Dropout(dropout)
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
        <span class="hljs-comment">#text:[sent_len,batch_size] 两个参数分别是行数列数</span>
        text,_=x
        <span class="hljs-comment">#维度换位 [batch_size,sent_len]</span>
        text=text.permute(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)
        <span class="hljs-comment"># [batch_size,sent_len,embedding_len]</span>
        embedded=self.embedding(text)
        <span class="hljs-comment">#[batch_size,emb_dim,sent_len]</span>
        embedded=embedded.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
        conved=self.convs(embedded)
        <span class="hljs-comment">##[batch_size,n_filters,sent_len-filter_sizes[n]+1]</span>
        pooled=[F.max_pool1d(conv,conv.shape[<span class="hljs-number">2</span>])
               <span class="hljs-keyword">for</span> conv <span class="hljs-keyword">in</span> conved]
        cat=self.dropout(torch.cat(pooled,dim=<span class="hljs-number">1</span>))
        cat=cat.reshae((cat.shape[<span class="hljs-number">0</span>],<span class="hljs-number">-1</span>))
        <span class="hljs-keyword">return</span> self.fc(cat)
</code></pre>
<p>TextCNN层中的张量维度变换比较复杂，我是一下子反应不过来的，后面自己会debug彻底弄清楚每一步是怎么卷积的，维度怎么变换的。</p>
<h2 id="argparse"><a class="markdownIt-Anchor" href="#argparse"></a> argparse</h2>
<p>代码还有一个比较值得借鉴的点就是使用了argparse模块对各种参数进行管理，不是一个个手动去定义变量，更加清晰。</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> argparse

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_args</span><span class="hljs-params">(data_dir, cache_dir, embedding_folder, model_dir, log_dir)</span>:</span>
    parser = argparse.ArgumentParser(description=<span class="hljs-string">'SST'</span>)

    <span class="hljs-comment"># data_util</span>
    parser.add_argument(<span class="hljs-string">'--model_name'</span>, default=<span class="hljs-string">'TextCNN'</span>, type=str, help=<span class="hljs-string">'参数所属模型名'</span>)
    parser.add_argument(<span class="hljs-string">'--seed'</span>, default=<span class="hljs-number">47</span>, type=int, help=<span class="hljs-string">'随机种子'</span>)
    parser.add_argument(<span class="hljs-string">'--data_path'</span>, default=data_dir, type=str, help=<span class="hljs-string">'SST2数据集位置'</span>)
    parser.add_argument(<span class="hljs-string">'--cache_path'</span>, default=cache_dir, type=str, help=<span class="hljs-string">'数据集地址'</span>)
    parser.add_argument(<span class="hljs-string">'--sequence_length'</span>, default=<span class="hljs-number">60</span>, type=int, help=<span class="hljs-string">'句子长度'</span>)

    <span class="hljs-comment"># 输出文件名</span>
    parser.add_argument(<span class="hljs-string">'--model_dir'</span>, default=model_dir + <span class="hljs-string">'TextCNN/'</span>, type=str, help=<span class="hljs-string">'输出模型地址'</span>)
    parser.add_argument(<span class="hljs-string">'--log_dir'</span>, default=log_dir + <span class="hljs-string">'TextCNN/'</span>, type=str, help=<span class="hljs-string">'日志文件地址'</span>)
    parser.add_argument(<span class="hljs-string">'--do_train'</span>, action=<span class="hljs-string">'store_true'</span>, help=<span class="hljs-string">'Whether to run training'</span>)
    parser.add_argument(<span class="hljs-string">'--print_step'</span>, default=<span class="hljs-number">100</span>, type=int, help=<span class="hljs-string">'多少步存储一次模型'</span>)

    <span class="hljs-comment"># 优化参数</span>
    parser.add_argument(<span class="hljs-string">'--batch_size'</span>, default=<span class="hljs-number">64</span>, type=int)
    parser.add_argument(<span class="hljs-string">'--epoch_num'</span>, default=<span class="hljs-number">5</span>, type=int)
    parser.add_argument(<span class="hljs-string">'--dropout'</span>, default=<span class="hljs-number">0.4</span>, type=float)

    <span class="hljs-comment"># 模型参数</span>
    parser.add_argument(<span class="hljs-string">'--output_dim'</span>, default=<span class="hljs-number">2</span>, type=int)

    <span class="hljs-comment"># TextCNN参数</span>
    parser.add_argument(<span class="hljs-string">'--filter_num'</span>, default=<span class="hljs-number">200</span>, type=int, help=<span class="hljs-string">'filter数量'</span>)
    parser.add_argument(<span class="hljs-string">'--filter_sizes'</span>, default=<span class="hljs-string">'1 2 3 4 5'</span>, type=str, help=<span class="hljs-string">'filter的size'</span>)

    <span class="hljs-comment"># word embdding</span>
    parser.add_argument(<span class="hljs-string">'--glove_word_file'</span>, default=embedding_folder + <span class="hljs-string">'glove.6B.50d.txt'</span>,
                        type=str, help=<span class="hljs-string">'path of embedding file'</span>)
    parser.add_argument(<span class="hljs-string">'--glove_words_size'</span>, default=int(<span class="hljs-number">2.2e6</span>), type=int, help=<span class="hljs-string">'Corpus size for Glove'</span>)
    parser.add_argument(<span class="hljs-string">'--glove_word_dim'</span>, default=<span class="hljs-number">50</span>, type=int, help=<span class="hljs-string">'word embedding size(default:300)'</span>)
    config = parser.parse_args([])
    <span class="hljs-keyword">return</span> config
</code></pre>
<p>注意的一点就是parse_args()在notebook中需要加入[]作为参数，否则会报错。</p>
<h2 id="device"><a class="markdownIt-Anchor" href="#device"></a> device</h2>
<p>指定设备device，这个很容易理解</p>
<pre class="highlight"><code class="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_device</span><span class="hljs-params">()</span>:</span>
    device=torch.device(<span class="hljs-string">'cuda:0'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)
    n_gpu=torch.cuda.device_count()
    <span class="hljs-keyword">if</span> torch.cuda.is_available():
        print(<span class="hljs-string">'device is cuda,# cuda is:'</span>,n_gpu)
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">'device is cpu,not recommend'</span>)
    <span class="hljs-keyword">return</span> device,n_gpu
</code></pre>
<p>统计gpu数代码：<code>torch.cuda.device_count()</code></p>
<p>指定device代码：<code>torch.device('cuda:0')</code></p>
<h2 id="加载数据集"><a class="markdownIt-Anchor" href="#加载数据集"></a> 加载数据集</h2>
<p>对api还不太熟悉，这里也比较复杂。</p>
<pre class="highlight"><code class="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_sst2</span><span class="hljs-params">(path,text_field,label_field,batch_size,device,embedding_file,cache_dir)</span>:</span>
    train,dev,test=data.TabularDataset.splits(
    	path=path,train=<span class="hljs-string">'train.tsv'</span>,validationj=<span class="hljs-string">'dev.tsv'</span>,
        test=<span class="hljs-string">'test.tsv'</span>,format=<span class="hljs-string">'tsv'</span>,skip_header=<span class="hljs-literal">True</span>,
        fields=[(<span class="hljs-string">'text'</span>,text_field),(<span class="hljs-string">'label'</span>,label_field)]
    )
    print(<span class="hljs-string">'the size of train:{},dev:{},test:{}'</span>.format(
        len(train.examples), len(dev.examples), len(test.examples)
    ))
    <span class="hljs-comment">#将词向量放到缓存目录中，下次直接从换从目录中加载</span>
    vectors=vocab.Vectors(embedding_file,cache_dir)
    
    <span class="hljs-comment">#使用之前构建的词向量映射文本序列，将子序列转化为数字序列</span>
    text_field.build_vocab(
    	train,dev,test,max_size=<span class="hljs-number">25000</span>,
        vectors=vectors,unk_init=torch.Tensor.normal_
    )
    label_field.build_vocab(train,dev,test)
    
    train_iter,dev_iter,test_iter=data.BucketIterator.splits(
    	(train,dev,test),batch_sizes=(batch_size,len(dev),len(test)),
        sort_key=<span class="hljs-keyword">lambda</span> x:len(x.text),sort_within_batch=<span class="hljs-literal">True</span>,repeat=<span class="hljs-literal">False</span>,
        shuffle=<span class="hljs-literal">True</span>,device=device
    )
    <span class="hljs-keyword">return</span> train_iter,dev_iter,test_iter
</code></pre>
<p>得总结一下套路，看起来懵懵的</p>
<h2 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h2>
<pre class="highlight"><code class="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(epoch_num,model,train_dataloader,dev_dataloader,optimizer,criterion,
          label_list,out_model_file,log_dir,print_step,data_type=<span class="hljs-string">'word'</span>)</span>:</span>
    <span class="hljs-comment">#首先转为训练模式</span>
    model.train()
    writer=SummaryWriter(log_dir=log_dir+time.strftime(<span class="hljs-string">'%H%M%S'</span>,time.gmtime()))

    global_step=<span class="hljs-number">0</span>
    best_dev_loss=float(<span class="hljs-string">'inf'</span>)
    
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(int(epoch_num)):
        print(<span class="hljs-string">f'--------------Epoch:<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>:<span class="hljs-number">02</span>}</span>--------------'</span>)
        
        epoch_loss=<span class="hljs-number">0</span>
        train_step=<span class="hljs-number">0</span>
        
        all_preds=np.array([],dtype=int)
        all_labels=np.array([],dtype=int)
        
        <span class="hljs-keyword">for</span> step,batch <span class="hljs-keyword">in</span> enumerate(tqdm(train_dataloader,desc=<span class="hljs-string">'Iteration'</span>)):
            <span class="hljs-comment">#首先梯度归零</span>
            optimzier.zero_grad()
            logits=model(batch.text)
            
            <span class="hljs-comment">#计算损失</span>
            loss=criterion(logits.view(<span class="hljs-number">-1</span>,len(label_list)),batch.label)
            labels=batch.label.detach().cpu().numpy()
            preds=np.argmax(logits.detach().cpu().numpy(),axis=<span class="hljs-number">1</span>)
            
            <span class="hljs-comment">#计算梯度</span>
            loss.backward()
            optimizer.step()
            global_step+=<span class="hljs-number">1</span>
            
            epoch_loss+=loss.item()
            train_steps+=<span class="hljs-number">1</span>
            
            all_preds=np.append(all_preds,preds)
            all_labels=np.append(all_labels,labels)
            
            <span class="hljs-comment">#打印</span>
            <span class="hljs-keyword">if</span> global_step%print_step==<span class="hljs-number">0</span>:
                train_loss=epoch_loss/train_steps
                train_acc,train_report=classification_metric(
                    all_preds,all_labels,label_list)
                dev_loss,dev_acc,dev_report=evaluate(
                    model,dev_dataloader,criterion,label_list,data_type
                )
                c=global_step/print_step

                writer.add_scalar(<span class="hljs-string">'loss/train'</span>,train_loss,c)
                writer.add_scalar(<span class="hljs-string">'loss/dev'</span>,dev_loss,c)

                writer.add_scalar(<span class="hljs-string">'acc/train'</span>,train_acc,c)
                writer.add_scalar(<span class="hljs-string">'acc/dev'</span>,dev_acc,c)

                <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> label_list:
                    writer.add_scalar(label+<span class="hljs-string">':'</span>+<span class="hljs-string">'f1/train'</span>,
                                train_report[label][<span class="hljs-string">'f1-score'</span>],c)
                    writer.add_scalar(label+<span class="hljs-string">':'</span>+<span class="hljs-string">'f1/dev'</span>,
                                dev_report[label][<span class="hljs-string">'f1-score'</span>],c)
                print_list=[<span class="hljs-string">'macro avg'</span>,<span class="hljs-string">'weighted avg'</span>]
                <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> print_list:
                    writer.add_scalar(label+<span class="hljs-string">':'</span>+<span class="hljs-string">'f1/train'</span>,
                                train_report[label][<span class="hljs-string">'f1-score'</span>],c)
                    writer.add_scalar(label+<span class="hljs-string">':'</span>+<span class="hljs-string">'f1/dev'</span>,
                                dev_report[label][<span class="hljs-string">'f1-score'</span>],c)
                <span class="hljs-keyword">if</span> dev_loss&lt;best_dev_loss:
                    best_dev_loss=dev_loss
                    torch.save(model.state_dict(),out_model_file)
                model.train()
    writer.close()
</code></pre>
<p>训练函数有几个点总结，传入参数有：迭代次数、model、训练集、验证集，optimzer、损失函数、打印的epoch数，等等。</p>
<h2 id="evaluate"><a class="markdownIt-Anchor" href="#evaluate"></a> evaluate</h2>
<p>用于在训练过程中对训练结果进行测试</p>
<pre class="highlight"><code class="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(model,iterator,criterion,label_list,data_type=<span class="hljs-string">'word'</span>)</span>:</span>
    model.eval()
    epoch_loss=<span class="hljs-number">0</span>
    all_preds=np.array([],dtype=int)
    all_labels=np.array([],dtype=int)

    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> iterator:
            <span class="hljs-keyword">if</span> data_type==<span class="hljs-string">'word'</span>:
                <span class="hljs-keyword">with</span> torch.no_grad():
                    logits=model(batch.text)
            <span class="hljs-keyword">elif</span> data_type==<span class="hljs-string">'highway'</span>:
                <span class="hljs-keyword">with</span> torch.no_grad():
                    logits=model(batch.text_word,batch.text_char)

            loss=criterion(logits.view(<span class="hljs-number">-1</span>,len(label_list)),batch.label)

            labels=batch.label.detach().cpu().numpy()
            preds=np.argmax(logits.detach().cpu().numpy(),axis=<span class="hljs-number">1</span>)

            all_preds=np.append(all_preds,preds)
            all_labels=np.append(all_labels,labels)
            epoch_loss+=loss.item()
    acc,report=classification_metric(all_preds,all_labels,label_list)
    <span class="hljs-keyword">return</span> epoch_loss/len(iterator),acc,report
</code></pre>
<p>测试过程中记得要切换到eval模式，并且关闭梯度追踪<code>with torch.no_grad()</code>。</p>
<h2 id="main函数"><a class="markdownIt-Anchor" href="#main函数"></a> main函数</h2>
<pre class="highlight"><code class="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">(config)</span>:</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(config.model_dir):
        os.makedirs(config.model_dir)

    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(config.log_dir):
        os.makedirs(config.log_dir)

    print(<span class="hljs-string">'\t \t \t the model name is {}'</span>.format(config.model_name))
    device, n_gpu = get_device()

    <span class="hljs-comment"># 指定torch 随机种子</span>
    torch.manual_seed(config.seed)
    <span class="hljs-comment"># 指定numpy 随机种子</span>
    np.random.seed(config.seed)
    torch.manual_seed(config.seed)
    <span class="hljs-keyword">if</span> n_gpu &gt; <span class="hljs-number">0</span>:
        <span class="hljs-comment"># 指定cuda随机种子</span>
        torch.cuda.manual_seed_all(config.seed)
        <span class="hljs-comment"># CuDNN的卷积操作就是每次一样,实验可重复</span>
        torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span>

    <span class="hljs-comment"># sst2 数据准备</span>
    text_field = data.Field(tokenize=<span class="hljs-string">'toktok'</span>, lower=<span class="hljs-literal">True</span>, include_lengths=<span class="hljs-literal">True</span>, fix_length=config.sequence_length)
    label_field = data.LabelField(dtype=torch.long)

    train_iterator, dev_iterator, test_iterator = load_sst2(config.data_path, text_field, label_field,
                                                            config.batch_size,
                                                            device, config.glove_word_file, config.cache_path)

    <span class="hljs-comment"># 词向量准备 获取词向量按index顺序排序</span>
    pretrained_embeddings = text_field.vocab.vectors
    model_file = config.model_dir + <span class="hljs-string">'model1.pt'</span>

    <span class="hljs-comment"># 模型准备 卷积核大小</span>
    filter_sizes = [int(val) <span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> config.filter_sizes.split()]
    model = TextCNN(config.glove_word_dim, config.filter_num, filter_sizes,
                    config.output_dim, config.dropout, pretrained_embeddings)
    optimizer = torch.optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()

    model = model.to(device)
    criterion = criterion.to(device)

    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> config.do_train:
        train(config.epoch_num, model, train_iterator, dev_iterator, optimizer, criterion,
              [<span class="hljs-string">'0'</span>, <span class="hljs-string">'1'</span>], model_file, config.log_dir,config.print_step, <span class="hljs-string">'word'</span>)
    model.load_state_dict(torch.load(model_file))

    test_loss, test_acc, test_report = evaluate(model, test_iterator, criterion, [<span class="hljs-string">'0'</span>, <span class="hljs-string">'1'</span>], <span class="hljs-string">'word'</span>)
    print(<span class="hljs-string">'-----------Test----------'</span>)
    print(<span class="hljs-string">'\t Loss:{}|Acc:{}|Macro avg F1{}|Weighted avg F1{}'</span>.format(
        test_loss, test_acc, test_report[<span class="hljs-string">'macro avg'</span>][<span class="hljs-string">'f1-score'</span>], test_report[<span class="hljs-string">'weighted avg'</span>][<span class="hljs-string">'f1-score'</span>]
    ))
</code></pre>
<p>太多啦，懒得再记录鸟~下面分析一下卷积的过程吧。</p>
<h2 id="卷积的过程"><a class="markdownIt-Anchor" href="#卷积的过程"></a> 卷积的过程</h2>
<p>代码中用了5个卷积核，卷积核的大小分别是[1,2,3,4,5]。一维卷积层的输入是词向量的拼接，本代码中使用的词向量是50维，每个样本有60个词，因此卷积的输入是50x60的矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>。一维卷积核的输入通道数就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>的宽50，输出通道数是卷积核的个数200，则卷积核[1,2,3,4,5]对应的输出分别为[200*x60,200x59,200x58,200x57,200x56]。再分别对行进行最大池化，得到的输出为5个[200x1]的向量，将这5个向量拼接起来成[1000x1]的向量，输入前馈神经网络[1000x2]，即可进行二分类。老实说，一维卷积的过程自己没太看懂。代码中将卷积核的个数作为一维卷积的输出通道数，embedding维度作为卷积的输入通道数。一直以来看到的都是二维卷积，一维卷积有点懵，下面画个草图理解下吧。</p>
<p><a href="https://imgchr.com/i/l2WFPS" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2020/01/08/l2WFPS.md.jpg" srcset="/img/loading.gif" alt="l2WFPS.md.jpg" /></a></p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/CNN">CNN</a>
                
                  <a class="hover-with-bg" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">文本分类</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <script defer src="https://utteranc.es/client.js"
          repo="mz2sj/utterances"
          issue-term="pathname"
  
          label="utterances"
    
          theme="github-light"
          crossorigin="anonymous"
          async>
  </script>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    


    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "TextCNN&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- KaTeX -->
    <link rel="stylesheet" href="/lib/katex/katex.min.css"  >
  









</body>
</html>

---
title: 03-爬虫
date: 2020-04-25 22:00:44
tags: [selenium,cookie,session,代理]
categories: [爬虫]
---

### proxy ip 代理

代理主要用来解决ip频繁访问被服务器封号的问题，在requests中使用代理很简单.proxies格式是一个字典：` {‘http’: ‘[http://IP:port‘,‘https’:'https://IP:port‘}](http://xn--ip:port%2Chttps:'https-mc9nd6a//IP:port‘}) `,我们将它传入requests中的get方法就可以使用

```python
requests.get(url, headers = headers, proxies = proxies, timeout = 10)
```

我们可以从网上提供的一些代理池中获取有效的代理，具体代码如下：

```python
from bs4 import BeautifulSoup
import requests
import re
import json


def open_proxy_url(url):
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'
    headers = {'User-Agent': user_agent}
    try:
        r = requests.get(url, headers = headers, timeout = 10)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('无法访问网页' + url)


def get_proxy_ip(response):
    proxy_ip_list = []
    soup = BeautifulSoup(response, 'html.parser')
    proxy_ips = soup.find(id = 'ip_list').find_all('tr')
    for proxy_ip in proxy_ips:
        if len(proxy_ip.select('td')) >=8:
            ip = proxy_ip.select('td')[1].text
            port = proxy_ip.select('td')[2].text
            protocol = proxy_ip.select('td')[5].text
            if protocol in ('HTTP','HTTPS','http','https'):
                proxy_ip_list.append(f'{protocol}://{ip}:{port}')
    return proxy_ip_list


def open_url_using_proxy(url, proxy):
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'
    headers = {'User-Agent': user_agent}
    proxies = {}
    if proxy.startswith(('HTTPS','https')):
        proxies['https'] = proxy
    else:
        proxies['http'] = proxy

    try:
        r = requests.get(url, headers = headers, proxies = proxies, timeout = 10)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return (r.text, r.status_code)
    except:
        print('无法访问网页' + url)
        print('无效代理IP: ' + proxy)
        return False


def check_proxy_avaliability(proxy):
    url = 'http://www.baidu.com'
    result = open_url_using_proxy(url, proxy)
    VALID_PROXY = False
    if result:
        text, status_code = result
        if status_code == 200:
            r_title = re.findall('<title>.*</title>', text)
            if r_title:
                if r_title[0] == '<title>百度一下，你就知道</title>':
                    VALID_PROXY = True
        if VALID_PROXY:
            check_ip_url = 'https://jsonip.com/'
            try:
                text, status_code = open_url_using_proxy(check_ip_url, proxy)
            except:
                return

            print('有效代理IP: ' + proxy)
            with open('valid_proxy_ip.txt','a') as f:
                f.writelines(proxy)
            try:
                source_ip = json.loads(text).get('ip')
                print(f'源IP地址为：{source_ip}')
                print('='*40)
            except:
                print('返回的非json,无法解析')
                print(text)
    else:
        print('无效代理IP: ' + proxy)


if __name__ == '__main__':
    proxy_url = 'https://www.xicidaili.com/'
    proxy_ip_filename = 'proxy_ip.txt'
    text = open(proxy_ip_filename, 'r').read()
    proxy_ip_list = get_proxy_ip(text)
    for proxy in proxy_ip_list:
        check_proxy_avaliability(proxy)
```

### selenium

- selenium是什么：一个自动化测试工具（大家都是这么说的）
- selenium应用场景：用代码的方式去模拟浏览器操作过程（如：打开浏览器、在输入框里输入文字、回车等），在爬虫方面很有必要

selenium模拟浏览器进行操作，需要电脑安装相应的浏览器和浏览器驱动

 1、导入模块： 

```python
from selenium import webdriver  # 启动浏览器需要用到
from selenium.webdriver.common.keys import Keys  # 提供键盘按键支持（最后一个K要大写）
```

2、创建一个WebDriver实例： 

```python
driver = webdriver.Chrome("chromedriver驱动程序路径")
```

 3、打开一个页面: 

```python
driver.get("http://www.python.org")  # 这个时候chromedriver会打开一个Chrome浏览器窗口，显示的是网址所对应的页面
```

 4、关闭页面 

```python
driver.close()  # 关闭浏览器一个Tab
# or
driver.quit()  # 关闭浏览器窗口
```

#### 高级 查找元素

 在打开页面和关闭页面中间，就是各种操作！而查找元素这一点，和爬虫常见的HTML页面解析，定位到具体的某个元素基本一样，只不过，调用者是driver 

```python
element = driver.find_element_by_name("q")
```

#### 高级 页面交互

 找到元素后，就是进行“交互”，如键盘输入（需提前导入模块） 

```python
element.send_keys(“some text”）  # 往一个可以输入对象中输入“some text”
#甚至

element.send_keys(Keys.RETURN）  # 模拟键盘回车
#一般来说，这种方式输入后会一直存在，而要清空某个文本框中的文字，就需要：

element.clear()  # 清空element对象中的文字
```

### session和cookie

Session 是会话的意思，会话是产生在服务端的，用来保存当前用户的会话信息，而 Cookies 是保存在客户端（浏览器），有了 Cookie 以后，客户端（浏览器）再次访问服务端的时候，会将这个 Cookie 带上，这时，服务端可以通过 Cookie 来识别本次请求到底是谁在访问。

可以简单理解为 Cookies 中保存了登录凭证，我们只要持有这个凭证，就可以在服务端保持一个登录状态。

在爬虫中，有时候遇到需要登录才能访问的网页，只需要在登录后获取了 Cookies ，在下次访问的时候将登录后获取到的 Cookies 放在请求头中，这时，服务端就会认为我们的爬虫是一个正常登录用户。

#### session

- 那么，Cookies 是如何保持会话状态的呢？

  在客户端（浏览器）第一次请求服务端的时候，服务端会返回一个请求头中带有 `Set-Cookie` 字段的响应给客户端（浏览器），用来标记是哪一个用户，客户端（浏览器）会把这个 Cookies 给保存起来。

  我们来使用工具 PostMan 来访问下某东的登录页，看下返回的响应头：

  ![img](https://camo.githubusercontent.com/13e08343d36c9f3cce3d86318c6d096cb7fa55cb/68747470733a2f2f63646e2e6765656b64696767696e672e636f6d2f707974686f6e2d7370696465722f6a645f6c6f67696e2d687474702d726573706f6e73652e706e67)

- 当我们输入好用户名和密码时，客户端会将这个 Cookies 放在请求头一起发送给服务端，这时，服务端就知道是谁在进行登录操作，并且可以判断这个人输入的用户名和密码对不对，如果输入正确，则在服务端的 Session 记录一下这个人已经登录成功了，下次再请求的时候这个人就是登录状态了。

  如果客户端传给服务端的 Cookies 是无效的，或者这个 Cookies 根本不是由这个服务端下发的，或者这个 Cookies 已经过期了，那么接下里的请求将不再能访问需要登录后才能访问的页面。

  所以， Session 和 Cookies 之间是需要相互配合的，一个在服务端，一个在客户端。

#### cookie

 我们登陆一个网站，Chrome 中按 F12 打开开发者工具，选择 Application 标签，点开 Cookies 这一栏。

[![J6iXA1.png](https://s1.ax1x.com/2020/04/25/J6iXA1.png)](https://imgchr.com/i/J6iXA1)

- Name：这个是 Cookie 的名字。一旦创建，该名称便不可更改。
- Value：这个是 Cookie 的值。
- Domain：这个是可以访问该 Cookie 的域名。例如，如果设置为 .jd.com ，则所有以 jd.com ，结尾的域名都可以访问该Cookie。
- Max Age：Cookie 失效的时间，单位为秒，也常和 Expires 一起使用。 Max Age 如果为正数，则在 Max Age 秒之后失效，如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会保存该 Cookie 。
- Path：Cookie 的使用路径。如果设置为 /path/ ，则只有路径为 /path/ 的页面可以访问该 Cookie 。如果设置为 / ，则本域名下的所有页面都可以访问该 Cookie 。
- Size：Cookie 的大小。
- HTTPOnly：如果此项打勾，那么通过 JS 脚本将无法读取到 Cookie 信息，这样能有效的防止 XSS 攻击，窃取 Cookie 内容，可以增加 Cookie 的安全性。
- Secure：如果此项打勾，那么这个 Cookie 只能用 HTTPS 协议发送给服务器，用 HTTP 协议是不发送的。

那么有的网站为什么这次关闭了，下次打开的时候还是登录状态呢？

这就要说到 Cookie 的持久化了，其实也不能说是持久化，就是 Cookie 失效的时间设置的长一点，比如直接设置到 2099 年失效，这样，在浏览器关闭后，这个 Cookie 是会保存在我们的硬盘中的，下次打开浏览器，会再从我们的硬盘中将这个 Cookie 读取出来，用来维持用户的会话状态。

第二个问题产生了，服务端的会话也会无限的维持下去么，当然不会，这就要在 Cookie 和 Session 上做文章了， Cookie 中可以使用加密的方式将用户名记录下来，在下次将 Cookies 读取出来由请求发送到服务端后，服务端悄悄的自己创建一个用户已经登录的会话，这样我们在客户端看起来就好像这个登录会话是一直保持的。

#### 重要概念

当我们关闭浏览器的时候会自动销毁服务端的会话，这个是错误的，因为在关闭浏览器的时候，浏览器并不会额外的通知服务端说，我要关闭了，你把和我的会话销毁掉吧。

因为服务端的会话是保存在内存中的，虽然一个会话不会很大，但是架不住会话多啊，硬件毕竟是会有限制的，不能无限扩充下去的，所以在服务端设置会话的过期时间就非常有必要。

当然，有没有方式能让浏览器在关闭的时候同步的关闭服务端的会话，当然是可以的，我们可以通过脚本语言 JS 来监听浏览器关闭的动作，当浏览器触发关闭动作的时候，由 JS 像服务端发起一个请求来通知服务端销毁会话。

由于不同的浏览器对 JS 事件的实现机制不一致，不一定保证 JS 能监听到浏览器关闭的动作，所以现在常用的方式还是在服务端自己设置会话的过期时间

#### 模拟登陆163

```python
import time

from selenium import webdriver
from selenium.webdriver.common.by import By
```

```python
"""
使用selenium进行模拟登陆
1.初始化ChromDriver
2.打开163登陆页面
3.找到用户名的输入框，输入用户名
4.找到密码框，输入密码
5.提交用户信息
"""
name = '*'
passwd = '*'
driver = webdriver.Chrome('./chromedriver')
driver.get('https://mail.163.com/')
# 将窗口调整最大
driver.maximize_window()
# 休息5s
time.sleep(5)
current_window_1 = driver.current_window_handle
print(current_window_1)
```

```python
button = driver.find_element_by_id('lbNormal')
button.click()

driver.switch_to.frame(driver.find_element_by_xpath("//iframe[starts-with(@id, 'x-URS-iframe')]"))
```

```python
email = driver.find_element_by_name('email')
#email = driver.find_element_by_xpath('//input[@name="email"]')
email.send_keys(name)
password = driver.find_element_by_name('password')
#password = driver.find_element_by_xpath("//input[@name='password']")
password.send_keys(passwd)
submit = driver.find_element_by_id("dologin")
time.sleep(15)
submit.click()
time.sleep(10)
print(driver.page_source)
driver.quit()
```

这次的内容有点多，很多都是点了一下，并没有深入介绍，想要熟练掌握，还要多加练习，多啃文档。

### 丁香园自动化登陆

```python
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from lxml import etree
import time
```

```python
name='1876180****'
passwd='dxy123456'

driver=webdriver.Chrome(r'C:\Users\MZ\Downloads\chromedriver_win32\chromedriver.exe')
driver.get('http://www.dxy.cn/bbs/thread/626626#626626')
driver.maximize_window()
time.sleep(5)
current_window_1=driver.current_window_handle
print(current_window_1)
```

```python
#点击登陆按钮
button=driver.find_element_by_xpath('//*[@id="J-eventBanner"]/div/div/a[2]')
button.click()

#点击账号密码登陆
button=driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[1]/a[2]')
button.click()
```

```python
email=driver.find_element_by_id('username')
email.send_keys(name)
password=driver.find_element_by_name('password')
password.send_keys(passwd)
submit=driver.find_element_by_xpath('//*[@id="user"]/div[1]/div[3]/button')
time.sleep(3)
submit.click()
time.sleep(10)
print(driver.page_source)
```

至此即可获得登陆后的界面信息，抽取评论应该和上次所用方法一样，有时间再做补充。


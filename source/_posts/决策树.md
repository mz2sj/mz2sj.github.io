---
title: 决策树
date: 2021-06-08 20:57:19
tags: [数据分析,机器学习]
categories: [数据分析,算法]
---

决策树学习通常包括3个步骤：特征选择、决策树的生成、决策树的剪枝。

## ID3

ID3通过信息增益来选择特征和分裂点，信息增益由信息熵和条件熵计算得出。

熵的公式计算公式如下：

$\begin{array}{l}
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{ \mid C_{k} \mid}{|D|} \\\end{array}$

当样本中各类别数据概率相等时，熵最大。换种方式理解，拿出一个样本，它是各个类别的概率都是一样的，此时最难分辨样本的类别。熵越大样本的不确定性越高。

条件熵计算公式如下：

$H(D \mid A)=\sum_{i=1}^{n} \frac{\mid D_{i} \mid}{\mid D \mid} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D| \mid} \sum_{k=1}^{k} \frac{\left|D_{i \mid}\right|}{\mid D_{i \mid}} \log _{2} \frac{\mid D_{i k} \mid}{\mid D_{i \mid}} \\$

其中$i$代表$A$特征有几个类别，即A特征的基数。条件熵是在某特征各个类别值出现概率的基础上计算熵。

熵减去条件熵就是某个特征的信息增益：

$ g(D, A)=H(D)-H(D \mid A)$



ID3算法通过信息增益来选择特征：信息增益大的特征 具有更强的分类能力。如果一个特征的信息增益为0，即表示该特征没有什么分类能力。



ID3算法步骤简化理解：

1.计算各个特征的信息增益$A_g$,选择最大的$A_g$值对应的特征。

2.如果$A_g$小于阈值$\epsilon$,则该特征分类点下面所有样本实例数最多的类别就是该节点的类标记。

3.否则，对于$A_g$中的各个类别值，分裂成各个子节点。

4.迭代进行上面3步，直到信息增益均小于阈值$\epsilon$或者没有特征可以选择为止。



缺点：

1.ID3没有剪枝操作，只有输的生成，所以容易过拟合。

2.信息增益准则对可取值数目较多的属性有所偏好。

3.没有考虑缺失值的处理。

### C4.5

C4.5为了克服ID3对高基数特征有所偏好的特点，引入了信息增益比来选择特征，信息增益比计算公式如下：

$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)} 
$

$H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}$

信息增益比其实就是某个特征的信息增益与将某个特征类别的信息熵的比，信息增益比倾向于基数低的特征。



C4.5的决策树生成与ID3相同，引入了剪枝策略，包括预剪枝和后剪枝。

#### 预剪枝

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 节点内数据样本低于某一阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。

预剪枝可能会带来欠拟合问题。

#### 后剪枝

 C4.5 采用的**悲观剪枝方法**，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。  C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。 



缺点：

- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行

缺点还要再进行理解。

### CART

#### CART回归树

##### 回归树表示

一棵 CART 回归树对应着输入空间的一个划分，以及在划分单元上的输出值。
设输出  y  为连续变量, 训练数据集 $ \mathbb{D}=\left\{\left(\overrightarrow{\mathbf{x}}_{1}, \tilde{y}_{1}\right),\left(\overrightarrow{\mathbf{x}}_{2}, \tilde{y}_{2}\right), \cdots,\left(\overrightarrow{\mathbf{x}}_{N}, \tilde{y}_{N}\right)\right\} $
设已经将输入空间划分为  M  个单元 $ R_{1}, R_{2}, \cdots, R_{M} $, 且在每个单元  R_{m}  上有一个固定的输出值  $c_{m}$  则 CART 回归树模型可以表示为:

$f(\overrightarrow{\mathbf{x}})=\sum_{m=1}^{M} c_{m} I\left(\overrightarrow{\mathbf{x}} \in R_{m}\right)$

其中$  I(\cdot) $ 为示性函数。

##### 回归树损失函数

如果已知输入空间的单元划分，基于平方误差最小的准则，则 CART 回归树在训练数据集上的损失函数为:

$\sum_{m=1}^{M} \sum_{\overrightarrow{\mathbf{x}}_{i} \in R_{m}}\left(\tilde{y}_{i}-c_{m}\right)^{2}$

根据损失函数最小, 则可以求解出每个单元上的最优输出值$  \hat{c}_{m} $ 为  :$ R_{m}  $上所有输入样本  $\overrightarrow{\mathbf{x}}_{i} $ 对应的输出 $ \tilde{y}_{i} $ 的平均值。
即:  $\hat{c}_{m}=\frac{1}{N_{m}} \sum_{\overrightarrow{\mathbf{x}}_{i} \in R_{m}} \tilde{y}_{i} $, 其中  $N_{m} $ 表示单元  $R_{m} $ 中的样本数量。

##### 如何选择切分点进行划分

设输入为  $n$  维:$ \overrightarrow{\mathbf{x}}=\left(x_{1}, x_{2}, \cdots, x_{n}\right)^{T}$  。
1.选择第  $j$  维 $ x_{j} $ 和它的取值  $s$  作为切分变量和切分点。定义两个区域:

$\begin{array}{l}
R_{1}(j, s)=\left\{\overrightarrow{\mathbf{x}} \mid x_{j} \leq s\right\} \\
R_{2}(j, s)=\left\{\overrightarrow{\mathbf{x}} \mid x_{j}>s\right\}
\end{array}$

2.然后寻求最优切分变量  $j$  和最优切分点  $s$  。即求解:

$\left(j^{*}, s^{*}\right)=\min _{j, s}\left[\min _{c_{1}} \sum_{\vec{x}_{i} \in R_{1}(j, s)}\left(\tilde{y}_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{\vec{x}_{i} \in R_{2}(j, s)}\left(\tilde{y}_{i}-c_{2}\right)^{2}\right]$

其意义为：
- 首先假设已知切分变量  $j$ , 则遍历最优切分点  $s_{1}$  则到:

$\hat{c}_{1}=\frac{1}{N_{1}} \sum_{\overrightarrow{\mathbf{x}}_{i} \in R_{1}(j, s)} \tilde{y}_{i}, \quad \hat{c}_{2}=\frac{1}{N_{2}} \sum_{\vec{x}_{i} \in R_{2}(j, s)} \tilde{y}_{i}$

其中  $N_{1}$  和 $ N_{2} $ 分别代表区域  $R_{1}$  和 $ R_{2}$  中的样本数量。

- 然后遍历所有的特征维度, 对每个维度找到最优切分点。从这些 (切分维度,最优切分点) 中找到使但损失函数最小的那个。

3.依次将输入空间划分为两个区域，然后重复对子区域划分，直到满足停止条件为止。这样的回归树称为最小二乘回归树。

#### CART分类树生成

cart分类树采用基尼指数选择最优特征。 

$\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$

对于二分类问题,基尼指数为

$Gini(p)=p(1-p)+(1-p)(1-(1-p))=2p(1-p)$

如果样本集合D根据特征A的某一值a分为$D_1$和$D_2$两部分，则在特征A的条件下，基尼指数定义为

$\operatorname{Gini}(D \mid A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)$



 cart分类树生成过程与回归树类似， 遍历所有可能的维度$j$ 和该维度所有可能的取值 $s$，取使得基尼系数最小的那个维度 $j$和切分点$s$ 。 划分区域中样本比例高的类别即是该样本区域对应的类别。



cart 分类树和cart 回归树通常的停止条件为：

- 结点中样本个数小于预定值，这表示树已经太复杂。
- 样本集的损失函数或者基尼指数小于预定值，表示结点已经非常纯净。
- 没有更多的特征可供切分。



#### CART剪枝

`CART` 树的剪枝是从完全生长的`CART` 树底端减去一些子树，使得`CART` 树变小（即模型变简单），从而使得它对未知数据有更好的预测能力。`CART`剪枝算法分为两步：首先从生成算法的决策树$T_0$不算剪枝，形成一个子树序列${T_0,T_1,...,T_n}$,然后通过交叉验证法在独立的验证数据集上对子树序列进行预测，从中选择最优子树。


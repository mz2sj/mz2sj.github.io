---
title: 02-二手车价格预测之特征工程
date: 2020-03-28 19:32:28
tags:[数据分析]
categories:[数据分析,特征工程]
---

今天要做的笔记是特征工程，感觉这次官方给的代码不是特别全。最近在忙作业和kaggle的比赛，后面自己有时间会再完善一下。自己将按照特征工程的常见任务进行介绍。

### 异常处理

数据收集的过程中可能会产生脏数据，导致产生离群值，数据分布明显不合理，这时候我们要对数据进行异常处理。常见的异常处理方法有：

#### 通过箱型图删除异常值

官方提供了相关代码

```python
def outliers_proc(data,col_name,scale=3):

  def box_plot_outliers(data_ser,box_scale):
    iqr=box_scale*(data_ser.quantile(0.75)-data_ser.quantile(0.25))
    val_low=data_ser.quantile(0.25)-iqr
    val_up=data_ser.quantile(0.75)+iqr
    rule_low=(data_ser<val_low)
    rule_up=(data_ser>val_up)
    return (rule_low,rule_up),(val_low,val_up)

  data_n=data.copy()
  data_series=data_n[col_name]
  rule,value=box_plot_outliers(data_series,box_scale=scale)
  index=np.arange(data_series.shape[0])[rule[0]|rule[1]]
  print('Delete number is :{}'.format(len(index)))
  data_n=data_n.drop(index)
  data_n.reset_index(drop=True,inplace=True)
  print('Now column number is :{}'.format(data_n.shape[0]))
  index_low=np.arange(data_series.shape[0])[rule[0]]
  outliers=data_series.iloc[index_low]
  print('Description of data less than the lower bound is :')
  print(pd.Series(outliers).describe())
  index_up=np.arange(data_series.shape[0])[rule[1]]
  outliers=data_series.iloc[index_up]
  print('Description of data larger than the upper bound is:')
  print(pd.Series(outliers).describe())

  fig,ax=plt.subplots(1,2,figsize=(10,7))
  sns.boxplot(y=data[col_name],data=data,palette='Set1',ax=ax[0])
  sns.boxplot(y=data_n[col_name],data=data_n,palette='Set1',ax=ax[1])
  return data_n
```

<img src="https://s1.ax1x.com/2020/03/28/GA39YT.png" alt="GA39YT.png" style="zoom: 80%;" />

可以看出异常值得到了有效剔除，不过这段代码不是太懂，有时间还要再研究一下。

#### BOX-COX 转换（处理有偏分布）

 [待完善](https://zhuanlan.zhihu.com/p/40125782 ) ，我们希望数据尽量符合正态分布，除了这种方法，常见的还有对数据取log、倒数等。

#### 长尾截断

对此自己持怀疑态度，网上也没找到相关代码。如果数据的特征少时，截断一部分数据还能接受，如果符合长尾的特则很多再进行长尾阶段似乎显得不合适。

### 特征归一化/标准化

sklearn有专门的数据归一化和标注化的包，当然自己写也不难

#### 归一化

```pyth
def max_min(x):
  return (x-np.min(x))/(np.max(x)-np.min(x))
```

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x = scaler.fit_transform(x)
```



#### 标准化

```python
def standard(x):
  return (x-np.mean(x))/(np.var(x))
```

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x = scaler.fit_transform(x)
```

#### 幂率分布

这种情况我还没遇到过，在此记录下

$log(\frac{1+x}{1+median})$

### 数据分桶

通俗理解下，就是将数据分段。举个例子年龄段的分桶

#### 等频分桶

待完善

#### 等距分桶

```python
bin=[i*10 for i in range(31)]
data['power_bin']=pd.cut(data['power'],bin,labels=False)
data[['power_bin','power']].head()
```

#### Best-KS分桶

待完善

#### 卡方分桶

待完善

### 缺失值处理

对于树模型，模型本身已经有了对于缺失值的处理机制，这时候我们可以不对数据进行处理。

其他的常见方法是对缺失值进行填充，常见的有平均数、中位数、众数等等。

分箱，对含有缺失值的数据进行分箱，缺失的数据单独为一类，但是如果还想利用原来未缺失的数据值，分箱并未对缺失值数据进行填充。

### 特征构造

#### 构造统计量

如各个类别的数据求均值、方差、极值、中位数等等

```python
train_gb=train.groupby('brand')
all_info={}
for kind,kind_data in train_gb:
  info={}
  kind_data=kind_data[kind_data['price']>0]
  info['brand_amount']=len(kind_data)
  info['brand_price_max']=kind_data.price.max()
  info['brand_price_median']=kind_data.price.median()
  info['brand_price_min']=kind_data.price.min()
  info['brand_price_sum']=kind_data.price.sum()
  info['brand_price_std']=kind_data.price.std()
  info['brand_price_average']=round(kind_data.price.sum()/(len(kind_data)+1),2)
  all_info[kind]=info
brand_fe=pd.DataFrame(all_info).T.reset_index().rename(columns={'index':'brand'})
data=data.merge(brand_fe,how='left',on='brand')
```

#### 时间特征

求相对时间、节假日等，在本项目中，汽车的使用时间很明显就会对销售价格产生影响。

```python
ata['used_time']=(pd.to_datetime(data['creatDate'],format='%Y%m%d',errors='coerce')-
            pd.to_datetime(data['regDate'],format='%Y%m%d',errors='coerce')).dt.days
```

#### 地理特征

前提是你得知道数据中所包含的地理信息，

```python
data['city']=data['regionCode'].apply(lambda x:str(x))[:-3]
```



除了以上的特征构造方式外，还有特征组合、对数据的非线性变换（log、根值、平方）等等。私以为构造可解释的特征组合还比较有说服力，胡乱组合的特征可能就有点碰运气了。

### 特征筛选

#### 过滤式

通过相关系数选择发、互信息法、卡方检验、Relief、方差选择法选择特征，相关系数法比较好理解，越相关的特征当然更值得选择。

```python
data_numeric=data[['power','kilometer','brand_amount','brand_price_average','brand_price_max','brand_price_median']]
correlation=data_numeric.corr()

f,ax=plt.subplots(figsize=(7,7))
plt.title('Correlation of Numeric Features with Price',y=1,size=16)
sns.heatmap(correlation,square=True,vmax=0.8)
```

u1s1,热力图可以直接看出哪些特征比较相关，不过这个图似乎少了price这个变量

[<img src="https://s1.ax1x.com/2020/03/28/GAaTkd.png" alt="GAaTkd.png" style="zoom:80%;" />

#### 包裹式

把学习器的性能作为特征子集的评价准则，类似于auto-ml？或者是logistic回归的变量系数，系数越大越相关。利用包帮我们自助寻找特征

```python
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LinearRegression
sfs=SFS(LinearRegression(),
        k_features=10,
        forward=True,
        floating=False,
        scoring='r2',
        cv=0)
x=data.drop(['price'],axis=1)
x=x.fillna(0)
y=y.fillna(0)
y=data['price']
sfs.fit(x,y)
sfs.k_feature_names_
```

```python
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
fig1=plot_sfs(sfs.get_metric_dict(),kind='std_dev')
plt.grid()
plt.show()
```

#### 嵌入式

结合过滤式和包裹式，在训练过程进行特征选择。其实就是把数据输入算法中，让算法自己取选择特征呗。

### 降维

降维自己还是有点了解的，对于高维度数据可以降到低维度。在数据可视化也常用到。常用的降维算法有PCA、t-SNE、LDA、LSA等等。



这次的内容好多啊，感觉给的笔记也不是很全，还需要自己再多做总结。
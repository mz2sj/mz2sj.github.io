---
title: 02-爬虫
date: 2020-04-23 19:49:12
tags: [正则表达式,bs4,Xpath]
categories: [爬虫]
---

### 正则表达式

#### 正则表达式语法

- `.` 表示任何单个字符
- `[ ]` 字符集，对单个字符给出取值范围 ，如`[abc]`表示a、b、c，`[a‐z]`表示a到z单个字符
- `[^ ]` 非字符集，对单个字符给出排除范围 ，如`[^abc]`表示非a或b或c的单个字符
- `*` 前一个字符0次或无限次扩展，如abc* 表示 ab、abc、abcc、abccc等
- `+` 前一个字符1次或无限次扩展 ，如abc+ 表示 abc、abcc、abccc等
- `?` 前一个字符0次或1次扩展 ，如abc? 表示 ab、abc
- `|` 左右表达式任意一个 ，如abc|def 表示 abc、def
- `{m}` 扩展前一个字符m次 ，如ab{2}c表示abbc
- `{m,n}` 扩展前一个字符m至n次（含n） ，如ab{1,2}c表示abc、abbc
- `^` 匹配字符串开头 ，如^abc表示abc且在一个字符串的开头
- `$` 匹配字符串结尾 ，如abc$表示abc且在一个字符串的结尾
- `( )` 分组标记，内部只能使用 | 操作符 ，如(abc)表示abc，(abc|def)表示abc、def，使用分组除了整体匹配的符号外，单独的分组也会匹配出来。
- `\d` 数字，等价于`[0‐9]`
- `\w` 单词字符，等价于`[A‐Za‐z0‐9_]`

#### re库

- re.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象
  - re.search(pattern, string, flags=0)
- re.match() 从一个字符串的开始位置起匹配正则表达式，返回match对象
  - re.match(pattern, string, flags=0)
- re.findall() 搜索字符串，以列表类型返回全部能匹配的子串
  - re.findall(pattern, string, flags=0)
- re.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型
  - re.split(pattern, string, maxsplit=0, flags=0)
- re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象
  - re.finditer(pattern, string, flags=0)
- re.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串
  - re.sub(pattern, repl, string, count=0, flags=0)
  - flags : 正则表达式使用时的控制标记：
    - re.I --> re.IGNORECASE : 忽略正则表达式的大小写，`[A‐Z]`能够匹配小写字符
    - re.M --> re.MULTILINE : 正则表达式中的^操作符能够将给定字符串的每行当作匹配开始
    - re.S --> re.DOTALL : 正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符
#### 淘宝商品爬虫

```python
import requests
import re

def getHTMLText(url):
    try:
        kv={'cookie':'thw=cn; cna=+xjxFvvjZFUCAbegU4BJw+f0; hng=CN%7Czh-CN%7CCNY%7C156; _samesite_flag_=true; cookie2=18ea7aba395e98b25e202aca290dfc51; t=063bd64499346909d85799069ecdfb0a; _tb_token_=e7b0d70737676; sgcookie=EQbeuix2KIOgXG2FFLZOJ; unb=2657451481; uc3=vt3=F8dBxGR2VD%2BIzyEg%2Beo%3D&lg2=UtASsssmOIJ0bQ%3D%3D&id2=UU6kVNSBaZ7V0Q%3D%3D&nk2=oHW%2BtqmuGFM%3D; csg=4349f769; lgc=%5Cu5B5F%5Cu95471023; cookie17=UU6kVNSBaZ7V0Q%3D%3D; dnk=%5Cu5B5F%5Cu95471023; skt=490ef5758fe425d4; existShop=MTU4NzU0MjE2Mg%3D%3D; uc4=nk4=0%40oicDEdY%2Fq4hj7fRzY6bY5rGJXw%3D%3D&id4=0%40U2xpViK2NwPK%2B6Z6pycbkB5Z8GcI; tracknick=%5Cu5B5F%5Cu95471023; _cc_=VFC%2FuZ9ajQ%3D%3D; _l_g_=Ug%3D%3D; sg=313; _nk_=%5Cu5B5F%5Cu95471023; cookie1=WqUIbiciTOhdw8TeNfR1ltACceU5jKLQIv2L5E78Zzo%3D; enc=5kK7r49vDFT6xXh5WeuK224BSMneKyQf4H%2F7R%2FSyjHeMJIz8BwUqOX967CJBECgM1E03U6Y1ORAfflhD%2FmV1Mg%3D%3D; JSESSIONID=F55DD890B8CDAD1F8644BE9DABD3A503; tfstk=cEb1BFvrHAD1s1bZ8tNebz9sepTVa6yBtl9O1MBYuf-uj7C6psAjzL_IQiDnJ3dC.; uc1=cookie16=VT5L2FSpNgq6fDudInPRgavC%2BQ%3D%3D&cookie21=Vq8l%2BKCLjhS4UhJVbhgU&cookie15=V32FPkk%2Fw0dUvg%3D%3D&existShop=false&pas=0&cookie14=UoTUPcllKBuWuA%3D%3D; mt=ci=75_1; v=0; isg=BPDwL_0CWmxemQaV6E8w83tQwb5COdSDUALMBOpBvMsepZBPkkmkE0aX-a3FLoxb; l=eBxryhwuQJlCKyvkBOfaFurza77OSIRYYuPzaNbMiT5POS5B5QzGWZX3qAT6C3GVh6ByR3JMwUXJBeYBqQAonxv92j-la_kmn',
            'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'}
        r=requests.get(url,timeout=30,headers=kv)
        r.raise_for_status()
        r.encoding=r.apparent_encoding
        return r.text
    except:
        return '爬取失败'
    

def parsePage(glist,html):
    try:
        price_list=re.findall('\"view_price\":\"\d+\.\d+\"',html)
        name_list=re.findall(r'\"raw_title\":\"[a-zA-Z0-9_\w\s\\\u3010\u3011\-*]+\"',html)
        for i in range(len(price_list)):
            price=eval(price_list[i].split(":")[1])
            name=eval(name_list[i].split(":")[1])
#             print(price,name)
            glist.append([price,name])
    except:
        print('解析失败')
        
        
def printGoodList(glist):
    tplt="{0:^4}\t{1:6}\t{2:^10}"
    print(tplt.format('序号',"商品价格",'商品名称'))
    count=0
    for g in glist:
        count+=1
        print(tplt.format(count,g[0],g[1]))
        
goods_name='书包'
start_url='https://s.taobao.com/search?q='+goods_name
info_list=[]
page=3


count=0
for i in range(page):
    count+=1
    try:
        url=start_url+"&s"+str(44*i)
        html=getHTMLText(url)
        parsePage(info_list,html)
        print("\r爬取页面当前进度:{:.2f}%".format(count*100/page),end="")
    except:
        continue
```

正则表达式的问题主要出现在匹配商品名称上面，自己的表达式为`\"raw_title\":\"[a-zA-Z0-9_\w\s\\\u3010\u3011\-*]+\"`,主要出现的问题是商品名称会出现许多特殊符号，比如【-，等等，自己想用`.*`来匹配，但是`.`会匹配所有字符，产生越界问题。参考答案给的解决方案是`\"raw_title\"\:\".*?\"`,在`.*`后面加上问号就可以解决越界问题。

### BeautifulSoup

#### BeautifulSoup的基本元素

1. Beautiful Soup库的理解： Beautiful Soup库是解析、遍历、维护“标签树”的功能库，对应一个HTML/XML文档的全部内容
2. BeautifulSoup类的基本元素:
   - `Tag 标签，最基本的信息组织单元，分别用<>和标明开头和结尾；`
   - `Name 标签的名字，…的名字是'p'，格式：.name;`
   - `Attributes 标签的属性，字典形式组织，格式：.attrs;`
   - `NavigableString 标签内非属性字符串，<>…中字符串，格式：.string;`
   - `Comment 标签内字符串的注释部分，一种特殊的Comment类型;`

```python
# 导入bs4库
from bs4 import BeautifulSoup
import requests # 抓取页面

r = requests.get('https://python123.io/ws/demo.html') # Demo网址
demo = r.text  # 抓取的数据
demo

# 解析HTML页面
soup = BeautifulSoup(demo, 'html.parser')  # 抓取的页面数据；bs4的解析器
# 有层次感的输出解析后的HTML页面
print(soup.prettify())
```

#####  标签

直接使用tag名获得

```python
soup.a 
soup.title
```

返回的string对象可以继续调用BeautifulSoup的层级标签功能

##### 标签的名字

```python
soup.a.name
soup.a.parent.name
soup.p.parent.name
```

可以用来查看层级标签的名字，当层级标签比较复杂时可以使用。

##### 标签的属性

```python
tag = soup.a
print(tag.attrs)
print(tag.attrs['class'])
print(type(tag.attrs))
```

注意，返回的是关于标签的一个列表

##### string

````python
print(soup.a.string)
print(type(soup.a.string))
````

返回标签内的非属性字符串。

##### prettify()

```python
print(soup.prettify())
```

返回层次化的标签结构

#### 基于bs4库的HTML内容遍历方法

HTML基本格式:`<>…`构成了所属关系，形成了标签的树形结构

- 标签树的下行遍历
  - .contents 子节点的列表，将``所有儿子节点存入列表
  - .children 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点
  - .descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历
- 标签树的上行遍
  - .parent 节点的父亲标签
  - .parents 节点先辈标签的迭代类型，用于循环遍历先辈节点
- 标签树的平行遍历
  - .next_sibling 返回按照HTML文本顺序的下一个平行节点标签
  - .previous_sibling 返回按照HTML文本顺序的上一个平行节点标签
  - .next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签
  - .previous_siblings 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签

##### 标签树的下行遍历

```python
import requests
from bs4 import BeautifulSoup

r=requests.get('http://python123.io/ws/demo.html')
demo=r.text
soup=BeautifulSoup(demo,'html.parser')

print(soup.contents)# 获取整个标签树的儿子节点

print(soup.body.contents)#返回标签树的body标签下的节点

print(soup.head)#返回head标签

for child in soup.body.children:#遍历儿子节点，不会遍历儿子节点下的节点
    print(child)
    
for child in soup.body.descendants:#遍历子孙节点，遍历父节点下面所有节点
    print(child)
```

##### 标签树的上行遍历

```python
soup.title.parent
soup.title.parent

for parent in soup.a.parents: # 遍历先辈的信息
    if parent is None:
        print(parent)
    else:
        print(parent.name)
```

##### 标签树的平行遍历

注意：

- 标签树的平行遍历是有条件的
- 平行遍历发生在同一个父亲节点的各节点之间
- 标签中的内容也构成了节点,连续的字符串也是节点

```python
print(soup.a.next_sibling)#a标签的下一个标签
print(soup.a.next_sibling.next_sibling)#a标签的下一个标签的下一个标签
print(soup.a.previous_sibling)#a标签的前一个标签
print(soup.a.previous_sibling.previous_sibling)#a标签的前一个标签的前一个标签
for sibling in soup.a.next_siblings:#遍历后续节点
    print(sibling)
for sibling in soup.a.previous_siblings:#遍历之前的节点
    print(sibling)
```

#### 基于bs4库的html查找方法

- <>.find_all(name, attrs, recursive, string, **kwargs)
  - 参数：
  - ∙ name : 对标签名称的检索字符串
  - ∙ attrs: 对标签属性值的检索字符串，可标注属性检索
  - ∙ recursive: 是否对子孙全部检索，默认True
  - ∙ string: <>…</>中字符串区域的检索字符串
    - 简写：
    - ``(..) 等价于 ``.find_all(..)
    - soup(..) 等价于 soup.find_all(..)
- 扩展方法：
  - <>.find() 搜索且只返回一个结果，同.find_all()参数
  - <>.find_parents() 在先辈节点中搜索，返回列表类型，同.find_all()参数
  - <>.find_parent() 在先辈节点中返回一个结果，同.find()参数
  - <>.find_next_siblings() 在后续平行节点中搜索，返回列表类型，同.find_all()参数
  - <>.find_next_sibling() 在后续平行节点中返回一个结果，同.find()参数
  - <>.find_previous_siblings() 在前序平行节点中搜索，返回列表类型，同.find_all()参数
  - <>.find_previous_sibling() 在前序平行节点中返回一个结果，同.find()参数

```python
import requests
from bs4 import BeautifulSoup

r = requests.get('http://python123.io/ws/demo.html')
demo = r.text
soup = BeautifulSoup(demo,'html.parser')
soup
```

```python
# name : 对标签名称的检索字符串
soup.find_all('a')
soup.find_all(['a', 'p'])
# attrs: 对标签属性值的检索字符串，可标注属性检索
soup.find_all("p","course")
```

```python
soup.find_all(id="link") # 完全匹配才能匹配到
#  recursive: 是否对子孙全部检索，默认True
soup.find_all('p',recursive=False)
# string: <>…</>中字符串区域的检索字符串
soup.find_all(string = "Basic Python") # 完全匹配才能匹配到
```

#### 好大学排名爬虫给你

- 爬取url：http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html
- 爬取思路：
  1. 从网络上获取大学排名网页内容
  2. 提取网页内容中信息到合适的数据结构（二维数组）-排名，学校名称，总分
  3. 利用数据结构展示并输出结果

```python
# 导入库
import requests
from bs4 import BeautifulSoup
import bs4
```

自己和官方代码比起来就比较简陋了

```python
r=requests.get('http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html')
r.raise_for_status()
r.encoding=r.apparent_encoding  #自动获取编码，可以省好多事
demo=r.text
demo
```

```python
soup=BeautifulSoup(demo,'html.parser')

print('{0:^8}\t{1:^18}\t{2:^18}'.format("排名","学校名称","总分"))
for tr in trs:
    rank=tr.find_all('td')[0].string 
    school_name=tr.find_all('td')[1].string
    score=tr.find_all('td')[4].string
    print('{0:^8}\t{1:^18}\t{2:^18}'.format(rank,school_name,score))
```

### Xpath

#### Xpath 语法

- XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。
- 在XPath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。
- XML文档是被作为节点树来对待的。

XPath使用路径表达式在XML文档中选取节点。节点是通过沿着路径选取的。下面列出了最常用的路径表达式：

- nodename 选取此节点的所有子节点。
- / 从根节点选取。
- // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。
- . 选取当前节点。
- .. 选取当前节点的父节点。
- @ 选取属性。
- /text() 提取标签下面的文本内容
  - 如：
  - /标签名 逐层提取
  - /标签名 提取所有名为<>的标签
  - //标签名[@属性=“属性值”] 提取包含属性为属性值的标签
  - @属性名 代表取某个属性名的属性值
- 详细学习：https://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html

#### 使用lxml解析

- 导入库：from lxml import etree
- lxml将html文本转成xml对象
  - tree = etree.HTML(html)
- 用户名称：tree.xpath(’//div[@class=“auth”]/a/text()’)
- 回复内容：tree.xpath(’//td[@class=“postbody”]’) 因为回复内容中有换行等标签，所以需要用string()来获取数据。
  - string()的详细见链接：https://www.cnblogs.com/CYHISTW/p/12312570.html
- Xpath中text()，string()，data()的区别如下：
  - text()仅仅返回所指元素的文本内容。
  - string()函数会得到所指元素的所有节点文本内容，这些文本讲会被拼接成一个字符串。
  - data()大多数时候，data()函数和string()函数通用，而且不建议经常使用data()函数，有数据表明，该函数会影响XPath的性能。

#### 丁香园评论爬虫

chrome的开发工具里面是有copy Xpath这个选项的

```python
# 导入库
from lxml import etree
import requests

url = "http://www.dxy.cn/bbs/thread/626626#626626"
```

```python
req = requests.get(url)
html = req.text
```

```python
tree = etree.HTML(html) 
tree
```

```python
user = tree.xpath('//div[@class="auth"]/a/text()')
# print(user)
content = tree.xpath('//td[@class="postbody"]')
```

```python
results = []
for i in range(0, len(user)):
    # print(user[i].strip()+":"+content[i].xpath('string(.)').strip())
    # print("*"*80)
    # 因为回复内容中有换行等标签，所以需要用string()来获取数据
    results.append(user[i].strip() + ":  " + content[i].xpath('string(.)').strip())
```

```python
# 打印爬取的结果
for i,result in zip(range(0, len(user)),results):
    print("user"+ str(i+1) + "-" + result)
    print("*"*100)
```

比较让自己困惑的是text()和string(.)的用法，博客上介绍说text()返回的是匹配到的所有元素包含文字的列表，而string(.)返回的是合并后的结果。而且string(.)的用法要在获取上级元素后，单独对每个元素调用.xpath('string(.)')。后来我才发现，如果文字中内嵌有其他标签，比如br等，就会将文字分割成单独的各个列表元素，和上一层及的元素并列产生问题。因此我们可以对同一层级的元素使用string(.),这样不管同级元素是否有下级元素，都会统一成一个元素。

收工~~~
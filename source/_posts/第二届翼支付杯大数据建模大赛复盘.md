---
title: 第二届翼支付杯大数据建模大赛复盘
date: 2020-10-03 15:51:44
tags: ['数据竞赛']
categories: ['数据竞赛']
---

暑假的时候参加了一个数据竞赛--第二届翼支付杯大数据建模大赛-信用风险用户识别。最后复赛A榜排在第15名，B榜12名。第一次正式参加比赛，原本只想排到六七十名就够了，没想到最后能进到前二十，感谢队友和自己的付出。虽然最后的结果还不错，但是做的还是模模糊糊的，现在对一些top方案和自己参加比赛的想法做一些总结。

## 数据

### 基础信息 base_df

| 字段名            | 字段说明（数据经过脱敏处理）                                 |
| ----------------- | ------------------------------------------------------------ |
| user              | 样本编号，e.g., Train_00000、Train_00001...                  |
| sex               | 性别，编码后取值为：category 0、category1                    |
| age               | 年龄，处理后仅保留大小关系，为某一区间的整数                 |
| provider          | 运营商类型，编码后取值为：category 0、category 1...          |
| level             | 用户等级，编码后取值为：category 0、category 1...            |
| verified          | 是否实名，编码后取值为：category 0、category1                |
| using_time        | 使用时长，处理后仅保留大小关系，为某一区间的整数             |
| regist_type       | 注册类型，编码后取值为：category 0、category 1...            |
| card_a_cnt        | a类型卡的数量，处理后仅保留大小关系，为某一区间的整数        |
| card_b_cnt        | b类型卡的数量，处理后仅保留大小关系，为某一区间的整数        |
| card_c_cnt        | c类型卡的数量，处理后仅保留大小关系，为某一区间的整数        |
| card_d_cnt        | d类型卡的数量，处理后仅保留大小关系，为某一区间的整数        |
| op1_cnt           | 某类型1操作数量，处理后仅保留大小关系，为某一区间的整数      |
| op2_cnt           | 某类型2操作数量，处理后仅保留大小关系，为某一区间的整数      |
| service1_cnt      | 某业务1产生数量，处理后仅保留大小关系，为某一区间的整数      |
| service1_amt      | 某业务1产生金额，处理后仅保留大小关系，为某一区间的整数      |
| service2_cnt      | 某业务2产生数量，处理后仅保留大小关系，为某一区间的整数      |
| agreement_total   | 开通协议数量，处理后仅保留大小关系，为某一区间的整数         |
| agreement1        | 是否开通协议1，编码后取值为：category 0、category1           |
| agreement2        | 是否开通协议2，编码后取值为：category 0、category1           |
| agreement3        | 是否开通协议3，编码后取值为：category 0、category1           |
| agreement4        | 是否开通协议4，编码后取值为：category 0、category1           |
| acc_count         | 账号数量，处理后仅保留大小关系，为某一区间的整数             |
| login_cnt_period1 | 某段时期1的登录次数，处理后仅保留大小关系，为某一区间的整数  |
| login_cnt_period2 | 某段时期2的登录次数，处理后仅保留大小关系，为某一区间的整数  |
| ip_cnt            | 某段时期登录ip个数，处理后仅保留大小关系，为某一区间的整数   |
| login_cnt_avg     | 某段时期登录次数均值，处理后仅保留大小关系，为某一区间的整数 |
| login_days_cnt    | 某段时期登录天数，处理后仅保留大小关系，为某一区间的整数     |
| province          | 省份，处理成类别编码                                         |
| city              | 城市，处理成类别编码                                         |
| balance           | 余额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| balance_avg       | 近某段时期余额均值等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| balance1          | 类型1余额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| balance1_avg      | 近某段时期类型1余额均值等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| balance2          | 类型2余额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| balance2_avg      | 近某段时期类型2余额均值等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| service3          | 是否服务3用户，编码后取值为：category 0、category1           |
| service3_level    | 服务3等级，编码后取值为：category 0、category1...            |
| product1_amount   | 产品1金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product2_amount   | 产品2金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product3_amount   | 产品3金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product4_amount   | 产品4金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product5_amount   | 产品5金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product6_amount   | 产品6金额等级，处理成保留大小关系的类别编码：level 1、level2... 例如：level 2 > level 1 |
| product7_cnt      | 产品7申请次数，处理后仅保留大小关系，为某一区间的整数        |
| product7_fail_cnt | 产品7申请失败次数，处理后仅保留大小关系，为某一区间的整数    |

### 操作信息 op_df

| 字段名    | 字段说明（数据经过脱敏处理）                                 |
| --------- | ------------------------------------------------------------ |
| user      | 样本编号，e.g., Train_00000、Train_00001...                  |
| op_type   | 操作类型编码，处理成类别编码                                 |
| op_mode   | 操作模式编码，处理成类别编码                                 |
| op_device | 操作设备编码，处理成类别编码                                 |
| ip        | 设备ip编码，处理成类别编码                                   |
| net_type  | 网络类型编码，处理成类别编码                                 |
| channel   | 渠道类型编码，处理成类别编码                                 |
| ip_3      | 设备ip前三位编码，处理成类别编码                             |
| tm_diff   | 距离某起始时间点的时间间隔，处理成如下格式。例如： 9 days 09:02:45.000000000，表示距离某起始时间点9天9小时2分钟45秒 |

#### 交易信息 trans_df

| 字段名     | 字段说明（数据经过脱敏处理）                                 |
| ---------- | ------------------------------------------------------------ |
| user       | 样本编号，e.g., Train_00000、Train_00001...                  |
| platform   | 平台类型编码，处理成类别编码                                 |
| tunnel_in  | 来源类型编码，处理成类别编码                                 |
| tunnel_out | 去向类型编码，处理成类别编码                                 |
| amount     | 交易金额，处理后仅保留大小关系，为某一区间的整数             |
| type1      | 交易类型1编码，处理成类别编码                                |
| type2      | 交易类型2编码，处理成类别编码                                |
| ip         | 设备ip编码，处理成类别编码                                   |
| ip_3       | 设备ip前三位编码，处理成类别编码                             |
| tm_diff    | 距离某起始时间点的时间间隔，处理成如下格式。例如： 9 days 09:02:45.000000000，表示距离某起始时间点9天9小时2分钟45秒 |

评价方式采用roc_auc_score

## top1方案

1.第一个比较有意思的点就是作者根据`user`键将trans_df和op_df中merge进了base_df的`label`,这样就能对trans_df和op_df作target_encoding啦~

```python
def gen_trans_op_label(trans, op, train_label):
    trans_ = trans.copy()
    op_ = op.copy()
    user_label_dict = dict(zip(train_label.user.values, 		       train_label.label.values))
    trans_['label'] = trans_['user'].map(user_label_dict)
    op_['label'] = op_['user'].map(user_label_dict)

    return trans_, op_
```

让我们学习一下pandas中的`map`操作。

其次就是将trans_df和op_df拼接成一张表trans_op_df，这样一张表就同时包含操作和交易信息,也许会带来更多的信息哦

![03tuDJ.png](https://s1.ax1x.com/2020/10/03/03tuDJ.png)

2.构建session，这个是根据操作或者交易的时间差来确定一个session，并不是很懂。当相邻两个操作或者交易时间差超过600时，赋值为1作cumsum。

```python
def gen_session_fea(file):
    tmp = file.groupby('user')['timestamp'].shift()
    file['delta_time'] = file['timestamp'] - tmp
    file['session'] = np.where(file['delta_time'] > 600, 1, 0)
    file['session'] = file.groupby(['user'])['session'].transform('cumsum')
    del file['delta_time']

    return file
```

怎么解释呢？当相邻的操作或交易在一定的时间范围内完成时，就认为是一个session，当很多操作在相邻的时间内完成，是否可以看成一种行为模式，用cumsum来累积？另外一个点就是pandas中的`groupby`后作shift的操作，可以对一个组内的某个变量进行偏移操作。

3.一些针对op_df和trans_df的交叉特征

```python
def gen_fea_op_df(op_df):
    op_df['op_pattern'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str)
    op_df['op_type_mode'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str)
    op_df['op_type_device'] = op_df['op_type'].map(str) + '_' + op_df['op_device'].map(str)
    op_df['op_mode_device'] = op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str)
    op_df['ip_net_type'] = op_df['ip'].map(str) + '_' + op_df['net_type'].map(str)
    op_df['ip3_net_type'] = op_df['ip_3'].map(str) + '_' + op_df['net_type'].map(str)
    op_df['net_type_channel'] = op_df['net_type'].map(str) + '_' +  op_df['channel'].map(str)
   # op_df['time_diff'] = op_df['timestamp'].diff(-1)
    op_df.rename(columns={'ip' : 'op_ip', 'ip_3': 'op_ip_3',}, inplace=True)
    return op_df
```

可以看出来的一点是，对于交叉特诊，作者也是将相关的一些特征进行组合。自己当时只做了一列列别数量较少的特征的交叉，在这里可以看到作者对ip等很多类别的特征也进行了交叉。

```python
def gen_fea_trans_df(trans_df):
    trans_df['tunnel_io'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['tunnel_out'].astype(str)
    trans_df['type'] = trans_df['type1'].astype(str) + '_' +trans_df['type2'].astype(str)
    trans_df['tunnel_io_type'] = trans_df['tunnel_io'].astype(str) + '_' + trans_df['type'].astype(str)
    trans_df['platform_tunnel_io_type'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io_type']
    trans_df['platform_tunnel_io'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io']
    trans_df['platform_type'] = trans_df['platform'].astype(str) + '_' + trans_df['type']
    trans_df['platform_amount'] = trans_df['platform'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['type_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['tunnel_io_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['type1_amount'] = trans_df['type1'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['type2_amount'] = trans_df['type2'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['tunnel_in_amount'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['tunnel_out_amount'] = trans_df['tunnel_out'].astype(str) + '_' + trans_df['amount'].astype(str)
    trans_df['amount_diff'] = trans_df['amount'].astype(int).diff(-1)
    trans_df['time_diff'] = trans_df['timestamp'].diff(-1)
    trans_df['amount_per_time'] = trans_df['amount_diff'] / np.where(trans_df['time_diff'] == 0, 0.01, trans_df['time_diff'])
    trans_df = gen_session_fea(trans_df)
```

比较有看点的是对amount做了`diff`操作，计算相邻两次交易的差额，并计算了相邻两次交易的时间，两者相除得到交易额与时间差的比值。

3.base_df交叉特征。对于base_df里的数值特征作加减乘除操作。

```python
def int_cols_cross(df, cols):
    """[summary]
        对base数据的int64特征进行min-max归一化后进行加减乘除交互
    Parameters
    ----------
    df : [DataFrame]
        [训练集和测试集合并的数据]
    cols : [list]
        [交互特征]

    Returns
    -------
    [DataFrame, list]
        [整数特征交互后的data, 及交互特征名称]
    """
    cross_feature = []
    df = df.copy()
    for i, col in tqdm(enumerate(cols), desc='extract cross feature for base'):
        for j in range(i + 1, len(cols)):
            df[col + '_' + 'div_' + cols[j]] = min_max_unif(df[col]) / min_max_unif(df[cols[j]])
            df[col + '_' + 'sub_' + cols[j]] = min_max_unif(df[col]) - min_max_unif(df[cols[j]])
            df[col + '_' + 'mul_' + cols[j]] = min_max_unif(df[col]) * min_max_unif(df[cols[j]])
            df[col + '_' + 'sum_' + cols[j]] = min_max_unif(df[col]) + min_max_unif(df[cols[j]])

            cross_feature.append(col + '_' + 'div_' + cols[j])
            cross_feature.append(col + '_' + 'sub_' + cols[j])
            cross_feature.append(col + '_' + 'mul_' + cols[j])
            cross_feature.append(col + '_' + 'sum_' + cols[j])

    return df, cross_feature
```

这个感觉很强呀，加减乘除都用到数值特征上。

3.count计数特征

```python
def gen_cnt_feature(df, feature):
    cnt_features = []
    for fea in feature:
        df[fea + '_count'] = df.groupby([fea])['user'].transform('count')
        cnt_features.append(fea + '_count')

    return df
```

但是为什么只对`cnt_feature = ['city', 'province', 'balance', 'ip_cnt', 'using_time', ]`这些特征作计数特征就不知道是为什么了，难道也是一个个试出来的嘛？

4.trans_df的amount统计特征

```python
def gen_user_amount_features(df):
    group_df = df.groupby(['user'])['amount'].agg({
        'user_amount_mean': 'mean',
       # 'user_amount_std': 'std',
        'user_amount_max': 'max',
        'user_amount_min': 'min',
        'user_amount_sum': 'sum',
        'user_amount_med': 'median',
        'user_amount_cnt': 'count',
        # 'user_amount_q1': lambda x: x.quantile(0.25),
        # 'user_amount_q3': lambda x: x.quantile(0.75),
        #'user_amount_qsub': lambda x: x.quantile(0.75) - x.quantile(0.25)
        #'user_amount_skew': 'skew',
        }).reset_index()
    return group_df
```

这个没什么好说的，肯定得做

5.nunique特征

```python
def gen_user_nunique_features(df, value, prefix):
    group_df = df.groupby(['user'])[value].agg({
        'user_{}_{}_nuniq'.format(prefix, value): 'nunique'
    }).reset_index()
    return group_df
```

计算一个用户在某个字段上有几个unique值.

trans_df:`['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', ]`

6.一个值有几个用户的nunique统计结果

首先计算一个值有几个用户

```python
def file_cols_user_nunique(file, feature_lst, prefix):
    col_nuniq_fea_lst = []
    for col in tqdm(feature_lst):
        col_nuniq = file.groupby(col)['user'].nunique()
        col_nuniq_dic = dict(zip(col_nuniq.index, col_nuniq.values))
        file[prefix + '_' + col + '_user_nuniq'] = file[col].map(col_nuniq_dic)
        col_nuniq_fea_lst.append(prefix + '_' + col + '_user_nuniq')

    return file, col_nuniq_fea_lst
```

在对上面的计算值作统计计算

```python
def gen_stastic_col_user_nunique(file, feat, prefix):
    group_df = file.groupby('user')[feat].agg({
        prefix + feat + '_mean': 'mean',
        prefix + feat + '_std': 'std',
        prefix + feat + '_max': 'max',
        prefix + feat + '_min': 'min',
        prefix + feat + '_sum': 'sum',
        prefix + feat + '_med': 'median',
        #prefix + feat + '_q1'  : lambda x: x.quantile(0.25),
        #prefix + feat + '_q3'  : lambda x: x.quantile(0.75),
        #prefix + feat + 'q_sub': lambda x: x.quantile(0.75) - x.quantile(0.25),
        #prefix + feat + '_skew': 'skew',
    })
    return group_df
```

这个在trans_df、op_df、trans_op_df都可以做。

7.利用pivot_table计算在各个值类别上交易的聚合

```python
def gen_user_group_amount_features(df, value):
    group_df = df.pivot_table(index='user',
                              columns=value,
                              values='amount',
                              dropna=False,
                              aggfunc=['count', 'sum',
                                       'mean', 'max', 'min', 'median',
                                       ]).fillna(0)
    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]
    group_df.reset_index(inplace=True)

    return group_df
```

这种特征构造方式在类别不是特别多的时候比较适用。

8.计算不同时间窗口下用户交易额统计信息

```python
def gen_user_window_amount_features(df, window):
    group_df = df[df['days_diff']>window].groupby('user')['amount'].agg({
        'user_amount_mean_{}d'.format(window): 'mean',
        'user_amount_std_{}d'.format(window): 'std',
        'user_amount_max_{}d'.format(window): 'max',
        'user_amount_min_{}d'.format(window): 'min',
        'user_amount_sum_{}d'.format(window): 'sum',
        'user_amount_med_{}d'.format(window): 'median',
        'user_amount_cnt_{}d'.format(window): 'count',
        # 'user_amount_q1_{}d'.format(window): lambda x: x.quantile(0.25),
        # 'user_amount_q3_{}d'.format(window): lambda x: x.quantile(0.75),
        # 'user_amount_qsub_{}d'.format(window): lambda x: x.quantile(0.75) - x.quantile(0.25),
        # 'user_amount_skew_{}d'.format(window): 'skew',
        # 'user_amount_q4_{}d'.format(window): lambda x: x.quantile(0.8),
        # 'user_amount_q5_{}d'.format(window): lambda x: x.quantile(0.3),
        # 'user_amount_q6_{}d'.format(window): lambda x: x.quantile(0.7),
        }).reset_index()
    return group_df
```

9.空值特征

```python
def gen_user_null_features(df, value, prefix):
    df['is_null'] = 0
    df.loc[df[value].isnull(), 'is_null'] = 1

    group_df = df.groupby(['user'])['is_null'].agg({'user_{}_{}_null_cnt'.format(prefix, value): 'sum',
                                                    'user_{}_{}_null_ratio'.format(prefix, value): 'mean'}).reset_index()
    return group_df
```

10.统计一些特殊值上时间的聚合特征

```python
def gen_file_type_days_diff(df, file, type, time_feat):
    plot_feats = []
    #file_type_unique = file[type].value_counts().index.tolist()
    file_type_unique = []

    if type == 'type1':
        file_type_unique = ['45a1168437c708ff',
                             'f67d4b5a05a1352a', 
                             ]
    elif type == 'type2':
        file_type_unique = ['11a213398ee0c623',]

    elif type == 'channel':
        file_type_unique = ['b2e7fa260df4998d',
                            '116a2503b987ea81',
                            '8adb3dcfea9dcf5e']

    elif type == 'tunnel_io':
        file_type_unique = ['b2e7fa260df4998d_6ee790756007e69a',]
    elif type == 'type':
        file_type_unique = ['f67d4b5a05a1352a_nan',
                            '19d44f1a51919482_11a213398ee0c623',
                            '45a1168437c708ff_11a213398ee0c623',
                            '674e8d5860bc033d_11a213398ee0c623',
                            '0a3cf8dac7dca9d1_b5a8be737a50b171']

    for tp in file_type_unique:
        assert file_type_unique != []
        group_df = file[file[type] == tp].groupby(['user'])[time_feat].agg(
            {'user_{}_{}_min_{}'.format(type, tp, time_feat): 'min',
             'user_{}_{}_mean_{}'.format(type, tp, time_feat): 'mean',
             'user_{}_{}_max_{}'.format(type, tp, time_feat): 'max',
             'user_{}_{}_std_{}'.format(type, tp, time_feat): 'std',
             'user_{}_{}_median_{}'.format(type, tp, time_feat): 'median',
             'user_{}_{}_sum_{}'.format(type, tp, time_feat): 'sum',
             # 'user_{}_{}_q1_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.25),
             # 'user_{}_{}_q3_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75),
             # 'user_{}_{}_q_sub_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75) - x.quantile(0.25),
             # 'user_{}_{}_skew_{}'.format(type, tp, time_feat): 'skew',
             }).reset_index()
        df = df.merge(group_df, on=['user'], how='left')
        stastic = ['min', 'max', 'max', 'std', 'median', 'sum',]
        for stast in stastic:
            plot_feats.append('user_{}_{}_{}_{}'.format(type, tp, stast, time_feat))

    return df, plot_feats
```

这个特殊值的选择作者也没有提到，应该是要根据一些画图分析得到。

11.doc2vec特征

```python
def d2v_feat(df, feat, length, num):
    print('Start training Doc2Vec models.......')
    df[feat] = df[feat].astype(str)
    group_df = df.groupby(['user'])[feat].agg(list).reset_index()
    documents = [TaggedDocument(doc, [i]) for i, doc in zip(group_df['user'].values, group_df[feat])]
    model = Doc2Vec(documents, vector_size=length, window=10, min_count=1, workers=1, seed=2020,
                    epochs=20,  hs=1, )
    if not os.path.exists('./d2v_models/'):
        os.makedirs('./d2v_models/')
    model.save('../d2v_models/d2v_testb_{}.model'.format(num))
    # model = Doc2Vec.load('./d2v_models/d2v_testb_{}.model'.format(num))
    doc_df = group_df['user'].apply(lambda x: ','.join([str(i) for i in model[x]])).str.split(',', expand=True).apply(pd.to_numeric)
    doc_df.columns = ['{}_d2v_{}'.format(feat, i) for i in range(length)]

    return pd.concat([group_df[['user']], doc_df], axis=1)
```

em...,看懂代码就行啦。用在trans_df的amount上。自己也做了这个特征，还用在了其他的字段序列上，感觉也不能无脑对所有字段都用，还是要根据实验结果有选择的用。

12.word2vec特征

```python
def w2v_feat(df, feat, length, num):
    """

    :param df:         进行word2vec编码的数据
    :param feat:       进行编码的特征
    :param length:     embedding向量长度
    :return:
    """
    global w2v_fea_lst
    w2v_fea_lst = []
    print('Start training Word2Vec models.....')
    df[feat] = df[feat].astype(str)
    group_df = df.groupby(['user'])[feat].agg(list).reset_index()
    model = Word2Vec(group_df[feat].values, size=length, window=10, min_count=1, sg=1, hs=1, workers=1,
                     iter=20, seed=2020,)
    # if feat == 'amount':
    #     model = Word2Vec.load('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num))
    # elif feat == 'channel':
    #     model = Word2Vec.load('../w2v_models/w2v_channel_16.model')
    # elif feat == 'trans_op_ip' or feat == 'trans_op_ip_3':
    #     model = Word2Vec.load('../w2v_models/w2v_trans_op_2.model')
    # else:
    #     if not os.path.exists('../w2v_models/'):
    #         os.makedirs('../w2v_models/')
    # model = Word2Vec.load('./w2v_models/w2v_testb_{}_{}.model'.format(feat, num))
    model.save('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num))

    group_df[feat] = group_df[feat].apply(lambda x: pd.DataFrame([model[c] for c in x]))
    for m in tqdm(range(length), desc='extract w2v {} statistic feature'.format(feat)):
        group_df['{}_w2v_{}_mean'.format(feat,m)] = group_df[feat].apply(lambda x: x[m].mean())
        # group_df['{}_w2v_{}_median'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].median())
        # group_df['{}_w2v_{}_max'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].max())
        # group_df['{}_w2v_{}_min'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].min())
        # group_df['{}_w2v_{}_sum'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].sum())
        # group_df['{}_w2v_{}_std'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].std())


        w2v_fea_lst.append('{}_w2v_{}_mean'.format(feat,m))
    del group_df[feat]

    return group_df
```

用在trans_df的amount上

13.tf_idf特征

```python
def gen_user_tfidf_features(df, value,):
    print('Start tfdif encoding for {}........'.format(value))
    df[value] = df[value].astype(str)
    df[value].fillna('-1', inplace=True)
    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()
    group_df.columns = ['user', 'list']
    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))
    enc_vec = TfidfVectorizer()
    tfidf_vec = enc_vec.fit_transform(group_df['list'])
    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)
    vec_svd = svd_enc.fit_transform(tfidf_vec)
    vec_svd = pd.DataFrame(vec_svd)
    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]
    group_df = pd.concat([group_df, vec_svd], axis=1)
    del group_df['list']
    return group_df
```

14.countvec特征

```python
def gen_user_countvec_features(df, value,):
    print('Start countvec encoding for {}........'.format(value))
    df[value] = df[value].astype(str)
    df[value].fillna('-1', inplace=True)
    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()
    group_df.columns = ['user', 'list']
    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))
    enc_vec = CountVectorizer()
    tfidf_vec = enc_vec.fit_transform(group_df['list'])
    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)
    vec_svd = svd_enc.fit_transform(tfidf_vec)
    vec_svd = pd.DataFrame(vec_svd)
    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]
    group_df = pd.concat([group_df, vec_svd], axis=1)
    del group_df['list']
    return group_df
```

15.计算各个用户在op_df和trans_df上在一些字段上的统计特征

```python
def gen_user_group_trans_op_features(df, columns, value):
    group_df = df.pivot_table(index='user',
                              columns=columns,
                              values=value,
                              dropna=False,
                              aggfunc=['count', 'sum',
                                       'mean', 'max', 'min', 'median',
                                       ]).fillna(0)
    group_df.columns = ['user_{}_{}_{}_{}'.format(columns, f[1], value, f[0]) for f in group_df.columns]
    group_df['op_trans_ratio'] = group_df['user_property_trans_{}_count'.format(value)] / group_df[
        'user_property_op_{}_count'.format(value)]

    group_df.reset_index(inplace=True)

    return group_df
```

16.获取各个用户在第一次和最后一次操作或交易时的特征

```python
def user_trans_behavior_feature(df, trans_op):
    print("Starting extract user's trans behavior......")

    # 获取第一次和最后一次行为
    group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[-1]).to_dict()
    df['last_beahvior'] = df['user'].map(group_dic)
    group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[0]).to_dict()
    df['first_beahvior'] = df['user'].map(group_dic)
    # 是否有过交易行为
    group_dic = trans_op.groupby('user').apply(lambda x: judge_has_trans(x)).to_dict()
    df['has_trans'] = df['user'].map(group_dic)
    # 最后一次交易days_diff
    group_dic = trans_op.groupby('user').apply(lambda x: last_trans_time(x)).to_dict()
    df['last_days_diff_trans'] = df['user'].map(group_dic)
    # 第一次交易days_diff
    group_dic = trans_op.groupby('user').apply(lambda x: first_trans_time(x)).to_dict()
    df['first_days_diff_trans'] = df['user'].map(group_dic)
    # 最后一次交易hour
    group_dic = trans_op.groupby('user').apply(lambda x: last_trans_hour(x)).to_dict()
    df['last_hour_trans'] = df['user'].map(group_dic)
    # 第一次交易hour
    group_dic = trans_op.groupby('user').apply(lambda x: first_trans_hour(x)).to_dict()
    df['first_hour_trans'] = df['user'].map(group_dic)
    # 最后一次交易week
    group_dic = trans_op.groupby('user').apply(lambda x: last_trans_week(x)).to_dict()
    df['last_week_trans'] = df['user'].map(group_dic)
    # 第一次交易week
    group_dic = trans_op.groupby('user').apply(lambda x: first_trans_week(x)).to_dict()
    df['first_week_trans'] = df['user'].map(group_dic)
    # 最后一次交易timestamp
    group_dic = trans_op.groupby('user').apply(lambda x: last_trans_timestamp(x)).to_dict()
    df['last_time_trans'] = df['user'].map(group_dic)
    # 第一次交易timestamp
    group_dic = trans_op.groupby('user').apply(lambda x: first_trans_timestamp(x)).to_dict()
    df['first_time_trans'] = df['user'].map(group_dic)
    # 平均交易次数
    group_dic = trans_op.groupby('user').apply(lambda x: gen_trans_count(x)).to_dict()
    df['trans_count'] = df['user'].map(group_dic)
    # 操作次数
    group_dic = trans_op.groupby('user').apply(lambda x: gen_op_count(x)).to_dict()
    df['op_count'] = df['user'].map(group_dic)

    return df
```

17.判断用户是否有交易

```python
data['has_trans'] = pd.factorize(data['has_trans'])[0]
```

二值特征用一列01就可以了

18.类别编码

```python
def kfold_stats_feature(train, test, feats, k, seed):
    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)  # 这里最好和后面模型的K折交叉验证保持一致

    train['fold'] = None
    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
        train.loc[val_idx, 'fold'] = fold_

    kfold_features = []
    for feat in tqdm(feats, desc='Target encoding for base feature'):
        nums_columns = ['label']
        for f in nums_columns:
            colname = feat + '_' + f + '_kfold_mean'
            kfold_features.append(colname)
            train[colname] = None
            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
                tmp_trn = train.iloc[trn_idx]
                order_label = tmp_trn.groupby([feat])[f].mean()
                tmp = train.loc[train.fold == fold_, [feat]]
                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)
                # fillna
                global_mean = tmp_trn[f].mean()
                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)
            train[colname] = train[colname].astype(float)

        for f in nums_columns:
            colname = feat + '_' + f + '_kfold_mean'
            test[colname] = None
            order_label = train.groupby([feat])[f].mean()
            test[colname] = test[feat].map(order_label)
            # fillna
            global_mean = train[f].mean()
            test[colname] = test[colname].fillna(global_mean)
            test[colname] = test[colname].astype(float)
    del train['fold']
    return train, test
```

对一些值类别比较多的变量作target encoding

19.对序列变量作target_encoding

```python
def target_encoding(file, train, test, feats, k, prefix, not_adp=True, agg_lst=['mean'], seed=2020):
    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)  # 这里最好和后面模型的K折交叉验证保持一致

    train['fold'] = None
    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
        train.loc[val_idx, 'fold'] = fold_

    tt_file = train[['user', 'label']].merge(file, on='user', how='left')

    te_features = []
    for feat in tqdm(feats, desc='Target encoding for {} feature '.format(prefix)):
        col_name = feat + '_te'
        # te_features.append(col_name)
        for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):
            tmp_users = train.iloc[trn_idx]['user'].values
            tmp_file = file[file.user.isin(tmp_users)]
            tmp_file = tmp_file.merge(train[['user', 'label']], on='user', how='left')

            if not_adp:
                match = tmp_file.groupby(feat)['label'].mean()
            else:
                match = tmp_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index()
                match['eu_mean'] = match['eu_sum'] / match['eu_count']
                match = match['eu_mean'].groupby(match[feat]).mean()

            tmp_users = train.iloc[val_idx]['user'].values
            tmp_file = file[file.user.isin(tmp_users)]
            tmp_file[col_name] = tmp_file[feat].map(match)
            for agg_ in agg_lst:
                tmp = tmp_file.groupby('user')[col_name].agg(agg_)
                train.loc[train.fold == fold_, col_name + '_' + agg_] = train.loc[train.fold == fold_, 'user'].map(tmp)

        if not_adp:
            match = tt_file.groupby(feat)['label'].mean()
        else:
            match = tt_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index()
            match['eu_mean'] = match['eu_sum'] / match['eu_count']
            match = match['eu_mean'].groupby(match[feat]).mean()

        tmp_file = file[file.user.isin(test['user'].values)]
        tmp_file[col_name] = tmp_file[feat].map(match)
        for agg_ in agg_lst:
            tmp = tmp_file.groupby('user')[col_name].agg(agg_)
            test[col_name + '_' + agg_] = test['user'].map(tmp)


    del train['fold']
    gc.collect()
    # print(train[te_features])

    return train, test
```

首先要把序列merge上label再进行编码聚合后输出单行的编码结果

20.有序变量编码

```python
def order_encode(df, col):
    """

    :param df:      Dataframe
    :param col:     feature
    :description:   对有序类别变量顺序编码
    :return:
    """
    df.loc[df[col].notnull(), col] = df.loc[df[col].notnull(), col].apply(lambda x: str(x).split(' ')[1]).astype(int)
    df[col] = df[col].fillna(-1).astype(int)

    return df
```

对于一些有序的变量要还原为对应数值以捕获对应的大小关系。

21.无序变量编码

```python
def label_encode(df, order_cols):
    # LabelEncoder
    cat_cols = [f for f in df.select_dtypes('object').columns if f not in ['user'] + order_cols]
    for col in cat_cols:
        le = LabelEncoder()
        df[col].fillna('-1', inplace=True)
        df[col] = le.fit_transform(df[col])
        #cat_cols.append(col)
    return  df
```

22.风险值较高的省份二值化编码

```python
def province_binary(df, ):
    """[summary]
        对风险率排名最高的五个省份进行二值化及组合编码
    Parameters
    ----------
    df : [data数据]
        [train or test]

    Returns
    -------
    [DataFrame]
        [经过省份二值化编码的训练集/测试集]
    """
    #省份二值化编码
    df['is_21_province'] = df.apply(lambda x: 1 if x.province == 21 else 0, axis=1)
    df['is_26_province'] = df.apply(lambda x: 1 if x.province == 26 else 0, axis=1)
    df['is_30_province'] = df.apply(lambda x: 1 if x.province == 30 else 0, axis=1)
    df['is_20_province'] = df.apply(lambda x: 1 if x.province == 20 else 0, axis=1)
    df['is_16_province'] = df.apply(lambda x: 1 if x.province == 16 else 0, axis=1)

    df['binary_province'] = df['is_21_province'].map(str) + df['is_26_province'].map(str) + \
                               df['is_30_province'].map(str) + df['is_20_province'].map(str) + df['is_16_province'].map(str)

    le = LabelEncoder()
    df['binary_province'].fillna('-1', inplace=True)
    df['binary_province'] = le.fit_transform(df['binary_province'])

    return df
```

23.交易操作比例

```python
train_data['trans_ratio'] = train_data['trans_count'] / train_data['op_count']
test_data['trans_ratio'] = test_data['trans_count'] / test_data['op_count']
```

到这里，作者用到的各种编码方式就介绍结束了。其实，自己已经用到了大多数编码方式，不多自己用的比较无脑，对可以用的变量自己都用了。虽热也有一些特征选择的方法，但好像并不是那么实用，问了一些大佬，推荐的做法是做一组或一个特征就实验一下，结果提升了就保留。对于一些类别变量，lgb不用onehot变量。

### top2

top2的很多特征编码方式和top1相近，这里主要介绍其特殊的特征处理方式。
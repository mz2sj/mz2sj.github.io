{"meta":{"title":"微风和暖","subtitle":"","description":"","author":"mz2sj","url":"https://mz2sj.github.io","root":"/"},"pages":[],"posts":[{"title":"Hello World","slug":"hello-world","date":"2023-01-08T06:30:22.504Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2023/01/08/hello-world/","link":"","permalink":"https://mz2sj.github.io/2023/01/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"决策树","slug":"决策树","date":"2021-06-08T12:57:19.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2021/06/08/决策树/","link":"","permalink":"https://mz2sj.github.io/2021/06/08/%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"","text":"决策树学习通常包括3个步骤：特征选择、决策树的生成、决策树的剪枝。 ID3 ID3通过信息增益来选择特征和分裂点，信息增益由信息熵和条件熵计算得出。 熵的公式计算公式如下： H(D)=−∑k=1K∣Ck∣∣D∣log⁡2∣Ck∣∣D∣\\begin{array}{l} H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log _{2} \\frac{ \\mid C_{k} \\mid}{|D|} \\\\\\end{array}H(D)=−∑k=1K​∣D∣∣Ck​∣​log2​∣D∣∣Ck​∣​​ 当样本中各类别数据概率相等时，熵最大。换种方式理解，拿出一个样本，它是各个类别的概率都是一样的，此时最难分辨样本的类别。熵越大样本的不确定性越高。 条件熵计算公式如下： H(D∣A)=∑i=1n∣Di∣∣D∣H(Di)=−∑i=1n∣Di∣∣D∣∣∑k=1k∣Di∣∣∣Di∣log⁡2∣Dik∣∣Di∣H(D \\mid A)=\\sum_{i=1}^{n} \\frac{\\mid D_{i} \\mid}{\\mid D \\mid} H\\left(D_{i}\\right)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D| \\mid} \\sum_{k=1}^{k} \\frac{\\left|D_{i \\mid}\\right|}{\\mid D_{i \\mid}} \\log _{2} \\frac{\\mid D_{i k} \\mid}{\\mid D_{i \\mid}} \\\\H(D∣A)=∑i=1n​∣D∣∣Di​∣​H(Di​)=−∑i=1n​∣D∣∣∣Di​∣​∑k=1k​∣Di∣​∣Di∣​∣​log2​∣Di∣​∣Dik​∣​ 其中iii代表AAA特征有几个类别，即A特征的基数。条件熵是在某特征各个类别值出现概率的基础上计算熵。 熵减去条件熵就是某个特征的信息增益： $ g(D, A)=H(D)-H(D \\mid A)$ ID3算法通过信息增益来选择特征：信息增益大的特征 具有更强的分类能力。如果一个特征的信息增益为0，即表示该特征没有什么分类能力。 ID3算法步骤简化理解： 1.计算各个特征的信息增益AgA_gAg​,选择最大的AgA_gAg​值对应的特征。 2.如果AgA_gAg​小于阈值ϵ\\epsilonϵ,则该特征分类点下面所有样本实例数最多的类别就是该节点的类标记。 3.否则，对于AgA_gAg​中的各个类别值，分裂成各个子节点。 4.迭代进行上面3步，直到信息增益均小于阈值ϵ\\epsilonϵ或者没有特征可以选择为止。 缺点： 1.ID3没有剪枝操作，只有输的生成，所以容易过拟合。 2.信息增益准则对可取值数目较多的属性有所偏好。 3.没有考虑缺失值的处理。 C4.5 C4.5为了克服ID3对高基数特征有所偏好的特点，引入了信息增益比来选择特征，信息增益比计算公式如下： gR(D,A)=g(D,A)HA(D) g_{R}(D, A)=\\frac{g(D, A)}{H_{A}(D)} gR​(D,A)=HA​(D)g(D,A)​ HA(D)=−∑i=1n∣Di∣∣D∣log⁡2∣Di∣∣D∣H_{A}(D)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} \\log _{2} \\frac{\\left|D_{i}\\right|}{|D|}HA​(D)=−∑i=1n​∣D∣∣Di​∣​log2​∣D∣∣Di​∣​ 信息增益比其实就是某个特征的信息增益与将某个特征类别的信息熵的比，信息增益比倾向于基数低的特征。 C4.5的决策树生成与ID3相同，引入了剪枝策略，包括预剪枝和后剪枝。 预剪枝 在节点划分前来确定是否继续增长，及早停止增长的主要方法有： 节点内数据样本低于某一阈值； 所有节点特征都已分裂； 节点划分前准确率比划分后准确率高。 预剪枝可能会带来欠拟合问题。 后剪枝 C4.5 采用的悲观剪枝方法，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。 C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。 缺点： 剪枝策略可以再优化； C4.5 用的是多叉树，用二叉树效率更高； C4.5 只能用于分类； C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算； C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行 缺点还要再进行理解。 CART CART回归树 回归树表示 一棵 CART 回归树对应着输入空间的一个划分，以及在划分单元上的输出值。 设输出 y 为连续变量, 训练数据集 $ \\mathbb{D}=\\left{\\left(\\overrightarrow{\\mathbf{x}}{1}, \\tilde{y}{1}\\right),\\left(\\overrightarrow{\\mathbf{x}}{2}, \\tilde{y}{2}\\right), \\cdots,\\left(\\overrightarrow{\\mathbf{x}}{N}, \\tilde{y}{N}\\right)\\right} $ 设已经将输入空间划分为 M 个单元 $ R_{1}, R_{2}, \\cdots, R_{M} $, 且在每个单元 R_{m} 上有一个固定的输出值 cmc_{m}cm​ 则 CART 回归树模型可以表示为: f(x→)=∑m=1McmI(x→∈Rm)f(\\overrightarrow{\\mathbf{x}})=\\sum_{m=1}^{M} c_{m} I\\left(\\overrightarrow{\\mathbf{x}} \\in R_{m}\\right)f(x)=∑m=1M​cm​I(x∈Rm​) 其中$ I(\\cdot) $ 为示性函数。 回归树损失函数 如果已知输入空间的单元划分，基于平方误差最小的准则，则 CART 回归树在训练数据集上的损失函数为: ∑m=1M∑x→i∈Rm(y~i−cm)2\\sum_{m=1}^{M} \\sum_{\\overrightarrow{\\mathbf{x}}_{i} \\in R_{m}}\\left(\\tilde{y}_{i}-c_{m}\\right)^{2}∑m=1M​∑xi​∈Rm​​(y~​i​−cm​)2 根据损失函数最小, 则可以求解出每个单元上的最优输出值$ \\hat{c}{m} $ 为 😒 R{m} $上所有输入样本 $\\overrightarrow{\\mathbf{x}}{i} $ 对应的输出 $ \\tilde{y}{i} $ 的平均值。 即: $\\hat{c}{m}=\\frac{1}{N{m}} \\sum_{\\overrightarrow{\\mathbf{x}}{i} \\in R{m}} \\tilde{y}{i} $, 其中 $N{m} $ 表示单元 $R_{m} $ 中的样本数量。 如何选择切分点进行划分 设输入为 nnn 维:$ \\overrightarrow{\\mathbf{x}}=\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)^{T}$ 。 1.选择第 jjj 维 $ x_{j} $ 和它的取值 sss 作为切分变量和切分点。定义两个区域: R1(j,s)={x→∣xj≤s}R2(j,s)={x→∣xj&gt;s}\\begin{array}{l} R_{1}(j, s)=\\left\\{\\overrightarrow{\\mathbf{x}} \\mid x_{j} \\leq s\\right\\} \\\\ R_{2}(j, s)=\\left\\{\\overrightarrow{\\mathbf{x}} \\mid x_{j}&gt;s\\right\\} \\end{array}R1​(j,s)={x∣xj​≤s}R2​(j,s)={x∣xj​&gt;s}​ 2.然后寻求最优切分变量 jjj 和最优切分点 sss 。即求解: (j∗,s∗)=min⁡j,s[min⁡c1∑x⃗i∈R1(j,s)(y~i−c1)2+min⁡c2∑x⃗i∈R2(j,s)(y~i−c2)2]\\left(j^{*}, s^{*}\\right)=\\min _{j, s}\\left[\\min _{c_{1}} \\sum_{\\vec{x}_{i} \\in R_{1}(j, s)}\\left(\\tilde{y}_{i}-c_{1}\\right)^{2}+\\min _{c_{2}} \\sum_{\\vec{x}_{i} \\in R_{2}(j, s)}\\left(\\tilde{y}_{i}-c_{2}\\right)^{2}\\right](j∗,s∗)=minj,s​[minc1​​∑xi​∈R1​(j,s)​(y~​i​−c1​)2+minc2​​∑xi​∈R2​(j,s)​(y~​i​−c2​)2] 其意义为： 首先假设已知切分变量 jjj , 则遍历最优切分点 s1s_{1}s1​ 则到: c^1=1N1∑x→i∈R1(j,s)y~i,c^2=1N2∑x⃗i∈R2(j,s)y~i\\hat{c}_{1}=\\frac{1}{N_{1}} \\sum_{\\overrightarrow{\\mathbf{x}}_{i} \\in R_{1}(j, s)} \\tilde{y}_{i}, \\quad \\hat{c}_{2}=\\frac{1}{N_{2}} \\sum_{\\vec{x}_{i} \\in R_{2}(j, s)} \\tilde{y}_{i}c^1​=N1​1​∑xi​∈R1​(j,s)​y~​i​,c^2​=N2​1​∑xi​∈R2​(j,s)​y~​i​ 其中 N1N_{1}N1​ 和 $ N_{2} $ 分别代表区域 R1R_{1}R1​ 和 $ R_{2}$ 中的样本数量。 然后遍历所有的特征维度, 对每个维度找到最优切分点。从这些 (切分维度,最优切分点) 中找到使但损失函数最小的那个。 3.依次将输入空间划分为两个区域，然后重复对子区域划分，直到满足停止条件为止。这样的回归树称为最小二乘回归树。 CART分类树生成 cart分类树采用基尼指数选择最优特征。 Gini⁡(p)=∑k=1Kpk(1−pk)=1−∑k=1Kpk2\\operatorname{Gini}(p)=\\sum_{k=1}^{K} p_{k}\\left(1-p_{k}\\right)=1-\\sum_{k=1}^{K} p_{k}^{2}Gini(p)=∑k=1K​pk​(1−pk​)=1−∑k=1K​pk2​ 对于二分类问题,基尼指数为 Gini(p)=p(1−p)+(1−p)(1−(1−p))=2p(1−p)Gini(p)=p(1-p)+(1-p)(1-(1-p))=2p(1-p)Gini(p)=p(1−p)+(1−p)(1−(1−p))=2p(1−p) 如果样本集合D根据特征A的某一值a分为D1D_1D1​和D2D_2D2​两部分，则在特征A的条件下，基尼指数定义为 Gini⁡(D∣A)=∣D1∣∣D∣Gini⁡(D1)+∣D2∣∣D∣Gini⁡(D2)\\operatorname{Gini}(D \\mid A)=\\frac{\\left|D_{1}\\right|}{|D|} \\operatorname{Gini}\\left(D_{1}\\right)+\\frac{\\left|D_{2}\\right|}{|D|} \\operatorname{Gini}\\left(D_{2}\\right)Gini(D∣A)=∣D∣∣D1​∣​Gini(D1​)+∣D∣∣D2​∣​Gini(D2​) cart分类树生成过程与回归树类似， 遍历所有可能的维度jjj 和该维度所有可能的取值 sss，取使得基尼系数最小的那个维度 jjj和切分点sss 。 划分区域中样本比例高的类别即是该样本区域对应的类别。 cart 分类树和cart 回归树通常的停止条件为： 结点中样本个数小于预定值，这表示树已经太复杂。 样本集的损失函数或者基尼指数小于预定值，表示结点已经非常纯净。 没有更多的特征可供切分。 CART剪枝 CART 树的剪枝是从完全生长的CART 树底端减去一些子树，使得CART 树变小（即模型变简单），从而使得它对未知数据有更好的预测能力。CART剪枝算法分为两步：首先从生成算法的决策树T0T_0T0​不算剪枝，形成一个子树序列T0,T1,...,Tn{T_0,T_1,...,T_n}T0​,T1​,...,Tn​,然后通过交叉验证法在独立的验证数据集上对子树序列进行预测，从中选择最优子树。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"算法","slug":"数据分析/算法","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"机器学习","slug":"机器学习","permalink":"https://mz2sj.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Regression","slug":"Regression","date":"2021-04-06T13:32:09.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2021/04/06/Regression/","link":"","permalink":"https://mz2sj.github.io/2021/04/06/Regression/","excerpt":"","text":"em…,我又来立flag了，之前买了《统计学习方法》这本书，本以为会发奋读书，没想到也是半途而废。现在想想还是得学点算法类的知识， 求求你了，孟镇，做个人吧，能写出来嘛？ 求求你，抄别人的也抄出来吧！ 线性回归 数据集： 假设数据集为: D={(x1,y1),(x2,y2),⋯ ,(xN,yN)}\\mathcal{D}=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}D={(x1​,y1​),(x2​,y2​),⋯,(xN​,yN​)} 后面我们记: X=(x1,x2,⋯ ,xN)T,Y=(y1,y2,⋯ ,yN)TX=\\left(x_{1}, x_{2}, \\cdots, x_{N}\\right)^{T}, Y=\\left(y_{1}, y_{2}, \\cdots, y_{N}\\right)^{T}X=(x1​,x2​,⋯,xN​)T,Y=(y1​,y2​,⋯,yN​)T 线性回归假设: f(w)=wTxf(w)=w^{T} xf(w)=wTx 最小二乘法 对这个问题, 采用二范数定义的平方误差来定义损失函数: L(w)=∑i=1N∥wTxi−yi∥22L(w)=\\sum_{i=1}^{N}\\left\\|w^{T} x_{i}-y_{i}\\right\\|_{2}^{2}L(w)=∑i=1N​∥∥​wTxi​−yi​∥∥​22​ 展开得到： L(w)=(wTx1−y1,⋯ ,wTxN−yN)⋅(wTx1−y1,⋯ ,wTxN−yN)T=(wTXT−YT)⋅(Xw−Y)=wTXTXw−YTXw−wTXTY+YTY=wTXTXw−2wTXTY+YTY\\begin{aligned} L(w) &amp;=\\left(w^{T} x_{1}-y_{1}, \\cdots, w^{T} x_{N}-y_{N}\\right) \\cdot\\left(w^{T} x_{1}-y_{1}, \\cdots, w^{T} x_{N}-y_{N}\\right)^{T} \\\\ &amp;=\\left(w^{T} X^{T}-Y^{T}\\right) \\cdot(X w-Y)=w^{T} X^{T} X w-Y^{T} X w-w^{T} X^{T} Y+Y^{T} Y \\\\ &amp;=w^{T} X^{T} X w-2 w^{T} X^{T} Y+Y^{T} Y \\end{aligned}L(w)​=(wTx1​−y1​,⋯,wTxN​−yN​)⋅(wTx1​−y1​,⋯,wTxN​−yN​)T=(wTXT−YT)⋅(Xw−Y)=wTXTXw−YTXw−wTXTY+YTY=wTXTXw−2wTXTY+YTY​ 最小化这个值的 $\\hat{w} $,这里注意对用变量的求导情况： w^=argmin⁡wL(w)⟶∂∂wL(w)=0⟶2XTXw^−2XTY=0⟶w^=(XTX)−1XTY=X+Y\\begin{aligned} \\hat{w}=\\operatorname{argmin}_{w} L(w) &amp; \\longrightarrow \\frac{\\partial}{\\partial w} L(w)=0 \\\\ &amp; \\longrightarrow 2 X^{T} X \\hat{w}-2 X^{T} Y=0 \\\\ &amp; \\longrightarrow \\hat{w}=\\left(X^{T} X\\right)^{-1} X^{T} Y=X^{+} Y \\end{aligned}w^=argminw​L(w)​⟶∂w∂​L(w)=0⟶2XTXw^−2XTY=0⟶w^=(XTX)−1XTY=X+Y​ 这个式子中 $\\left(X^{T} X\\right)^{-1} X^{T} $ 又被称为伪逆。对于行满秩或者列满秩的 X , 可以直接求解, 但是对于非满秩的样本集合，需要使用奇异值分解 (SVD) 的方法, 对 X 求奇异值分解, 得到 X=UΣVTX=U \\Sigma V^{T}X=UΣVT W^=(X⊤x)−1X⊤Y=((UΣV⊤)⊤(UΣV⊤))−1(UΣV)⊤Y=(VΣ⊤U⊤UΣV⊤)−1VΣ⊤U⊤Y=(VΣT⊤ΣV⊤)⊤Vε⊤U⊤Y=(V⊤)−1Σ−1(Σ⊤)−1V−1VΣ⊤U⊤Y=V∑−1(Σ⊤)−1Σ⊤U⊤Y=VΣ−1U⊤Y\\begin{aligned} \\hat{W}=\\left(X^{\\top} x\\right)^{-1} X^{\\top} Y &amp;=\\left(\\left(U \\Sigma V^{\\top}\\right)^{\\top}\\left(U \\Sigma V^{\\top}\\right)\\right)^{-1}\\left(U \\Sigma V\\right)^{\\top} Y \\\\ &amp;=\\left(V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top}\\right)^{-1} V \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=\\left(V_{\\Sigma} T^{\\top} \\Sigma V^{\\top}\\right)^{\\top} V \\varepsilon^{\\top} U^{\\top} Y \\\\ &amp;=\\left(V^{\\top}\\right)^{-1} \\Sigma^{-1}\\left(\\Sigma^{\\top}\\right)^{-1} V^{-1} V \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=V \\sum^{-1}\\left(\\Sigma^{\\top}\\right)^{-1} \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=V \\Sigma^{-1} U^{\\top} Y \\end{aligned}W^=(X⊤x)−1X⊤Y​=((UΣV⊤)⊤(UΣV⊤))−1(UΣV)⊤Y=(VΣ⊤U⊤UΣV⊤)−1VΣ⊤U⊤Y=(VΣ​T⊤ΣV⊤)⊤Vε⊤U⊤Y=(V⊤)−1Σ−1(Σ⊤)−1V−1VΣ⊤U⊤Y=V∑−1​(Σ⊤)−1Σ⊤U⊤Y=VΣ−1U⊤Y​ 于是: X+=VΣ−1UTX^{+}=V \\Sigma^{-1} U^{T}X+=VΣ−1UT 几何想象法 我们通过对样本XXX的组合，也就是x1,x2,...,xnx_1,x_2,...,x_nx1​,x2​,...,xn​获得预测值, Y=(y1,y2,⋯ ,yN)TY=\\left(y_{1}, y_{2}, \\cdots, y_{N}\\right)^{T}Y=(y1​,y2​,⋯,yN​)T是n∗1n*1n∗1的向量，则我们希望存在一个向量m∗1m*1m∗1的向量β\\betaβ ，使得XTβX^{T}\\betaXTβ与YYY之间的距离越小越好，那么XTβX^{T}\\betaXTβ与YYY之间的差就与XTX^{T}XT的基本空间垂直，即： XT⋅(Y−Xβ)=0⟶β=(XTX)−1XTYX^{T} \\cdot(Y-X \\beta)=0 \\longrightarrow \\beta=\\left(X^{T} X\\right)^{-1} X^{T} YXT⋅(Y−Xβ)=0⟶β=(XTX)−1XTY 噪声先验高斯分布的MLE 对于一维的情况，记 y=wTx+ϵy=w^{T} x+\\epsilony=wTx+ϵ, ϵ∼N(0,σ2)\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)ϵ∼N(0,σ2) , 那么 $ y \\sim \\mathcal{N}\\left(w^{T} x, \\sigma^{2}\\right)$ 。代入极大似然估计中： L(w)=log⁡p(Y∣X,w)=log⁡∏i=1Np(yi∣xi,w)=∑i=1Nlog⁡(12πσe−(yi−uTxi)22σ2)argmax⁡wL(w)=argmin⁡w∑i=1N(yi−wTxi)2\\begin{aligned} L(w)=\\log p(Y \\mid X, w) &amp;=\\log \\prod_{i=1}^{N} p\\left(y_{i} \\mid x_{i}, w\\right) \\\\ &amp;=\\sum_{i=1}^{N} \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma}} e^{-\\frac{\\left(y_{i}-u^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}}\\right) \\\\ \\underset{w}{\\operatorname{argmax}} L(w) &amp;=\\underset{w}{\\operatorname{argmin}} \\sum_{i=1^{N}}\\left(y_{i}-w^{T} x_{i}\\right)^{2} \\end{aligned}L(w)=logp(Y∣X,w)wargmax​L(w)​=logi=1∏N​p(yi​∣xi​,w)=i=1∑N​log(2πσ​1​e−2σ2(yi​−uTxi​)2​)=wargmin​i=1N∑​(yi​−wTxi​)2​ 这个表达式和最小二乘估计得到的结果一样。 权重先验为高斯分布的MAP-最大后验估计 取先验分布 $w \\sim \\mathcal{N}\\left(0, \\sigma_{0}^{2}\\right) $, ϵ∼N(0,σ2)\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)ϵ∼N(0,σ2) ,y∣x;w∼(w⊤x,σ2)y \\mid x ; w \\sim\\left(w^{\\top} x, \\sigma^{2}\\right)y∣x;w∼(w⊤x,σ2)。于是： w^=argmax⁡wp(w∣Y)=argmax⁡wp(Y∣w)p(w)=argmax⁡wlog⁡p(Y∣w)p(w)=argmax⁡w(log⁡p(Y∣w)+log⁡p(w))=argmin⁡w[(y−wTx)2+σ2σ02wTw]\\begin{aligned} \\hat{w}=\\operatorname{argmax}_{w} p(w \\mid Y) &amp;=\\underset{w}{\\operatorname{argmax}} p(Y \\mid w) p(w) \\\\ &amp;=\\underset{w}{\\operatorname{argmax}} \\log p(Y \\mid w) p(w) \\\\ &amp;=\\underset{w}{\\operatorname{argmax}}(\\log p(Y \\mid w)+\\log p(w)) \\\\ &amp;=\\underset{w}{\\operatorname{argmin}}\\left[\\left(y-w^{T} x\\right)^{2}+\\frac{\\sigma^{2}}{\\sigma_{0}^{2}} w^{T} w\\right] \\end{aligned}w^=argmaxw​p(w∣Y)​=wargmax​p(Y∣w)p(w)=wargmax​logp(Y∣w)p(w)=wargmax​(logp(Y∣w)+logp(w))=wargmin​[(y−wTx)2+σ02​σ2​wTw]​ 这里省略了XXX,$ p(Y)$ 和 www 没有关系: p(w∣Y)=p(w,Y)p(Y)=p(Y∣w∣p(w)p(Y)p(w \\mid Y)=\\frac{p(w, Y)}{p(Y)}=\\frac{p(Y|w| p(w)}{p(Y)}p(w∣Y)=p(Y)p(w,Y)​=p(Y)p(Y∣w∣p(w)​ $ p(Y)是已有先验数据和是已有先验数据和是已有先验数据和w$无关。 正则化 正则化的两种方式： L1:argmin⁡wL(w)+λ∥w∥1,λ&gt;0L2:argmin⁡wL(w)+λ∥w∥22,λ&gt;0\\begin{array}{l} L 1: \\underset{w}{\\operatorname{argmin}} L(w)+\\lambda\\|w\\|_{1}, \\lambda&gt;0 \\\\ L 2: \\operatorname{argmin}_{w} L(w)+\\lambda\\|w\\|_{2}^{2}, \\lambda&gt;0 \\end{array}L1:wargmin​L(w)+λ∥w∥1​,λ&gt;0L2:argminw​L(w)+λ∥w∥22​,λ&gt;0​ L1正则化 Lasso L1正则化可以引起稀疏解 从最小化损失的角度看，由于 L1\\mathrm{L} 1L1 项求导在O附近的左右导数都不是0, 因此更容易取到0 解。 L2正则化 Ridge w^=argmin⁡wL(w)+λwTw⟶∂∂wL(w)+2λw=0⟶2XTXw^−2XTY+2λw^=0⟶w^=(XTX+λI)−1XTY\\begin{aligned} \\hat{w}=\\operatorname{argmin}_{w} L(w)+\\lambda w^{T} w &amp; \\longrightarrow \\frac{\\partial}{\\partial w} L(w)+2 \\lambda w=0 \\\\ &amp; \\longrightarrow 2 X^{T} X \\hat{w}-2 X^{T} Y+2 \\lambda \\hat{w}=0 \\\\ &amp; \\longrightarrow \\hat{w}=\\left(X^{T} X+\\lambda \\mathbb{I}\\right)^{-1} X^{T} Y \\end{aligned}w^=argminw​L(w)+λwTw​⟶∂w∂​L(w)+2λw=0⟶2XTXw^−2XTY+2λw^=0⟶w^=(XTX+λI)−1XTY​ 可以看到，这个正则化参数和前面的 MAP 结果不谋而合。利用2范数进行正则化不仅可 以是模型选择 www 较小的参数，同时也避免 XTXX^{T}XXTX不可逆的问题。L2正则化可以降低过拟合。 正则化的知识还需要进一步理解。","categories":[{"name":"算法","slug":"算法","permalink":"https://mz2sj.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"数据分析","slug":"算法/数据分析","permalink":"https://mz2sj.github.io/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://mz2sj.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"numpy知识点","slug":"numpy知识点","date":"2020-12-16T02:30:38.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/12/16/numpy知识点/","link":"","permalink":"https://mz2sj.github.io/2020/12/16/numpy%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"记录一些常用函数和知识点 axis理解 numpy中axis的理解 axis=0,沿着竖直方向操作，操作对象是水平的一行， axis=1,沿着水平方向操作，操作对象是竖直的一列。 &gt;&gt;&gt;df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], \\ columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) &gt;&gt;&gt;df col1 col2 col3 col4 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 &gt;&gt;&gt; df.mean(axis=1) 0 1 1 2 2 3 如代码所示，axis=1表示沿水平方向操作相加求平均，操作对象是竖直的一列，也就是 1 1 1 1 2 + 2 + 2 + 2 3 3 3 3 同理看一下drop &gt;&gt;&gt; df.drop(\"col4\", axis=1) col1 col2 col3 0 1 1 1 1 2 2 2 2 3 3 3 drop的对象是一列，也就是drop下面这些列中的某一列 1 1 1 1 2 2 2 3 3 sum和drop不同的的操作含义和axis结合起来可以产生不同的效果。 np.full() 创造全为某个值的数组 np.full((2,5),6,dtype=np.int) array([[6, 6, 6, 6, 6], [6, 6, 6, 6, 6]], dtype=uint32) 创造形状相同，值为某个值的数组 x=np.arange(4,dtype=np.int64) np.full_like(x,6) array([6, 6, 6, 6], dtype=int64) np.asscalar() x = np.array([30]) np.asscalar(x) np.arange() np.arange(2, 101, 2) array([ 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]) np.linspace() 给定一段范围划分指定数目 np.linspace(3., 10, 50) array([ 3. , 3.14285714, 3.28571429, 3.42857143, 3.57142857, 3.71428571, 3.85714286, 4. , 4.14285714, 4.28571429, 4.42857143, 4.57142857, 4.71428571, 4.85714286, 5. , 5.14285714, 5.28571429, 5.42857143, 5.57142857, 5.71428571, 5.85714286, 6. , 6.14285714, 6.28571429, 6.42857143, 6.57142857, 6.71428571, 6.85714286, 7. , 7.14285714, 7.28571429, 7.42857143, 7.57142857, 7.71428571, 7.85714286, 8. , 8.14285714, 8.28571429, 8.42857143, 8.57142857, 8.71428571, 8.85714286, 9. , 9.14285714, 9.28571429, 9.42857143, 9.57142857, 9.71428571, 9.85714286, 10. ]) np.logspace() 同np.linspace(),只不过取了log np.logspace(3,10,50,endpoint=False) array([ 1.00000000e+03, 1.38038426e+03, 1.90546072e+03, 2.63026799e+03, 3.63078055e+03, 5.01187234e+03, 6.91830971e+03, 9.54992586e+03, 1.31825674e+04, 1.81970086e+04, 2.51188643e+04, 3.46736850e+04, 4.78630092e+04, 6.60693448e+04, 9.12010839e+04, 1.25892541e+05, 1.73780083e+05, 2.39883292e+05, 3.31131121e+05, 4.57088190e+05, 6.30957344e+05, 8.70963590e+05, 1.20226443e+06, 1.65958691e+06, 2.29086765e+06, 3.16227766e+06, 4.36515832e+06, 6.02559586e+06, 8.31763771e+06, 1.14815362e+07, 1.58489319e+07, 2.18776162e+07, 3.01995172e+07, 4.16869383e+07, 5.75439937e+07, 7.94328235e+07, 1.09647820e+08, 1.51356125e+08, 2.08929613e+08, 2.88403150e+08, 3.98107171e+08, 5.49540874e+08, 7.58577575e+08, 1.04712855e+09, 1.44543977e+09, 1.99526231e+09, 2.75422870e+09, 3.80189396e+09, 5.24807460e+09, 7.24435960e+09]) np.diagonal() 输出对角线，矩阵的迹 X = np.array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) np.diag(X) X.diagonal() array([ 0, 5, 10]) 创造对角线指定值，其余为0的矩阵 np.diagflat([1,2,3,4]) array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) np.tri() 创造对角线指定方向上下为1，其余为0的矩阵 np.tri(5,5,1) array([[1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) np.tri(5,5,0) array([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) np.tri(5,5,-1) array([[0., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.]]) #下三角 对角线以下保留，以上为0 np.tril(np.arange(1, 13).reshape(4, 3), -1) array([[ 0, 0, 0], [ 4, 0, 0], [ 7, 8, 0], [10, 11, 12]]) np.triu(np.arange(1, 13).reshape(4, 3), -1) array([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) 打平 x = np.array([[1, 2, 3], [4, 5, 6]]) out1=np.ravel(x,order='F') out2=x.flatten(order='F') np.swapaxes() x=np.zeros((3,4,5)) out1=np.swapaxes(x,1,0) out2=x.transpose([1,0,2]) np.expand_dims() x=np.zeros((3,4)) np.expand_dims(x,axis=1) np.squeeze(x) np.concatenate() x = np.array([[1, 2, 3], [4, 5, 6]]) y = np.array([[7, 8, 9], [10, 11, 12]]) out1 = np.concatenate((x, y), 1) #水平方向操作，竖直为操作对象 out2 = np.hstack((x, y)) #水平方向堆叠 #按列来堆叠 x = np.array((1,2,3)) y = np.array((4,5,6)) out1 = np.column_stack((x, y)) x = np.array([[1],[2],[3]]) y = np.array([[4],[5],[6]]) out = np.dstack((x, y)) [[[1 4]] [[2 5]] [[3 6]]] np.split() x = np.arange(1, 10) np.split(x, [4, 6]) [array([1, 2, 3, 4]), array([5, 6]), array([7, 8, 9])]","categories":[{"name":"numpy","slug":"numpy","permalink":"https://mz2sj.github.io/categories/numpy/"}],"tags":[{"name":"numpy","slug":"numpy","permalink":"https://mz2sj.github.io/tags/numpy/"}]},{"title":"第二届翼支付杯大数据建模大赛复盘","slug":"第二届翼支付杯大数据建模大赛复盘","date":"2020-10-03T07:51:44.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/10/03/第二届翼支付杯大数据建模大赛复盘/","link":"","permalink":"https://mz2sj.github.io/2020/10/03/%E7%AC%AC%E4%BA%8C%E5%B1%8A%E7%BF%BC%E6%94%AF%E4%BB%98%E6%9D%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E5%A4%A7%E8%B5%9B%E5%A4%8D%E7%9B%98/","excerpt":"","text":"暑假的时候参加了一个数据竞赛–第二届翼支付杯大数据建模大赛-信用风险用户识别。最后复赛A榜排在第15名，B榜12名。第一次正式参加比赛，原本只想排到六七十名就够了，没想到最后能进到前二十，感谢队友和自己的付出。虽然最后的结果还不错，但是做的还是模模糊糊的，现在对一些top方案和自己参加比赛的想法做一些总结。 数据 基础信息 base_df 字段名 字段说明（数据经过脱敏处理） user 样本编号，e.g., Train_00000、Train_00001… sex 性别，编码后取值为：category 0、category1 age 年龄，处理后仅保留大小关系，为某一区间的整数 provider 运营商类型，编码后取值为：category 0、category 1… level 用户等级，编码后取值为：category 0、category 1… verified 是否实名，编码后取值为：category 0、category1 using_time 使用时长，处理后仅保留大小关系，为某一区间的整数 regist_type 注册类型，编码后取值为：category 0、category 1… card_a_cnt a类型卡的数量，处理后仅保留大小关系，为某一区间的整数 card_b_cnt b类型卡的数量，处理后仅保留大小关系，为某一区间的整数 card_c_cnt c类型卡的数量，处理后仅保留大小关系，为某一区间的整数 card_d_cnt d类型卡的数量，处理后仅保留大小关系，为某一区间的整数 op1_cnt 某类型1操作数量，处理后仅保留大小关系，为某一区间的整数 op2_cnt 某类型2操作数量，处理后仅保留大小关系，为某一区间的整数 service1_cnt 某业务1产生数量，处理后仅保留大小关系，为某一区间的整数 service1_amt 某业务1产生金额，处理后仅保留大小关系，为某一区间的整数 service2_cnt 某业务2产生数量，处理后仅保留大小关系，为某一区间的整数 agreement_total 开通协议数量，处理后仅保留大小关系，为某一区间的整数 agreement1 是否开通协议1，编码后取值为：category 0、category1 agreement2 是否开通协议2，编码后取值为：category 0、category1 agreement3 是否开通协议3，编码后取值为：category 0、category1 agreement4 是否开通协议4，编码后取值为：category 0、category1 acc_count 账号数量，处理后仅保留大小关系，为某一区间的整数 login_cnt_period1 某段时期1的登录次数，处理后仅保留大小关系，为某一区间的整数 login_cnt_period2 某段时期2的登录次数，处理后仅保留大小关系，为某一区间的整数 ip_cnt 某段时期登录ip个数，处理后仅保留大小关系，为某一区间的整数 login_cnt_avg 某段时期登录次数均值，处理后仅保留大小关系，为某一区间的整数 login_days_cnt 某段时期登录天数，处理后仅保留大小关系，为某一区间的整数 province 省份，处理成类别编码 city 城市，处理成类别编码 balance 余额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 balance_avg 近某段时期余额均值等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 balance1 类型1余额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 balance1_avg 近某段时期类型1余额均值等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 balance2 类型2余额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 balance2_avg 近某段时期类型2余额均值等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 service3 是否服务3用户，编码后取值为：category 0、category1 service3_level 服务3等级，编码后取值为：category 0、category1… product1_amount 产品1金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product2_amount 产品2金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product3_amount 产品3金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product4_amount 产品4金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product5_amount 产品5金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product6_amount 产品6金额等级，处理成保留大小关系的类别编码：level 1、level2… 例如：level 2 &gt; level 1 product7_cnt 产品7申请次数，处理后仅保留大小关系，为某一区间的整数 product7_fail_cnt 产品7申请失败次数，处理后仅保留大小关系，为某一区间的整数 操作信息 op_df 字段名 字段说明（数据经过脱敏处理） user 样本编号，e.g., Train_00000、Train_00001… op_type 操作类型编码，处理成类别编码 op_mode 操作模式编码，处理成类别编码 op_device 操作设备编码，处理成类别编码 ip 设备ip编码，处理成类别编码 net_type 网络类型编码，处理成类别编码 channel 渠道类型编码，处理成类别编码 ip_3 设备ip前三位编码，处理成类别编码 tm_diff 距离某起始时间点的时间间隔，处理成如下格式。例如： 9 days 09:02:45.000000000，表示距离某起始时间点9天9小时2分钟45秒 交易信息 trans_df 字段名 字段说明（数据经过脱敏处理） user 样本编号，e.g., Train_00000、Train_00001… platform 平台类型编码，处理成类别编码 tunnel_in 来源类型编码，处理成类别编码 tunnel_out 去向类型编码，处理成类别编码 amount 交易金额，处理后仅保留大小关系，为某一区间的整数 type1 交易类型1编码，处理成类别编码 type2 交易类型2编码，处理成类别编码 ip 设备ip编码，处理成类别编码 ip_3 设备ip前三位编码，处理成类别编码 tm_diff 距离某起始时间点的时间间隔，处理成如下格式。例如： 9 days 09:02:45.000000000，表示距离某起始时间点9天9小时2分钟45秒 评价方式采用roc_auc_score top1方案 1.第一个比较有意思的点就是作者根据user键将trans_df和op_df中merge进了base_df的label,这样就能对trans_df和op_df作target_encoding啦~ def gen_trans_op_label(trans, op, train_label): trans_ = trans.copy() op_ = op.copy() user_label_dict = dict(zip(train_label.user.values, train_label.label.values)) trans_['label'] = trans_['user'].map(user_label_dict) op_['label'] = op_['user'].map(user_label_dict) return trans_, op_ 让我们学习一下pandas中的map操作。 其次就是将trans_df和op_df拼接成一张表trans_op_df，这样一张表就同时包含操作和交易信息,也许会带来更多的信息哦 2.构建session，这个是根据操作或者交易的时间差来确定一个session，并不是很懂。当相邻两个操作或者交易时间差超过600时，赋值为1作cumsum。 def gen_session_fea(file): tmp = file.groupby('user')['timestamp'].shift() file['delta_time'] = file['timestamp'] - tmp file['session'] = np.where(file['delta_time'] &gt; 600, 1, 0) file['session'] = file.groupby(['user'])['session'].transform('cumsum') del file['delta_time'] return file 怎么解释呢？当相邻的操作或交易在一定的时间范围内完成时，就认为是一个session，当很多操作在相邻的时间内完成，是否可以看成一种行为模式，用cumsum来累积？另外一个点就是pandas中的groupby后作shift的操作，可以对一个组内的某个变量进行偏移操作。 3.一些针对op_df和trans_df的交叉特征 def gen_fea_op_df(op_df): op_df['op_pattern'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str) op_df['op_type_mode'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str) op_df['op_type_device'] = op_df['op_type'].map(str) + '_' + op_df['op_device'].map(str) op_df['op_mode_device'] = op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str) op_df['ip_net_type'] = op_df['ip'].map(str) + '_' + op_df['net_type'].map(str) op_df['ip3_net_type'] = op_df['ip_3'].map(str) + '_' + op_df['net_type'].map(str) op_df['net_type_channel'] = op_df['net_type'].map(str) + '_' + op_df['channel'].map(str) # op_df['time_diff'] = op_df['timestamp'].diff(-1) op_df.rename(columns={'ip' : 'op_ip', 'ip_3': 'op_ip_3',}, inplace=True) return op_df 可以看出来的一点是，对于交叉特诊，作者也是将相关的一些特征进行组合。自己当时只做了一列列别数量较少的特征的交叉，在这里可以看到作者对ip等很多类别的特征也进行了交叉。 def gen_fea_trans_df(trans_df): trans_df['tunnel_io'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['tunnel_out'].astype(str) trans_df['type'] = trans_df['type1'].astype(str) + '_' +trans_df['type2'].astype(str) trans_df['tunnel_io_type'] = trans_df['tunnel_io'].astype(str) + '_' + trans_df['type'].astype(str) trans_df['platform_tunnel_io_type'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io_type'] trans_df['platform_tunnel_io'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io'] trans_df['platform_type'] = trans_df['platform'].astype(str) + '_' + trans_df['type'] trans_df['platform_amount'] = trans_df['platform'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_io_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type1_amount'] = trans_df['type1'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type2_amount'] = trans_df['type2'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_in_amount'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_out_amount'] = trans_df['tunnel_out'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['amount_diff'] = trans_df['amount'].astype(int).diff(-1) trans_df['time_diff'] = trans_df['timestamp'].diff(-1) trans_df['amount_per_time'] = trans_df['amount_diff'] / np.where(trans_df['time_diff'] == 0, 0.01, trans_df['time_diff']) trans_df = gen_session_fea(trans_df) 比较有看点的是对amount做了diff操作，计算相邻两次交易的差额，并计算了相邻两次交易的时间，两者相除得到交易额与时间差的比值。 3.base_df交叉特征。对于base_df里的数值特征作加减乘除操作。 def int_cols_cross(df, cols): \"\"\"[summary] 对base数据的int64特征进行min-max归一化后进行加减乘除交互 Parameters ---------- df : [DataFrame] [训练集和测试集合并的数据] cols : [list] [交互特征] Returns ------- [DataFrame, list] [整数特征交互后的data, 及交互特征名称] \"\"\" cross_feature = [] df = df.copy() for i, col in tqdm(enumerate(cols), desc='extract cross feature for base'): for j in range(i + 1, len(cols)): df[col + '_' + 'div_' + cols[j]] = min_max_unif(df[col]) / min_max_unif(df[cols[j]]) df[col + '_' + 'sub_' + cols[j]] = min_max_unif(df[col]) - min_max_unif(df[cols[j]]) df[col + '_' + 'mul_' + cols[j]] = min_max_unif(df[col]) * min_max_unif(df[cols[j]]) df[col + '_' + 'sum_' + cols[j]] = min_max_unif(df[col]) + min_max_unif(df[cols[j]]) cross_feature.append(col + '_' + 'div_' + cols[j]) cross_feature.append(col + '_' + 'sub_' + cols[j]) cross_feature.append(col + '_' + 'mul_' + cols[j]) cross_feature.append(col + '_' + 'sum_' + cols[j]) return df, cross_feature 这个感觉很强呀，加减乘除都用到数值特征上。 3.count计数特征 def gen_cnt_feature(df, feature): cnt_features = [] for fea in feature: df[fea + '_count'] = df.groupby([fea])['user'].transform('count') cnt_features.append(fea + '_count') return df 但是为什么只对cnt_feature = ['city', 'province', 'balance', 'ip_cnt', 'using_time', ]这些特征作计数特征就不知道是为什么了，难道也是一个个试出来的嘛？ 4.trans_df的amount统计特征 def gen_user_amount_features(df): group_df = df.groupby(['user'])['amount'].agg({ 'user_amount_mean': 'mean', # 'user_amount_std': 'std', 'user_amount_max': 'max', 'user_amount_min': 'min', 'user_amount_sum': 'sum', 'user_amount_med': 'median', 'user_amount_cnt': 'count', # 'user_amount_q1': lambda x: x.quantile(0.25), # 'user_amount_q3': lambda x: x.quantile(0.75), #'user_amount_qsub': lambda x: x.quantile(0.75) - x.quantile(0.25) #'user_amount_skew': 'skew', }).reset_index() return group_df 这个没什么好说的，肯定得做 5.nunique特征 def gen_user_nunique_features(df, value, prefix): group_df = df.groupby(['user'])[value].agg({ 'user_{}_{}_nuniq'.format(prefix, value): 'nunique' }).reset_index() return group_df 计算一个用户在某个字段上有几个unique值. trans_df:['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', ] 6.一个值有几个用户的nunique统计结果 首先计算一个值有几个用户 def file_cols_user_nunique(file, feature_lst, prefix): col_nuniq_fea_lst = [] for col in tqdm(feature_lst): col_nuniq = file.groupby(col)['user'].nunique() col_nuniq_dic = dict(zip(col_nuniq.index, col_nuniq.values)) file[prefix + '_' + col + '_user_nuniq'] = file[col].map(col_nuniq_dic) col_nuniq_fea_lst.append(prefix + '_' + col + '_user_nuniq') return file, col_nuniq_fea_lst 在对上面的计算值作统计计算 def gen_stastic_col_user_nunique(file, feat, prefix): group_df = file.groupby('user')[feat].agg({ prefix + feat + '_mean': 'mean', prefix + feat + '_std': 'std', prefix + feat + '_max': 'max', prefix + feat + '_min': 'min', prefix + feat + '_sum': 'sum', prefix + feat + '_med': 'median', #prefix + feat + '_q1' : lambda x: x.quantile(0.25), #prefix + feat + '_q3' : lambda x: x.quantile(0.75), #prefix + feat + 'q_sub': lambda x: x.quantile(0.75) - x.quantile(0.25), #prefix + feat + '_skew': 'skew', }) return group_df 这个在trans_df、op_df、trans_op_df都可以做。 7.利用pivot_table计算在各个值类别上交易的聚合 def gen_user_group_amount_features(df, value): group_df = df.pivot_table(index='user', columns=value, values='amount', dropna=False, aggfunc=['count', 'sum', 'mean', 'max', 'min', 'median', ]).fillna(0) group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns] group_df.reset_index(inplace=True) return group_df 这种特征构造方式在类别不是特别多的时候比较适用。 8.计算不同时间窗口下用户交易额统计信息 def gen_user_window_amount_features(df, window): group_df = df[df['days_diff']&gt;window].groupby('user')['amount'].agg({ 'user_amount_mean_{}d'.format(window): 'mean', 'user_amount_std_{}d'.format(window): 'std', 'user_amount_max_{}d'.format(window): 'max', 'user_amount_min_{}d'.format(window): 'min', 'user_amount_sum_{}d'.format(window): 'sum', 'user_amount_med_{}d'.format(window): 'median', 'user_amount_cnt_{}d'.format(window): 'count', # 'user_amount_q1_{}d'.format(window): lambda x: x.quantile(0.25), # 'user_amount_q3_{}d'.format(window): lambda x: x.quantile(0.75), # 'user_amount_qsub_{}d'.format(window): lambda x: x.quantile(0.75) - x.quantile(0.25), # 'user_amount_skew_{}d'.format(window): 'skew', # 'user_amount_q4_{}d'.format(window): lambda x: x.quantile(0.8), # 'user_amount_q5_{}d'.format(window): lambda x: x.quantile(0.3), # 'user_amount_q6_{}d'.format(window): lambda x: x.quantile(0.7), }).reset_index() return group_df 9.空值特征 def gen_user_null_features(df, value, prefix): df['is_null'] = 0 df.loc[df[value].isnull(), 'is_null'] = 1 group_df = df.groupby(['user'])['is_null'].agg({'user_{}_{}_null_cnt'.format(prefix, value): 'sum', 'user_{}_{}_null_ratio'.format(prefix, value): 'mean'}).reset_index() return group_df 10.统计一些特殊值上时间的聚合特征 def gen_file_type_days_diff(df, file, type, time_feat): plot_feats = [] #file_type_unique = file[type].value_counts().index.tolist() file_type_unique = [] if type == 'type1': file_type_unique = ['45a1168437c708ff', 'f67d4b5a05a1352a', ] elif type == 'type2': file_type_unique = ['11a213398ee0c623',] elif type == 'channel': file_type_unique = ['b2e7fa260df4998d', '116a2503b987ea81', '8adb3dcfea9dcf5e'] elif type == 'tunnel_io': file_type_unique = ['b2e7fa260df4998d_6ee790756007e69a',] elif type == 'type': file_type_unique = ['f67d4b5a05a1352a_nan', '19d44f1a51919482_11a213398ee0c623', '45a1168437c708ff_11a213398ee0c623', '674e8d5860bc033d_11a213398ee0c623', '0a3cf8dac7dca9d1_b5a8be737a50b171'] for tp in file_type_unique: assert file_type_unique != [] group_df = file[file[type] == tp].groupby(['user'])[time_feat].agg( {'user_{}_{}_min_{}'.format(type, tp, time_feat): 'min', 'user_{}_{}_mean_{}'.format(type, tp, time_feat): 'mean', 'user_{}_{}_max_{}'.format(type, tp, time_feat): 'max', 'user_{}_{}_std_{}'.format(type, tp, time_feat): 'std', 'user_{}_{}_median_{}'.format(type, tp, time_feat): 'median', 'user_{}_{}_sum_{}'.format(type, tp, time_feat): 'sum', # 'user_{}_{}_q1_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.25), # 'user_{}_{}_q3_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75), # 'user_{}_{}_q_sub_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75) - x.quantile(0.25), # 'user_{}_{}_skew_{}'.format(type, tp, time_feat): 'skew', }).reset_index() df = df.merge(group_df, on=['user'], how='left') stastic = ['min', 'max', 'max', 'std', 'median', 'sum',] for stast in stastic: plot_feats.append('user_{}_{}_{}_{}'.format(type, tp, stast, time_feat)) return df, plot_feats 这个特殊值的选择作者也没有提到，应该是要根据一些画图分析得到。 11.doc2vec特征 def d2v_feat(df, feat, length, num): print('Start training Doc2Vec models.......') df[feat] = df[feat].astype(str) group_df = df.groupby(['user'])[feat].agg(list).reset_index() documents = [TaggedDocument(doc, [i]) for i, doc in zip(group_df['user'].values, group_df[feat])] model = Doc2Vec(documents, vector_size=length, window=10, min_count=1, workers=1, seed=2020, epochs=20, hs=1, ) if not os.path.exists('./d2v_models/'): os.makedirs('./d2v_models/') model.save('../d2v_models/d2v_testb_{}.model'.format(num)) # model = Doc2Vec.load('./d2v_models/d2v_testb_{}.model'.format(num)) doc_df = group_df['user'].apply(lambda x: ','.join([str(i) for i in model[x]])).str.split(',', expand=True).apply(pd.to_numeric) doc_df.columns = ['{}_d2v_{}'.format(feat, i) for i in range(length)] return pd.concat([group_df[['user']], doc_df], axis=1) em…,看懂代码就行啦。用在trans_df的amount上。自己也做了这个特征，还用在了其他的字段序列上，感觉也不能无脑对所有字段都用，还是要根据实验结果有选择的用。 12.word2vec特征 def w2v_feat(df, feat, length, num): \"\"\" :param df: 进行word2vec编码的数据 :param feat: 进行编码的特征 :param length: embedding向量长度 :return: \"\"\" global w2v_fea_lst w2v_fea_lst = [] print('Start training Word2Vec models.....') df[feat] = df[feat].astype(str) group_df = df.groupby(['user'])[feat].agg(list).reset_index() model = Word2Vec(group_df[feat].values, size=length, window=10, min_count=1, sg=1, hs=1, workers=1, iter=20, seed=2020,) # if feat == 'amount': # model = Word2Vec.load('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) # elif feat == 'channel': # model = Word2Vec.load('../w2v_models/w2v_channel_16.model') # elif feat == 'trans_op_ip' or feat == 'trans_op_ip_3': # model = Word2Vec.load('../w2v_models/w2v_trans_op_2.model') # else: # if not os.path.exists('../w2v_models/'): # os.makedirs('../w2v_models/') # model = Word2Vec.load('./w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) model.save('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) group_df[feat] = group_df[feat].apply(lambda x: pd.DataFrame([model[c] for c in x])) for m in tqdm(range(length), desc='extract w2v {} statistic feature'.format(feat)): group_df['{}_w2v_{}_mean'.format(feat,m)] = group_df[feat].apply(lambda x: x[m].mean()) # group_df['{}_w2v_{}_median'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].median()) # group_df['{}_w2v_{}_max'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].max()) # group_df['{}_w2v_{}_min'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].min()) # group_df['{}_w2v_{}_sum'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].sum()) # group_df['{}_w2v_{}_std'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].std()) w2v_fea_lst.append('{}_w2v_{}_mean'.format(feat,m)) del group_df[feat] return group_df 用在trans_df的amount上 13.tf_idf特征 def gen_user_tfidf_features(df, value,): print('Start tfdif encoding for {}........'.format(value)) df[value] = df[value].astype(str) df[value].fillna('-1', inplace=True) group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index() group_df.columns = ['user', 'list'] group_df['list'] = group_df['list'].apply(lambda x: ','.join(x)) enc_vec = TfidfVectorizer() tfidf_vec = enc_vec.fit_transform(group_df['list']) svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020) vec_svd = svd_enc.fit_transform(tfidf_vec) vec_svd = pd.DataFrame(vec_svd) vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)] group_df = pd.concat([group_df, vec_svd], axis=1) del group_df['list'] return group_df 14.countvec特征 def gen_user_countvec_features(df, value,): print('Start countvec encoding for {}........'.format(value)) df[value] = df[value].astype(str) df[value].fillna('-1', inplace=True) group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index() group_df.columns = ['user', 'list'] group_df['list'] = group_df['list'].apply(lambda x: ','.join(x)) enc_vec = CountVectorizer() tfidf_vec = enc_vec.fit_transform(group_df['list']) svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020) vec_svd = svd_enc.fit_transform(tfidf_vec) vec_svd = pd.DataFrame(vec_svd) vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)] group_df = pd.concat([group_df, vec_svd], axis=1) del group_df['list'] return group_df 15.计算各个用户在op_df和trans_df上在一些字段上的统计特征 def gen_user_group_trans_op_features(df, columns, value): group_df = df.pivot_table(index='user', columns=columns, values=value, dropna=False, aggfunc=['count', 'sum', 'mean', 'max', 'min', 'median', ]).fillna(0) group_df.columns = ['user_{}_{}_{}_{}'.format(columns, f[1], value, f[0]) for f in group_df.columns] group_df['op_trans_ratio'] = group_df['user_property_trans_{}_count'.format(value)] / group_df[ 'user_property_op_{}_count'.format(value)] group_df.reset_index(inplace=True) return group_df 16.获取各个用户在第一次和最后一次操作或交易时的特征 def user_trans_behavior_feature(df, trans_op): print(\"Starting extract user's trans behavior......\") # 获取第一次和最后一次行为 group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[-1]).to_dict() df['last_beahvior'] = df['user'].map(group_dic) group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[0]).to_dict() df['first_beahvior'] = df['user'].map(group_dic) # 是否有过交易行为 group_dic = trans_op.groupby('user').apply(lambda x: judge_has_trans(x)).to_dict() df['has_trans'] = df['user'].map(group_dic) # 最后一次交易days_diff group_dic = trans_op.groupby('user').apply(lambda x: last_trans_time(x)).to_dict() df['last_days_diff_trans'] = df['user'].map(group_dic) # 第一次交易days_diff group_dic = trans_op.groupby('user').apply(lambda x: first_trans_time(x)).to_dict() df['first_days_diff_trans'] = df['user'].map(group_dic) # 最后一次交易hour group_dic = trans_op.groupby('user').apply(lambda x: last_trans_hour(x)).to_dict() df['last_hour_trans'] = df['user'].map(group_dic) # 第一次交易hour group_dic = trans_op.groupby('user').apply(lambda x: first_trans_hour(x)).to_dict() df['first_hour_trans'] = df['user'].map(group_dic) # 最后一次交易week group_dic = trans_op.groupby('user').apply(lambda x: last_trans_week(x)).to_dict() df['last_week_trans'] = df['user'].map(group_dic) # 第一次交易week group_dic = trans_op.groupby('user').apply(lambda x: first_trans_week(x)).to_dict() df['first_week_trans'] = df['user'].map(group_dic) # 最后一次交易timestamp group_dic = trans_op.groupby('user').apply(lambda x: last_trans_timestamp(x)).to_dict() df['last_time_trans'] = df['user'].map(group_dic) # 第一次交易timestamp group_dic = trans_op.groupby('user').apply(lambda x: first_trans_timestamp(x)).to_dict() df['first_time_trans'] = df['user'].map(group_dic) # 平均交易次数 group_dic = trans_op.groupby('user').apply(lambda x: gen_trans_count(x)).to_dict() df['trans_count'] = df['user'].map(group_dic) # 操作次数 group_dic = trans_op.groupby('user').apply(lambda x: gen_op_count(x)).to_dict() df['op_count'] = df['user'].map(group_dic) return df 17.判断用户是否有交易 data['has_trans'] = pd.factorize(data['has_trans'])[0] 二值特征用一列01就可以了 18.类别编码 def kfold_stats_feature(train, test, feats, k, seed): folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed) # 这里最好和后面模型的K折交叉验证保持一致 train['fold'] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): train.loc[val_idx, 'fold'] = fold_ kfold_features = [] for feat in tqdm(feats, desc='Target encoding for base feature'): nums_columns = ['label'] for f in nums_columns: colname = feat + '_' + f + '_kfold_mean' kfold_features.append(colname) train[colname] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): tmp_trn = train.iloc[trn_idx] order_label = tmp_trn.groupby([feat])[f].mean() tmp = train.loc[train.fold == fold_, [feat]] train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label) # fillna global_mean = tmp_trn[f].mean() train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean) train[colname] = train[colname].astype(float) for f in nums_columns: colname = feat + '_' + f + '_kfold_mean' test[colname] = None order_label = train.groupby([feat])[f].mean() test[colname] = test[feat].map(order_label) # fillna global_mean = train[f].mean() test[colname] = test[colname].fillna(global_mean) test[colname] = test[colname].astype(float) del train['fold'] return train, test 对一些值类别比较多的变量作target encoding 19.对序列变量作target_encoding def target_encoding(file, train, test, feats, k, prefix, not_adp=True, agg_lst=['mean'], seed=2020): folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed) # 这里最好和后面模型的K折交叉验证保持一致 train['fold'] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): train.loc[val_idx, 'fold'] = fold_ tt_file = train[['user', 'label']].merge(file, on='user', how='left') te_features = [] for feat in tqdm(feats, desc='Target encoding for {} feature '.format(prefix)): col_name = feat + '_te' # te_features.append(col_name) for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): tmp_users = train.iloc[trn_idx]['user'].values tmp_file = file[file.user.isin(tmp_users)] tmp_file = tmp_file.merge(train[['user', 'label']], on='user', how='left') if not_adp: match = tmp_file.groupby(feat)['label'].mean() else: match = tmp_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index() match['eu_mean'] = match['eu_sum'] / match['eu_count'] match = match['eu_mean'].groupby(match[feat]).mean() tmp_users = train.iloc[val_idx]['user'].values tmp_file = file[file.user.isin(tmp_users)] tmp_file[col_name] = tmp_file[feat].map(match) for agg_ in agg_lst: tmp = tmp_file.groupby('user')[col_name].agg(agg_) train.loc[train.fold == fold_, col_name + '_' + agg_] = train.loc[train.fold == fold_, 'user'].map(tmp) if not_adp: match = tt_file.groupby(feat)['label'].mean() else: match = tt_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index() match['eu_mean'] = match['eu_sum'] / match['eu_count'] match = match['eu_mean'].groupby(match[feat]).mean() tmp_file = file[file.user.isin(test['user'].values)] tmp_file[col_name] = tmp_file[feat].map(match) for agg_ in agg_lst: tmp = tmp_file.groupby('user')[col_name].agg(agg_) test[col_name + '_' + agg_] = test['user'].map(tmp) del train['fold'] gc.collect() # print(train[te_features]) return train, test 首先要把序列merge上label再进行编码聚合后输出单行的编码结果 20.有序变量编码 def order_encode(df, col): \"\"\" :param df: Dataframe :param col: feature :description: 对有序类别变量顺序编码 :return: \"\"\" df.loc[df[col].notnull(), col] = df.loc[df[col].notnull(), col].apply(lambda x: str(x).split(' ')[1]).astype(int) df[col] = df[col].fillna(-1).astype(int) return df 对于一些有序的变量要还原为对应数值以捕获对应的大小关系。 21.无序变量编码 def label_encode(df, order_cols): # LabelEncoder cat_cols = [f for f in df.select_dtypes('object').columns if f not in ['user'] + order_cols] for col in cat_cols: le = LabelEncoder() df[col].fillna('-1', inplace=True) df[col] = le.fit_transform(df[col]) #cat_cols.append(col) return df 22.风险值较高的省份二值化编码 def province_binary(df, ): \"\"\"[summary] 对风险率排名最高的五个省份进行二值化及组合编码 Parameters ---------- df : [data数据] [train or test] Returns ------- [DataFrame] [经过省份二值化编码的训练集/测试集] \"\"\" #省份二值化编码 df['is_21_province'] = df.apply(lambda x: 1 if x.province == 21 else 0, axis=1) df['is_26_province'] = df.apply(lambda x: 1 if x.province == 26 else 0, axis=1) df['is_30_province'] = df.apply(lambda x: 1 if x.province == 30 else 0, axis=1) df['is_20_province'] = df.apply(lambda x: 1 if x.province == 20 else 0, axis=1) df['is_16_province'] = df.apply(lambda x: 1 if x.province == 16 else 0, axis=1) df['binary_province'] = df['is_21_province'].map(str) + df['is_26_province'].map(str) + \\ df['is_30_province'].map(str) + df['is_20_province'].map(str) + df['is_16_province'].map(str) le = LabelEncoder() df['binary_province'].fillna('-1', inplace=True) df['binary_province'] = le.fit_transform(df['binary_province']) return df 23.交易操作比例 train_data['trans_ratio'] = train_data['trans_count'] / train_data['op_count'] test_data['trans_ratio'] = test_data['trans_count'] / test_data['op_count'] 到这里，作者用到的各种编码方式就介绍结束了。其实，自己已经用到了大多数编码方式，不多自己用的比较无脑，对可以用的变量自己都用了。虽热也有一些特征选择的方法，但好像并不是那么实用，问了一些大佬，推荐的做法是做一组或一个特征就实验一下，结果提升了就保留。对于一些类别变量，lgb不用onehot变量。 top2 top2的很多特征编码方式和top1相近，这里主要介绍其特殊的特征处理方式。","categories":[{"name":"数据竞赛","slug":"数据竞赛","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"}],"tags":[{"name":"数据竞赛","slug":"数据竞赛","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"}]},{"title":"01-业务知识-如何梳理业务逻辑","slug":"01-业务知识-如何梳理业务逻辑","date":"2020-09-08T00:39:24.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/09/08/01-业务知识-如何梳理业务逻辑/","link":"","permalink":"https://mz2sj.github.io/2020/09/08/01-%E4%B8%9A%E5%8A%A1%E7%9F%A5%E8%AF%86-%E5%A6%82%E4%BD%95%E6%A2%B3%E7%90%86%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91/","excerpt":"","text":"因为不懂业务，所以找了门课来看看，记录一下自己的历程吧！ 所谓七步成诗法，一一道来~ 我们挣什么钱（经营模式）老板决定 这个好理解一点，企业可能同样是做一件事，但是他的目标可能是不一样的。 比较经典的是上面这个例子，比如视频网站，真正的使用者使用户，用户只需要看广告，而广告主却是真正付钱的人，视频网站营收的大头也是广告费用。用户是🐏，广告主是🐕。 再比如信用卡业务的例子，信用卡中乱收费的收入占比只有2%，真正占收入的大头是信贷。 如何弄清企业的经营模式呢？ 1.与业内人士多交流。 2.多看新闻、财报、行业文章 3.站在企业经营者的角度思考问题 4.关注公司内部销售渠道考核指标，年报 我们挣谁的钱（目标客户）市场部/产品 这个部分是找🐏和🐕的问题，以信用卡问题为例，如何找到那些信贷需求旺盛的群体呢？ 目标用户群依次是：企业经营者、小老板商户、高级白领、小白领、屌丝。 锁定核心用户的原则： 对用户而言：这个业务是我的刚性需求，非用不可 对企业而言：这个客户有强大的付钱能力。企业是有钱就要上。 为什么客户要用我们的产品（产品属性）市场部/产品 大部分产品并非生活必需品，要培养用户的使用习惯。比如信用卡会给你一定额度，让你先免费用爽几天、做一些优惠活动，让你感觉很方便。 第一时间捕获用户的需求，然后推销自己的产品。 如何了解产品呢？ 1.亲身体验 2.站在客户角度，我这个产品满足了客户哪些需求 3.结合产品使用场景，分析产品功能设置原因 4.了解配套的运营推广活动，全面认识产品 如何找到客户 （销售渠道）渠道/推广 常规渠道有：销售队伍业务员、电话销售（这个最烦了）、分行网点、网站。 特殊渠道：vip呗 传统行业，门店、卖场很重要。 如何弄清销售渠道呢？ 1.收集产品销售场所，了解可能的销售渠道。 2.分清消费模式是B2C还是B2B2C 3.了解各渠道消费量，销售额占比 4.了解配合销售做的推广、广告 为什么客户不用别人的产品（市场策略）市场部/运营 这是个竞争策略的问题，是和同行竞争的问题。别人用我的不用你的，肯定是因为我的好，哪里好呢： 1.产品核心功能强 2.产品附加值高 3.搞促销、搞宣传、粉丝经营 主要竞争方式有： 1.爆款产品 2.产品组合 3.促销活动 4.品牌宣传 5.会员策略 弄清竞争策略的方式： 1.收集别人的营销活动信息、品牌广告。 2.整理信息，归类，广告属于哪些类别，促销活动对象规则是什么。 谁来干（部门分工） 看一眼传统企业的架构 再看一眼互联网企业 干了挣了多少（营收情况） 销售收入是大部分公司唯一利润来源，所以传统企业渠道十分重要。 成本来源： 1.推广成本 2.产品陈本 3.运作成本：公司部门运作 4.渠道成本：单位产品在渠道销售的成本，推广或销售 5.新客户获取成本 了解营收就要多看财报，找到公司各部门的日月季报，从中发现用户数、付费情况、活动投入等信息。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"业务","slug":"业务","permalink":"https://mz2sj.github.io/tags/%E4%B8%9A%E5%8A%A1/"},{"name":"课程","slug":"课程","permalink":"https://mz2sj.github.io/tags/%E8%AF%BE%E7%A8%8B/"}]},{"title":"01-sql刷题牛客","slug":"01-sql刷题牛客","date":"2020-08-25T15:22:16.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/08/25/01-sql刷题牛客/","link":"","permalink":"https://mz2sj.github.io/2020/08/25/01-sql%E5%88%B7%E9%A2%98%E7%89%9B%E5%AE%A2/","excerpt":"","text":"今天是刷题记录的第一天，加油啦，小孟冲冲冲！！！ 查找最晚入职员工的所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天(sqlite里面的注释为--,mysql为comment) CREATE TABLE employees ( `emp_no int(11) NOT NULL, -- '员工编号'` `birth_date date NOT NULL,` `first_name varchar(14) NOT NULL,` `last_name varchar(16) NOT NULL,` `gender char(1) NOT NULL,` `hire_date date NOT NULLPRIMARY KEY (emp_no)); 自己的写法，考虑的不是那么全面，只能查出一条数据 SELECT * FROM employees ORDER BY hire_date DESC LIMIt 1; 但是假如最后一天有多人入职，这种写法就hold不住了。 SELECT * FROM employees WHERE hire_date=(SELECT MAX(hire_date) FROM employees); 查找入职员工时间排名倒数第三的员工所有信息，为了减轻入门难度，目前所有的数据里员工入职的日期都不是同一天 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 这题主要的考点是查找倒数第三，这种关于顺序的问题可以用 LIMIT(A,B)来处理，表示从第A行取第B个数据，LIMIT(2,1)意思就是从第2个数据取第1个数据，也就是第三名。 SELECT * FROM employees WHERE emp_no IN (SELECT emp_no FROM employees ORDER BY hire_date DESC LIMIT 2,1); 查找各个部门当前(dept_manager.to_date='9999-01-01')领导当前(salaries.to_date='9999-01-01')薪水详情以及其对应部门编号dept_no(注:请以salaries表为主表进行查询，输出结果以salaries.emp_no升序排序，并且请注意输出结果里面dept_no列是最后一列) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, -- '员工编号', `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, -- '部门编号' `emp_no` int(11) NOT NULL, -- '员工编号' `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); 常规的连接查询，注意输出就可以了 SELECT s.emp_no,s.salary,s.from_date,s.to_date,dm.dept_no FROM salaries s INNER JOIN dept_manager dm ON s.emp_no=dm.emp_no WHERE dm.to_date='9999-01-01' AND s.to_date='9999-01-01' ORDER BY s.emp_no ASC; 查找所有已经分配部门的员工的last_name和first_name以及dept_no(请注意输出描述里各个列的前后顺序) CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 两个表只有一个共同列，直接使用内连接。此外，因为dept_no 已经指定ＮＯＴ ＮＵＬＬ了所以题目所有的员工必然都分配了部门。 SELECT e.last_name,e.first_name,de.dept_no FROM employees e INNER JOIN dept_emp de ON e.emp_no=de.emp_no WHERE NOT (de.dept_no ISNULL); ５. 查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括暂时没有分配具体部门的员工(请注意输出描述里各个列的前后顺序) CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 这题说了包括暂时没有分配具体部门的员工,所有只要是员工就得查出来,所以对employees使用左连接啦,使用内连接的话,有的员工可能没有分配部门,那么部门表就不会有他,那再内连接就找不到他的数据了. SELECT e.last_name,e.first_name,de.dept_no FROM employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no; 查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序(请注意，一个员工可能有多次涨薪的情况) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 几天不写手感又生疏了呢！重点在于刚入职，入职时间就是hire_date呀 SELECT e.emp_no,s.salary FROM employees e INNER JOIN salaries s ON e.emp_no=s.emp_no AND s.from_date=e.hire_date ORDER BY e.emp_no DESC; 也可以用聚合函数，HAVING用来进行过滤 SELECT emp_no,salary FROM salaries GROUP BY emp_no HAVING from_date=MIN(from_date) ORDER BY emp_no DESC; 查找薪水变动超过15次的员工号emp_no以及其对应的变动次数t CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 这题考察的还是聚合函数，注意题目要求变动次数t SELECT emp_no,COUNT(*) AS t FROM salaries GROUP BY emp_no HAVING t&gt;15; 找出所有员工当前(to_date='9999-01-01')具体的薪水salary情况，对于相同的薪水只显示一次,并按照逆序显示 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 单纯的查salary表里有多少个salary，相同薪水只显示一次，用DISTINCT可以解决 SELECT DISTINCT(salary) FROM salaries WHERE to_date='9999-01-01' ORDER BY salary DESC; 看讨论区说DISTINCT消耗资源较大，用GROUP BY 解决数据重复问题,注意GROUP BY 要放在WHERE 子句后面 SELECT salary FROM salaries WHERE to_date='9999-01-01' GROUP BY salary ORDER BY salary DESC; 获取所有部门当前(dept_manager.to_date='9999-01-01')manager的当前(salaries.to_date='9999-01-01')薪水情况，给出dept_no, emp_no以及salary(请注意，同一个人可能有多条薪水情况记录) CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 注意时间，其他没什么 SELECT dm.dept_no,s.emp_no,s.salary FROM dept_manager dm INNER JOIN salaries s ON dm.emp_no=s.emp_no WHERE dm.to_date='9999-01-01' AND s.to_date='9999-01-01'; 获取所有非manager的员工emp_no CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 这个题的特点是解法多，先来简单的 SELECT e.emp_no FROM employees e WHERE e.emp_no NOT IN (SELECT dm.emp_no FROM dept_manager dm); 也可以利用连接查询获取dept_no为空的员工 SELECT e.emp_no FROM employees e LEFT JOIN dept_manager dm ON e.emp_no=dm.emp_no WHERE dm.dept_no IS NULL; 还有集合运算的解法 EXCEPT 集合差运算|UNION 集合并运算|INTERSECT 集合交运算 SELECT emp_no FROM employees EXCEPT SELECT emp_no FROM dept_manager; 获取所有员工当前的(dept_manager.to_date='9999-01-01')manager，如果员工是manager的话不显示(也就是如果当前的manager是自己的话结果不显示)。输出结果第一列给出当前员工的emp_no,第二列给出其manager对应的emp_no。 CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, -- '所有的员工编号' `dept_no` char(4) NOT NULL, -- '部门编号' `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, -- '部门编号' `emp_no` int(11) NOT NULL, -- '经理编号' `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); 关于时间限制的坑，还有一点就是该关联的键是dept_no,是根据dept_no来找相同部门的,再有一点就是学到了&lt;&gt;不等于。NOT IN的写法也是可以的，但是要是查询的结果，比较两个单独变量相等用不等于呀。 SELECT de.emp_no AS emp_no,dm.emp_no AS manager_no FROM dept_emp de INNER JOIN dept_manager dm ON de.dept_no=dm.dept_no WHERE de.emp_no &lt;&gt; dm.emp_no AND dm.to_date='9999-01-01' 获取所有部门中当前(dept_emp.to_date = '9999-01-01')员工当前(salaries.to_date='9999-01-01')薪水最高的相关信息，给出dept_no, emp_no以及其对应的salary，按照部门升序排列。 CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 用group_by反而错了，看下别人的解释 使用group by子句时，select子句中只能有聚合键、聚合函数、常数，就是除了常数和聚合其他键的函数外，不能有其他键，有其他键一定要使用聚合。emp_no并不符合这个要求。 从titles表获取按照title进行分组，每组个数大于等于2，给出title以及对应的数目t。 CREATE TABLE IF NOT EXISTS \"titles\" ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); 简单的GROUP BY~ SELECT title,COUNT(*) AS t FROM titles GROUP BY title; 注意对于重复的emp_no进行忽略(即emp_no重复的title不计算，title对应的数目t不增加)。 CREATE TABLE IF NOT EXISTS `titles` ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); 重复的emp_no可以用DISTINCT区分，记住一个点，去重用DISTINCT SELECT title,COUNT(DISTINCT emp_no) AS t FROM titles GROUP BY title HAVING t&gt;=2; 查找employees表所有emp_no为奇数，且last_name不为Mary(注意大小写)的员工信息，并按照hire_date逆序排列(题目不能使用mod函数) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 被这个不能使用mod函数给唬住了，咱还可以使用%嘛 SELECT * FROM employees WHERE last_name!='Mary' AND emp_no%2==1 ORDER BY hire_date DESC; 统计出当前(titles.to_date='9999-01-01')各个title类型对应的员工当前(salaries.to_date='9999-01-01')薪水对应的平均工资。结果给出title以及平均工资avg。 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); CREATE TABLE IF NOT EXISTS \"titles\" ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); GROUPY BY 要放在WHERE之后，WHERE子句中不能使用GROUPY BY SELECT t.title,AVG(s.salary) AS avg FROM salaries s INNER JOIN titles t ON s.emp_no=t.emp_no WHERE t.to_date='9999-01-01' AND s.to_date='9999-01-01' GROUP BY t.title; 获取当前（to_date='9999-01-01'）薪水第二多的员工的emp_no以及其对应的薪水salary CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 先上一个错误做法 SELECT emp_no,salary FROM salaries WHERE to_date='9999-01-01' ORDER by salary DESC LIMIT 1,1 假如最大工资又两个是相同的，那么这样拿到的数据就不是第二大的了。以后碰到这种牵扯导顺序的题，要考虑能否把数据查全查准，用GROUP BY 查到值排名顺序的数据 SELECT emp_no,salary FROM salaries WHERE salary =(SELECT salary FROM salaries GROUP BY salary ORDER BY salary DESC LIMIT 1,1) AND to_date='9999-01-01'; 查找当前薪水(to_date='9999-01-01')排名第二多的员工编号emp_no、薪水salary、last_name以及first_name，你可以不使用order by完成吗 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 排序题不让用ORDER BY,求第二大salary,小于最大值中的最大值就是第二大啦. SELECT e.emp_no,MAX(s.salary),e.last_name,e.first_name FROM employees e INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.salary&lt;(SELECT MAX(salary) FROM salaries) AND s.to_date='9999-01-01'; 当然这种只能求解第二高，下面这个有点难，表内条件自连接 select e.emp_no,s.salary,e.last_name,e.first_name from employees e join salaries s on e.emp_no=s.emp_no and s.to_date='9999-01-01' and s.salary = ( select s1.salary from salaries s1 join salaries s2 on s1.salary&lt;=s2.salary and s1.to_date='9999-01-01' and s2.to_date='9999-01-01' group by s1.salary having count(distinct s2.salary)=2 ) 查找所有员工的last_name和first_name以及对应的dept_name，也包括暂时没有分配部门的员工 CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 考察外联结 SELECT e.last_name,e.first_name,d.dept_name FROM (employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no) LEFT JOIN departments d ON d.dept_no=de.dept_no; 也可以这样写 SELECT e.last_name,e.first_name,d.dept_name FROM (employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no) AS t LEFT JOIN departments d ON d.dept_no=t.dept_no; 查找员工编号emp_no为10001其自入职以来的薪水salary涨幅(总共涨了多少)growth(可能有多次涨薪，没有降薪) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 是计算涨幅呀，最简单的 SELECT MAX(salary)-MIN(salary) AS growth FROM salaries WHERE emp_no=10001; 保险起见还是按照日期来选初始工资和近期工资 SELECT (SELECT salary FROM salaries WHERE emp_no=10001 ORDER BY from_date DESC LIMIT 0,1) - (SELECT salary FROM salaries WHERE emp_no=10001 ORDER BY from_date ASC LIMIT 0,1) AS growth; 查找所有员工自入职以来的薪水涨幅情况，给出员工编号emp_no以及其对应的薪水涨幅growth，并按照growth进行升序 （注:可能有employees表和salaries表里存在记录的员工，有对应的员工编号和涨薪记录，但是已经离职了，离职的员工salaries表的最新的to_date!='9999-01-01'，这样的数据不显示在查找结果里面） CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, -- '入职时间' PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, -- '一条薪水记录开始时间' `to_date` date NOT NULL, -- '一条薪水记录结束时间' PRIMARY KEY (`emp_no`,`from_date`)); 这题的重点是找到现在的工资对应的时间和初到公司对应的时间，然后迎刃而解。注意hire_date是员工被雇佣的时间。 SELECT e.emp_no,a.salary-b.salary AS growth FROM employees e INNER JOIN salaries a ON e.emp_no=a.emp_no INNER JOIN salaries b ON e.hire_date=b.from_date WHERE a.to_date='9999-01-01' ORDER BY growth ASC; 统计各个部门的工资记录数，给出部门编码dept_no、部门名称dept_name以及部门在salaries表里面有多少条记录sum CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 统计各个部门，统计各个部门，明显是按某类计算肯定要GROUP BY 啊，你这脑子 SELECT d.dept_no,d.dept_name,COUNT(t.salary) AS sum FROM (salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no) AS t INNER JOIN departments d ON t.dept_no=d.dept_no GROUP BY d.dept_no; 23.❌ 对所有员工的当前(to_date='9999-01-01')薪水按照salary进行按照1-N的排名，相同salary并列且按照emp_no升序排列 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 这题的难点是如何在不同rank()函数的情况下进行排名，排序其实可以转化为当前值小于其他值的个数，又因为salary有并列行为，所以进行计数时要使用distinct，至于有的人提到使用groupby这点还没太看懂。 SELECT s1.emp_no,s1.salary , (SELECT COUNT(DISTINCT s2.salary) FROM salaries s2 WHERE s2.salary&gt;=s1.salary AND s2.to_date='9999-01-01') AS rank FROM salaries s1 WHERE s1.to_date='9999-01-01' ORDER BY rank,s1.emp_no ASC; 起始sql中有关排序的函数也可以解决上面的问题 select emp_no,salary, dense_rank() over (order by salary desc) as rank from salaries where to_date='9999-01-01' order by rank asc,emp_no asc; 下面介绍几个函数的区别: 比如对分数进行排名：90、85、85、70 ROW_NUMBER:1,2,3,4 RANK:1,2,2,4 DENSE_RANK:1、2、2、3 NTILE函数是将有序分区中的行分发到指定数目的组中，各个组有编号，编号从1开始，就像我们说的’分区’一样 ，分为几个区，一个区会有多少个。 获取所有非manager员工当前的薪水情况，给出dept_no、emp_no以及salary ，当前表示to_date='9999-01-01' CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 这个没有很难，但是前面salary少写了一个字母就耽误了很长时间。这里不能INNNER JOIN dept_manager，否则就只生下来manager，写代码还是要有逻辑，不能瞎写。 SELECT de.dept_no,s.emp_no,s.salary FROM salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no AND s.to_date='9999-01-01' WHERE de.emp_no NOT IN (SELECT emp_no FROM dept_manager) AND de.to_date='9999-01-01'; 25.❌ 获取员工其当前的薪水比其manager当前薪水还高的相关信息，当前表示to_date='9999-01-01', 结果第一列给出员工的emp_no， 第二列给出其manager的manager_no， 第三列给出该员工当前的薪水emp_salary, 第四列给该员工对应的manager当前的薪水manager_salary CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 看到这种题目要学会分解，dept_emp和salaries能组成员工薪水表，dept_manager和salaies能组成经理薪水表，两者都共有一个dept_no，那么只要两者dept_no相同，再比较两表的薪水大小就ok了。 SELECT t1.emp_no,t2.emp_no AS manager_no,t1.salary AS emp_salary,t2.salary AS manager_salary FROM (SELECT s.emp_no,s.salary,de.dept_no FROM salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no AND s.to_date='9999-01-01' AND de.to_date='9999-01-01') t1 INNER JOIN (SELECT s.emp_no,s.salary,dm.dept_no FROM salaries s INNER JOIN dept_manager dm ON s.emp_no=dm.emp_no AND s.to_date='9999-01-01' AND dm.to_date='9999-01-01') t2 ON t1.dept_no=t2.dept_no WHERE t1.salary&gt;t2.salary; 这里还有一个点就是JOIN后的ON的表达式不仅局限于等于还可以使用大于等于，和where语句起到的作用相似。或者用下面这种写法，将复杂查询分为两个简单查询，再组合。salary表可以出现两次，用where字句起到类似join的作用。 SELECT s1.emp_no,s2.emp_no AS manager_no,s1.salary AS emp_salary,s2.salary AS manager_salary FROM dept_emp de,dept_manager dm,salaries s1,salaries s2 WHERE de.emp_no=s1.emp_no AND dm.emp_no=s2.emp_no AND de.dept_no=dm.dept_no AND s1.salary&gt;s2.salary AND de.to_date='9999-01-01' AND dm.to_date='9999-01-01' AND s1.to_date='9999-01-01' AND s2.to_date='9999-01-01' 汇总各个部门当前员工的title类型的分配数目，即结果给出部门编号dept_no、dept_name、其部门下所有的当前(dept_emp.to_date = '9999-01-01')员工的当前(titles.to_date = '9999-01-01')title以及该类型title对应的数目count，结果按照dept_no升序排序 (注：因为员工可能有离职，所有dept_emp里面to_date不为'9999-01-01'就已经离职了，不计入统计，而且员工可能有晋升，所以如果titles.to_date 不为 '9999-01-01'，那么这个可能是员工之前的职位信息，也不计入统计) CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE IF NOT EXISTS `titles` ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); 输入描述: 自己竟然写出来了 SELECT de.dept_no,d.dept_name,t.title,COUNT(de.emp_no) as count FROM dept_emp de INNER JOIN titles t ON de.emp_no=t.emp_no INNER JOIN departments d ON de.dept_no=d.dept_no WHERE de.to_date='9999-01-01' AND t.to_date='9999-01-01' GROUP BY de.dept_no,t.title; 这道题也不难，无脑连接后加上条件再groupby就ok了，倒是groupby的用法自己又快忘光了呢！ ❌ 给出每个员工每年薪水涨幅超过5000的员工编号emp_no、薪水变更开始日期from_date以及薪水涨幅值salary_growth，并按照salary_growth逆序排列。 提示：在sqlite中获取datetime时间对应的年份函数为strftime('%Y', to_date) (数据保证每个员工的每条薪水记录to_date-from_date=1年，而且同一员工的下一条薪水记录from_data=上一条薪水记录的to_data) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 如：插入 INSERT INTO salaries VALUES(10001,52117,'1986-06-26','1987-06-26'); INSERT INTO salaries VALUES(10001,62102,'1987-06-26','1988-06-25'); INSERT INTO salaries VALUES(10002,72527,'1996-08-03','1997-08-03'); INSERT INTO salaries VALUES(10002,72527,'1997-08-03','1998-08-03'); INSERT INTO salaries VALUES(10002,72527,'1998-08-03','1999-08-03'); INSERT INTO salaries VALUES(10003,43616,'1996-12-02','1997-12-02'); INSERT INTO salaries VALUES(10003,43466,'1997-12-02','1998-12-02'); 先放上自己的写法，对于同一个人相邻两条记录的比较我们可以直接让同样的表按某种条件内连接就ok SELECT s1.emp_no,s2.from_date,(s2.salary-s1.salary) AS salary_growth FROM salaries s1 INNER JOIN salaries s2 ON s1.emp_no=s2.emp_no AND s1.to_date=s2.from_date WHERE salary_growth&gt;5000 ORDER BY salary_growth DESC; 这道题评论也给我看晕了，主要是存在多次涨薪的问题，比如说在一年内多次涨薪的问题。给我整晕了。 SELECT s2.emp_no, s2.from_date, (s2.salary - s1.salary) AS salary_growth FROM salaries AS s1, salaries AS s2 WHERE s1.emp_no = s2.emp_no AND salary_growth &gt; 5000 AND (strftime(\"%Y\",s2.to_date) - strftime(\"%Y\",s1.to_date) = 1 OR strftime(\"%Y\",s2.from_date) - strftime(\"%Y\",s1.from_date) = 1 ) ORDER BY salary_growth DESC 写上去就算看一看strftime的用法吧。 28.❌ CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); 查找描述信息(film.description)中包含robot的电影对应的分类名称(category.name)以及电影数目(count(film.film_id))，而且还需要该分类包含电影总数量(count(film_category.category_id))&gt;=5部 第二个条件有点无语，不是太理解,正确答案的理解是这两个条件是并列关系。 select name,count(name) from film,film_category,category where film.description like '%robot%' and film.film_id= film_category.film_id and film_category.category_id= category.category_id and category.category_id in (select category_id from film_category group by category_id having count(film_id)&gt;=5) 自己的想法是先满足第一个条件后，再分组，不过通不过测试 SELECT c.name,COUNT(f.film_id) AS film_num FROM film f INNER JOIN film_category fc ON f.film_id=fc.film_id INNER JOIN category c ON c.category_id=fc.category_id WHERE f.description like '%robot%' GROUP BY c.category_id HAVING film_num&gt;=5; CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); 使用join查询方式找出没有分类的电影id以及名称 看清题目，判断是否为NULL，用IS SELECT f.film_id,f.title FROM film f LEFT JOIN film_category fc ON f.film_id=fc.film_id WHERE fc.category_id IS NULL; CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); 你能使用子查询的方式找出属于Action分类的所有电影对应的title,description吗 注意action的条件 SELECT f.title,f.description FROM film f INNER JOIN film_category fc ON f.film_id=fc.film_id INNER JOIN category c ON fc.category_id=c.category_id WHERE c.name='Action'; 题目还提到要用子查询的方式 SELECT title,description FROM film where film_id IN (SELECT film_id from film_category WHERE category_id in (SELECT category_id from category WHERE name='Action')) 将employees表的所有员工的last_name和first_name拼接起来作为Name，中间以一个空格区分 (注：sqllite,字符串拼接为 || 符号，不支持concat函数，mysql支持concat函数) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT last_name || ' ' || first_name FROM employees; 再看看CONCAT用法 CONCAT方法： select CONCAT(CONCAT(last_name,\" \"),first_name) as name from employees 或者 select CONCAT(last_name,\" \"，first_name) as name from employees 创建一个actor表，包含如下列信息 列表 类型 是否为NULL 含义 actor_id smallint(5) not null 主键id first_name varchar(45) not null 名字 last_name varchar(45) not null 姓氏 last_update date not null 日期 create table if not exists actor( actor_id smallint(5) not null , first_name varchar(45) not null, last_name varchar(45) not null, last_update timestamp not null default (datetime('now','localtime')), primary key(actor_id) ) 题目已经先执行了如下语句: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL) 请你对于表actor批量插入如下数据(不能有2条insert语句哦!) actor_id first_name last_name last_update 1 PENELOPE GUINESS 2006-02-15 12:34:33 2 NICK WAHLBERG 2006-02-15 12:34:33 INSERT INTO actor VALUES (1,'PENELOPE','GUINESS','2006-02-15 12:34:33'), (2,'NICK','WAHLBERG','2006-02-15 12:34:33') 题目已经先执行了如下语句: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL); insert into actor values ('3', 'WD', 'GUINESS', '2006-02-15 12:34:33'); 对于表actor插入如下数据,如果数据已经存在，请忽略(不支持使用replace操作) actor_id first_name last_name last_update ‘3’ ‘ED’ ‘CHASE’ ‘2006-02-15 12:34:33’ 题目已经先执行了如下语句: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL); insert into actor values ('3', 'WD', 'GUINESS', '2006-02-15 12:34:33'); 对于表actor插入如下数据,如果数据已经存在，请忽略(不支持使用replace操作) actor_id first_name last_name last_update ‘3’ ‘ED’ ‘CHASE’ ‘2006-02-15 12:34:33’ sqlite insert or ignore into actor values(3,'ED','CHASE','2006-02-15 12:34:33'); mysql insert IGNORE into actor values(3,'ED','CHASE','2006-02-15 12:34:33'); 对于如下表actor，其对应的数据为: actor_id first_name last_name last_update 1 PENELOPE GUINESS 2006-02-15 12:34:33 2 NICK WAHLBERG 2006-02-15 12:34:33 请你创建一个actor_name表，并且将actor表中的所有first_name以及last_name导入该表. actor_name表结构如下： 列表 类型 是否为NULL 含义 first_name varchar(45) not null 名字 last_name varchar(45) not null 姓氏 一种写法是先建表，然后插入 CREATE TABLE actor_name( first_name varchar(45) not null, last_name varchar(45) not null); INSERT INTO actor_name SELECT first_name,last_name FROM actor; 另一种是用查询到的数据建表 sqlite CREATE TABLE actor_name AS SELECT first_name,last_name FROM actor mysql CREATE TABLE actor_name SELECT first_name,last_name FROM actor; 针对如下表actor结构创建索引： (注:在 SQLite 中,除了重命名表和在已有的表中添加列,ALTER TABLE 命令不支持其他操作， mysql支持ALTER TABLE创建索引) CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); 对first_name创建唯一索引uniq_idx_firstname，对last_name创建普通索引idx_lastname CREATE UNIQUE INDEX uniq_idx_firstname ON actor(first_name); CREATE INDEX idx_lastname ON actor(last_name); mysql ALTER TABLE actor ADD UNIQUE INDEX uniq_idx_firstname(first_name); ALTER TABLE actor ADD INDEX idx_lastname(last_name); 针对actor表创建视图actor_name_view，只包含first_name以及last_name两列，并对这两列重新命名，first_name为first_name_v，last_name修改为last_name_v： CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); CREATE VIEW actor_name_view AS SELECT first_name first_name_v,last_name last_name_v FROM actor; CREATE VIEW actor_name_view (fist_name_v, last_name_v) AS SELECT first_name, last_name FROM actor 针对salaries表emp_no字段创建索引idx_emp_no，查询emp_no为10005, 使用强制索引。 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); create index idx_emp_no on salaries(emp_no); sqlite SELECT * FROM salaries INDEXED BY idx_emp_no WHERE emp_no=10005; mysql SELECT * FROM salaries FROCE INDEX BY idx_emp_no WHERE emp_no=10005; 存在actor表，包含如下列信息： CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); 现在在last_update后面新增加一列名字为create_date, 类型为datetime, NOT NULL，默认值为’2020-10-01 00:00:00’ ALTER TABLE actor ADD COLUMN create_date datetime not null default '2020-10-01 00:00:00' AFTER last_update; 构造一个触发器audit_log，在向employees_test表中插入一条数据的时候，触发插入相关的数据到audit中。 CREATE TABLE employees_test( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL ) CREATE TABLE audit( EMP_no INT NOT NULL, NAME TEXT NOT NULL ); begin和end之间的语句要用分号结束 CREATE TRIGGER audit_log AFTER INSERT ON employees_test begin INSERT INTO audit(EMP_no,NAME) VALUES (new.ID,new.NAME); end; 删除emp_no重复的记录，只保留最小的id对应的记录。 CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); 对groupby又产生了大大的疑惑，不是说select后面只能接groupby的相关字段吗 DELETE FROM titles_test WHERE id NOT IN ( SELECT MIN(id) FROM titles_test GROUP BY emp_no) 两两比较找出最大值或者最小值 delete from titles_test where id in( select a.id from titles_test a,titles_test b where a.emp_no=b.emp_no and a.id&gt;b.id) 将所有to_date为9999-01-01的全部更新为NULL,且 from_date更新为2001-01-01。 CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); 更新后的值: titles_test 表的值： id emp_no title from_date to_date 1 10001 Senior Engineer 2001-01-01 NULL 2 10002 Staff 2001-01-01 NULL 3 10003 Senior Engineer 2001-01-01 NULL 4 10004 Senior Engineer 2001-01-01 NULL 5 10001 Senior Engineer 2001-01-01 NULL 6 10002 Staff 2001-01-01 NULL 7 10003 Senior Engineer 2001-01-01 NULL UPDATE语句更新多列用逗号隔开哦~不是用AND UPDATE titles_test SET to_date=NULL,from_date='2001-01-01' WHERE to_date='9999-01-01'; 将id=5以及emp_no=10001的行数据替换成id=5以及emp_no=10005,其他数据保持不变，使用replace实现，直接使用update会报错了。 CREATE TABLE titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); mark一下replace的用法 UPDATE titles_test SET emp_no=replace(emp_no,10001,10005) WHERE id=5; 全字段更新替换,会插入一条新记录。并且要将所有字段的值写出，否则将置为空。 REPLACE INTO titles_test VALUES (5, 10005, 'Senior Engineer', '1986-06-26', '9999-01-01') 将titles_test表名修改为titles_2017。 CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); sqlite ALTER TABLE titles_test RENAME TO titles_2017; mysql alter table titles_test rename titles_2017 在audit表上创建外键约束，其emp_no对应employees_test表的主键id。 (以下2个表已经创建了) CREATE TABLE employees_test( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL ); CREATE TABLE audit( EMP_no INT NOT NULL, create_date datetime NOT NULL ); 先看mysql的用法 ALTER TABLE audit ADD FOREIGN KEY (emp_no) REFERENCES employees_test (id); 再看sqlite,注意外键格式 DROP TABLE audit; CREATE TABLE audit( emp_no INT NOT NULL, create_date datetime NOT NULL, FOREIGN KEY(emp_no) REFERENCES employees_test(id)) 请你写出更新语句，将所有获取奖金的员工当前的(salaries.to_date='9999-01-01')薪水增加10%。(emp_bonus里面的emp_no都是当前获奖的所有员工) create table emp_bonus( emp_no int not null, btype smallint not null); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 如： INSERT INTO emp_bonus VALUES (10001,1); INSERT INTO salaries VALUES(10001,85097,'2001-06-22','2002-06-22'); INSERT INTO salaries VALUES(10001,88958,'2002-06-22','9999-01-01'); UPDATE salaries SET salary=salary*1.1 WHERE emp_no IN (SELECT emp_no FROM emp_bonus) AND to_date='9999-01-01'; 将employees表中的所有员工的last_name和first_name通过(')连接起来。(sqlite不支持concat，请用||实现，mysql支持concat) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 输出格式: SELECT last_name || \"'\" || first_name FROM employees; SELECT CONCAT(last_name, '''', first_name) as name FROM employees; 查找字符串’10,A,B’ 中逗号’,'出现的次数cnt。 技巧题哈，学习了LENGTH 和REPLACE的用法 SELECT LENGTH('10,A,B' )-LENGTH(REPLACE('10,A,B',',','')) AS cnt 获取Employees中的first_name，查询按照first_name最后两个字母，按照升序进行排列 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT first_name FROM (SELECT first_name,SUBSTR(first_name,-2) AS first_name_2 FROM employees) ORDER BY first_name_2 ASC; SELECT first_name FROM employees ORDER BY substr(first_name,-2,2) asc; SELECT first_name from employees order by right(first_name, 2); 按照dept_no进行汇总，属于同一个部门的emp_no按照逗号进行连接，结果给出dept_no以及连接出的结果employees CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); 输出格式: 记录一下GROUP_CONCAT的用法 sqlite SELECT dept_no,GROUP_CONCAT(emp_no) AS employees FROM dept_emp GROUP BY dept_no; mysql SELECT dept_no, group_concat(DISTINCT emp_no ORDER BY emp_no ASC SEPARATOR ',') AS employees FROM dept_emp GROUP BY dept_no; 查找排除最大、最小salary之后的当前(to_date = '9999-01-01' )员工的平均工资avg_salary。 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 关于时间的限制 SELECT AVG(salary) AS avg_salary FROM salaries WHERE to_date = '9999-01-01' AND salary NOT IN (SELECT MAX(salary) FROM salaries WHERE to_date = '9999-01-01') AND salary NOT IN (SELECT MIN(salary) FROM salaries WHERE to_date = '9999-01-01') SELECT AVG(salary) FROM salaries WHERE salary&lt;&gt;(SELECT MAX(salary) FROM salaries WHERE to_date='9999-01-01') AND salary&lt;&gt;(SELECT MIN(salary) FROM salaries WHERE to_date='9999-01-01') AND to_date='9999-01-01' 分页查询employees表，每5行一页，返回第2页的数据 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); LIMIT 后的数字代表返回几条记录，OFFSET 后的数字代表从第几条记录开始返回 SELECT * FROM employees LIMIT 5 OFFSET 5; 在 LIMIT X,Y 中，Y代表返回几条记录，X代表从第几条记录开始返回（第一条记录序号为0），切勿记反。 获取所有员工的emp_no、部门编号dept_no以及对应的bonus类型btype和received，没有分配奖金的员工不显示对应的bonus类型btype和received CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `emp_bonus`( emp_no int(11) NOT NULL, received datetime NOT NULL, btype smallint(5) NOT NULL); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT e.emp_no,de.dept_no,eb.btype,eb.received FROM employees e INNER JOIN dept_emp de ON e.emp_no=de.emp_no LEFT JOIN emp_bonus eb ON e.emp_no=eb.emp_no 使用含有关键字exists查找未分配具体部门的员工的所有信息。 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); 这个EXISTS当作条件判断得理解理解 IN是先执行子查询，得到一个结果集，将结果集代入外层谓词条件执行主查询，子查询只需要执行一次 EXISTS是先从主查询中取得一条数据，再代入到子查询中，执行一次子查询，判断子查询是否能返回结果，主查询有多少条数据，子查询就要执行多少次,EXISTS中要添加判断条件？？？ SELECT * FROM employees WHERE NOT EXISTS (SELECT emp_no FROM dept_emp WHERE employees.emp_no=dept_emp.emp_no) 获取有奖金的员工相关信息。 CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); create table emp_bonus( emp_no int not null, received datetime not null, btype smallint not null); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 给出emp_no、first_name、last_name、奖金类型btype、对应的当前薪水情况salary以及奖金金额bonus。 bonus类型btype为1其奖金为薪水salary的10%，btype为2其奖金为薪水的20%，其他类型均为薪水的30%。 当前薪水表示to_date='9999-01-01' SELECT e.emp_no,e.first_name,e.last_name,eb.btype,s.salary,s.salary*eb.btype*0.1 as bonus FROM employees e INNER JOIN emp_bonus eb ON e.emp_no=eb.emp_no INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.to_date='9999-01-01' 看看CASE WHEN THEN END的用法 SELECT e.emp_no,e.first_name,e.last_name,eb.btype,s.salary, (CASE eb.btype WHEN 1 THEN s.salary*0.1 WHEN 2 THEN s.salary*0.2 WHEN 3 THEN s.salary*0.3 END) AS bonus FROM employees e INNER JOIN emp_bonus eb ON e.emp_no=eb.emp_no INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.to_date='9999-01-01' 按照salary的累计和running_total，其中running_total为前N个当前( to_date = '9999-01-01')员工的salary累计和，其他以此类推。 具体结果如下Demo展示。。 CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); 输出格式: 让我们来学习一种新的求和方法，两表相同按某列拍好序后，比较大小即可求和。 SELECT emp_no,salary, (select sum(s1.salary) FROM salaries s1 WHERE s1.emp_no&lt;=s2.emp_no AND s1.to_date='9999-01-01') AS running_total FROM salaries s2 WHERE to_date='9999-01-01' ORDER BY s2.emp_no SELECT emp_no,salary, SUM(salary) OVER(ORDER BY emp_no) AS running_total FROM salaries WHERE to_date= '9999-01-01'; 58.❌ 对于employees表中，输出first_name排名(按first_name升序排序)为奇数的first_name CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); 对于这个COUNT的用法还是存疑(聚合或是满足某种条件之后？？？)，可以用于计数，两两比较之间的计数，用于排序 SELECT first_name FROM employees e1 WHERE (SELECT COUNT(*) FROM employees e2 WHERE e1.first_name&gt;=e2.first_name)%2=1; 用row_number() over 进行排序 select tt.first_name from (select first_name, row_number() over (order by first_name) as row_idx from employees) as tt where tt.row_idx%2=1; SELECT e.first_name FROM employees e JOIN ( SELECT first_name , ROW_NUMBER() OVER(ORDER BY first_name ASC) AS r_num FROM employees ) AS t ON e.first_name = t.first_name WHERE t.r_num % 2 = 1; 在牛客刷题的小伙伴们都有着牛客积分，积分(grade)表简化可以如下: SELECT number FROM grade GROUP BY number HAVING COUNT(id)&gt;=3 这个思路也很牛逼:要三次以上的积分，那么肯定要查找3个id不同但是积分相同的情况 SELECT DISTINCT g1.number AS times FROM grade g1, grade g2, grade g3 WHERE g1.id != g2.id AND g2.id != g3.id AND g1.id !=g3.id AND g1.number = g2.number AND g2.number = g3.number 60.❌ 在牛客刷题有一个通过题目个数的(passing_number)表，id是主键，简化如下: 第1行表示id为1的用户通过了4个题目; … 第6行表示id为6的用户通过了4个题目; 请你根据上表，输出通过的题目的排名，通过题目个数相同的，排名相同，此时按照id升序排列，数据如下: SELECT id,number,DENSE_RANK() OVER (ORDER BY number DESC) AS t_rank FROM passing_number ORDER BY t_rank,id ASC; 这种做法也得理解理解 select a.id,a.number, (select count(distinct b.number) from passing_number b where b.number&gt;=a.number ) from passing_number a order by a.number desc, a.id asc; 有一个person表，主键是id，如下: 有一个任务(task)表如下，主键也是id，如下: 请你找到每个人的任务情况，并且输出出来，没有任务的也要输出，而且输出结果按照person的id升序排序，输出情况如下: SELECT person.id,person.name,task.content FROM person LEFT JOIN task ON person.id=task.person_id ORDER BY person.id ASC; 现在有一个需求，让你统计正常用户发送给正常用户邮件失败的概率: 有一个邮件(email)表，id为主键， type是枚举类型，枚举成员为(completed，no_completed)，completed代表邮件发送是成功的，no_completed代表邮件是发送失败的。简况如下: 第1行表示为id为2的用户在2020-01-11成功发送了一封邮件给了id为3的用户; … 第3行表示为id为1的用户在2020-01-11没有成功发送一封邮件给了id为4的用户; … 第6行表示为id为4的用户在2020-01-12成功发送了一封邮件给了id为1的用户; 下面是一个用户(user)表，id为主键，is_blacklist为0代表为正常用户，is_blacklist为1代表为黑名单用户，简况如下: 结果表示: 2020-01-11失败的概率为0.500，因为email的第1条数据，发送的用户id为2是黑名单用户，所以不计入统计，正常用户发正常用户总共2次，但是失败了1次，所以概率是0.500; 2020-01-12没有失败的情况，所以概率为0.000. (注意: sqlite 1/2得到的不是0.5，得到的是0，只有1*1.0/2才会得到0.5，sqlite四舍五入的函数为round) 注意先乘1.0，不然会得到0 SELECT e.date,ROUND( SUM(CASE e.type WHEN 'completed' THEN 0 WHEN 'no_completed' THEN 1 END)*1.0/COUNT(e.type),3) AS p FROM email e INNER JOIN user u1 ON e.send_id=u1.id INNER JOIN user u2 ON e.receive_id=u2.id WHERE u1.is_blacklist=0 AND u2.is_blacklist=0 GROUP BY e.date ORDER BY e.date; 牛客每天有很多人登录，请你统计一下牛客每个用户最近登录是哪一天。 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备登录了牛客网 。。。 第4行表示id为3的用户在2020-10-13使用了客户端id为2的设备登录了牛客网 请你写出一个sql语句查询每个用户最近一天登录的日子，并且按照user_id升序排序，上面的例子查询结果如下: 老师说过select 的字段要包含在group或者要用聚合函数 SELECT user_id,MAX(date) d FROM login GROUP BY user_id ORDER BY user_id ASC; 牛客每天有很多人登录，请你统计一下牛客每个用户最近登录是哪一天，用的是什么设备. 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备登录了牛客网 。。。 第4行表示id为3的用户在2020-10-13使用了客户端id为2的设备登录了牛客网 还有一个用户(user)表，简况如下: 请你写出一个sql语句查询每个用户最近一天登录的日子，用户的名字，以及用户用的设备的名字，并且查询结果按照user的name升序排序，上面的例子查询结果如下 老生常谈，group by后select后只能跟聚合和group by的字段 SELECT u.name,c.name,l.date FROM login l INNER JOIN user u ON l.user_id=u.id INNER JOIN client c ON l.client_id=c.id INNER JOIN (SELECT user_id,MAX(date) max_date FROM login GROUP BY user_id) AS T ON u.id=T.user_id WHERE l.date=T.max_date ORDER BY u.name ASC 这个答案更简明 select u.name,c.name,l1.date from login l1,user u,client c where l1.date=(select max(l2.date) from login l2 where l1.user_id=l2.user_id) and l1.user_id=u.id and l1.client_id=c.id order by u.name 总结一下，遇到取最大最小值的问题或者是计数，可以对主键进行连接，然后增加判断条件，排序就用大于小于等于，最大最小就用max min 牛客每天有很多人登录，请你统计一下牛客新登录用户的次日成功的留存率， 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备第一次新登录了牛客网 。。。 第4行表示id为3的用户在2020-10-12使用了客户端id为2的设备登录了牛客网 。。。 最后1行表示id为1的用户在2020-10-14使用了客户端id为2的设备登录了牛客网 请你写出一个sql语句查询新登录用户次日成功的留存率，即第1天登陆之后，第2天再次登陆的概率,保存小数点后面3位(3位之后的四舍五入)，上面的例子查询结果如下: (sqlite里查找某一天的后一天的用法是:date(yyyy-mm-dd, ‘+1 day’)，四舍五入的函数为round，sqlite 1/2得到的不是0.5，得到的是0，只有1*1.0/2才会得到0.5 mysql里查找某一天的后一天的用法是:DATE_ADD(yyyy-mm-dd,INTERVAL 1 DAY)，四舍五入的函数为round) 牛客每天有很多人登录，请你统计一下牛客每个日期登录新用户个数， 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备登录了牛客网，因为是第1次登录，所以是新用户 。。。 第4行表示id为2的用户在2020-10-13使用了客户端id为2的设备登录了牛客网，因为是第2次登录，所以是老用户 。。 最后1行表示id为4的用户在2020-10-15使用了客户端id为1的设备登录了牛客网，因为是第2次登录，所以是老用户请你写出一个sql语句查询每个日期登录新用户个数，并且查询结果按照日期升序排序，上面的例子查询结果如下: 牛客每天有很多人登录，请你统计一下牛客每个日期新用户的次日留存率。 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备登录了牛客网，因为是第1次登录，所以是新用户 。。。 第4行表示id为2的用户在2020-10-13使用了客户端id为2的设备登录了牛客网，因为是第2次登录，所以是老用户 。。 最后1行表示id为4的用户在2020-10-15使用了客户端id为1的设备登录了牛客网，因为是第2次登录，所以是老用户 请你写出一个sql语句查询每个日期新用户的次日留存率，结果保留小数点后面3位数(3位之后的四舍五入)，并且查询结果按照日期升序排序，上面的例子查询结果如下: SELECT ROUND(COUNT(DISTINCT l2.user_id)*1.0/COUNT(DISTINCT l1.user_id),3) FROM login l1,login l2 WHERE l1.user_id=l2.user_id AND l2.date=date(l1.date, '+1 day')； 牛客每天有很多人登录，请你统计一下牛客每个日期登录新用户个数， 有一个登录(login)记录表，简况如下: 慢慢想吧 select a.date, sum(case when rank=1 then 1 else 0 end) new from (select date, row_number() over(partition by user_id order by date) rank from login) a group by date; select date,sum(t) from ( select date, case when (user_id,date) in (select user_id,min(date) from login group by user_id) then 1 else 0 end as t from login ) group by date order by date asc 牛客每天有很多人登录，请你统计一下牛客每个日期新用户的次日留存率。 有一个登录(login)记录表，简况如下: 70.❌ 牛客每天有很多人登录，请你统计一下牛客每个用户查询刷题信息，包括: 用户的名字，以及截止到某天，累计总共通过了多少题。 不存在没有登录却刷题的情况，但是存在登录了没刷题的情况，不会存在刷题表里面，有提交代码没有通过的情况，但是会记录在刷题表里，只不过通过数目是0。 有一个登录(login)记录表，简况如下: 第1行表示id为2的用户在2020-10-12使用了客户端id为1的设备登录了牛客网 。。。 第5行表示id为3的用户在2020-10-13使用了客户端id为2的设备登录了牛客网 有一个刷题（passing_number)表，简况如下: 第1行表示id为2的用户在2020-10-12通过了4个题目。 。。。 第3行表示id为1的用户在2020-10-13提交了代码但是没有通过任何题目。 第4行表示id为4的用户在2020-10-13通过了2个题目 还有一个用户(user)表，简况如下: 请你写出一个sql语句查询刷题信息，包括: 用户的名字，以及截止到某天，累计总共通过了多少题，并且查询结果先按照日期升序排序，再按照姓名升序排序，有登录却没有刷题的哪一天的数据不需要输出，上面的例子查询结果如下: 题目的含义是各个用户在不同时间累积答题求和，采用sum函数进行开窗处理，将user_id进行分区，再通过时间升序排序，进而实现了在每个user_id分区中以升序日期排序的通过题数的逐个递加 SELECT name,date, SUM(number) over(partition by user_id order by date) FROM passing_number p LEFT JOIN user u ON p.user_id=u.id ORDER BY date,name 还有一种是同一张表内连接的方法，这个得花时间理解，对于这种分组累加很有必要 SELECT u.name,pn1.date,SUM(pn2.number) FROM user u INNER JOIN passing_number pn1 ON u.id=pn1.user_id INNER JOIN passing_number pn2 ON pn1.user_id=pn2.user_id WHERE pn1.date&gt;=pn2.date GROUP BY pn1.id,pn1.date ORDER BY pn1.date ASC,u.name ASC; 牛客每次考试完，都会有一个成绩表(grade)，如下: 第1行表示用户id为1的用户选择了C++岗位并且考了11001分 。。。 第8行表示用户id为8的用户选择了前端岗位并且考了9999分 请你写一个sql语句查询各个岗位分数的平均数，并且按照分数降序排序，结果保留小数点后面3位(3位之后四舍五入): SELECT job,ROUND(AVG(score),3) AS avg FROM grade GROUP BY job ORDER BY avg DESC; 牛客每次考试完，都会有一个成绩表(grade)，如下: 第1行表示用户id为1的用户选择了C++岗位并且考了11001分 。。。 第8行表示用户id为8的用户选择了前端岗位并且考了9999分 请你写一个sql语句查询用户分数大于其所在工作(job)分数的平均分的所有grade的属性，并且以id的升序排序，如下: 感觉自己写的很烂 SELECT g.id,g.job,g.score FROM grade g,(SELECT job,AVG(score) avg_score FROM grade GROUP BY job) AS t WHERE g.job=t.job AND g.score&gt;t.avg_score ORDER BY g.id ASC; 这个开窗函数看起来就很简洁 SELECT id,job,score FROM (SELECT *,AVG(score) OVER (PARTITION BY job) avg_score FROM grade) WHERE score&gt;avg_score ORDER BY id; 牛客每次举办企业笔试的时候，企业一般都会有不同的语言岗位，比如C++工程师，JAVA工程师，Python工程师，每个用户笔试完有不同的分数，现在有一个分数(grade)表简化如下: 第1行表示用户id为1的选择了language_id为1岗位的最后考试完的分数为12000， … 第7行表示用户id为7的选择了language_id为2岗位的最后考试完的分数为11000， 不同的语言岗位(language)表简化如下: 请你找出每个岗位分数排名前2的用户，得到的结果先按照language的name升序排序，再按照积分降序排序，最后按照grade的id升序排序，得到结果如下: SELECT id,name,score FROM (SELECT g.id AS id,l.name AS name,g.score AS score,DENSE_RANK() OVER (PARTITION BY g.language_id ORDER BY score DESC) AS rank_score FROM grade g INNER JOIN language l ON g.language_id=l.id ORDER BY l.name ASC,score DESC,id ASC) WHERE rank_score&lt;=2 牛客每次考试完，都会有一个成绩表(grade)，如下: 第1行表示用户id为1的用户选择了C++岗位并且考了11001分 。。。 第8行表示用户id为8的用户选择了前端岗位并且考了9999分 请你写一个sql语句查询各个岗位分数升序排列之后的中位数位置的范围，并且按job升序排序，结果如下: 解释: 第1行表示C岗位的中位数位置范围为[2,2]，也就是2。因为C岗位总共3个人，是奇数，所以中位数位置为2是正确的(即位置为2的10000是中位数) 第2行表示Java岗位的中位数位置范围为[1,2]。因为Java岗位总共2个人，是偶数，所以要知道中位数，需要知道2个位置的数字，而因为只有2个人，所以中位数位置为[1,2]是正确的(即需要知道位置为1的12000与位置为2的13000才能计算出中位数为12500) 第3行表示前端岗位的中位数位置范围为[2,2]，也就是2。因为前端岗位总共3个人，是奇数，所以中位数位置为2是正确的(即位置为2的11000是中位数) (注意: sqlite 1/2得到的不是0.5，得到的是0，只有1*1.0/2才会得到0.5，sqlite四舍五入的函数为round，sqlite不支持floor函数，支持cast(x as integer) 函数，不支持if函数，支持case when …then …else …end函数) 牛客每次考试完，都会有一个成绩表(grade)，如下: 第1行表示用户id为1的用户选择了C++岗位并且考了11001分 。。。 第8行表示用户id为8的用户选择了前端岗位并且考了9999分 请你写一个sql语句查询各个岗位分数的中位数位置上的所有grade信息，并且按id升序排序，结果如下: ==================================================================================== 力扣刷题 ==================================================================================== 表1: Person ±------------±--------+ | 列名 | 类型 | ±------------±--------+ | PersonId | int | | FirstName | varchar | | LastName | varchar | ±------------±--------+ PersonId 是上表主键 表2: Address ±------------±--------+ | 列名 | 类型 | ±------------±--------+ | AddressId | int | | PersonId | int | | City | varchar | | State | varchar | ±------------±--------+ AddressId 是上表主键 编写一个 SQL 查询，满足条件：无论 person 是否有地址信息，都需要基于上述两表提供 person 的以下信息： FirstName, LastName, City, State SELECT p.FirstName,p.Lastname,a.City,a.State FROM Person p LEFT JOIN Address a on p.PersonId=a.PersonId; 编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary） 。 ±—±-------+ | Id | Salary | ±—±-------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | ±—±-------+ 例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null。 ±--------------------+ | SecondHighestSalary | ±--------------------+ | 200 | ±--------------------+ SELECT MAX(e1.Salary) SecondHighestSalary FROM Employee e1 WHERE e1.Salary&lt;(SELECT MAX(e2.Salary) FROM Employee e2); 编写一个 SQL 查询，获取 Employee 表中第 n 高的薪水（Salary）。 ±—±-------+ | Id | Salary | ±—±-------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | ±—±-------+ 例如上述 Employee 表，n = 2 时，应返回第二高的薪水 200。如果不存在第 n 高的薪水，那么查询应返回 null。 ±-----------------------+ | getNthHighestSalary(2) | ±-----------------------+ | 200 | ±-----------------------+ 服了，rank()是mysql自带的函数，不能用于变量命名 CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN RETURN ( # Write your MySQL query statement below. SELECT DISTINCT Salary FROM ( SELECT DENSE_RANK() OVER(ORDER BY Salary DESC) AS rank_s, Salary FROM Employee ) AS t WHERE rank_s=N ); END 编写一个 SQL 查询来实现分数排名。 如果两个分数相同，则两个分数排名（Rank）相同。请注意，平分后的下一个名次应该是下一个连续的整数值。换句话说，名次之间不应该有“间隔”。 ±—±------+ | Id | Score | ±—±------+ | 1 | 3.50 | | 2 | 3.65 | | 3 | 4.00 | | 4 | 3.85 | | 5 | 4.00 | | 6 | 3.65 | ±—±------+ 例如，根据上述给定的 Scores 表，你的查询应该返回（按分数从高到低排列）： ±------±-----+ | Score | Rank | ±------±-----+ | 4.00 | 1 | | 4.00 | 1 | | 3.85 | 2 | | 3.65 | 3 | | 3.65 | 3 | | 3.50 | 4 | ±------±-----+ 重要提示：对于 MySQL 解决方案，如果要转义用作列名的保留字，可以在关键字之前和之后使用撇号。例如 Rank SELECT Score,DENSE_RANK() OVER(ORDER BY Score DESC) AS 'Rank' FROM Scores; 180.❗ 编写一个 SQL 查询，查找所有至少连续出现三次的数字。 ±—±----+ | Id | Num | ±—±----+ | 1 | 1 | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 1 | | 6 | 2 | | 7 | 2 | ±—±----+ 例如，给定上面的 Logs 表， 1 是唯一连续出现至少三次的数字。 ±----------------+ | ConsecutiveNums | ±----------------+ | 1 | ±----------------+ SELECT DISTINCT l1.num ConsecutiveNums FROM logs l1 INNER JOIN logs l2 ON l1.Num=l2.Num AND l1.Id-l2.Id between 0 and 2 GROUP BY l1.id HAVING COUNT(l2.id)&gt;2; SELECT DISTINCT l1.num ConsecutiveNums FROM logs l1 INNER JOIN logs l2 ON l1.Num=l2.Num GROUP BY l1.id HAVING SUM(l1.id-l2.id BETWEEN 0 AND 2)&gt;2; Employee 表包含所有员工，他们的经理也属于员工。每个员工都有一个 Id，此外还有一列对应员工的经理的 Id。 ±—±------±-------±----------+ | Id | Name | Salary | ManagerId | ±—±------±-------±----------+ | 1 | Joe | 70000 | 3 | | 2 | Henry | 80000 | 4 | | 3 | Sam | 60000 | NULL | | 4 | Max | 90000 | NULL | ±—±------±-------±----------+ 给定 Employee 表，编写一个 SQL 查询，该查询可以获取收入超过他们经理的员工的姓名。在上面的表格中，Joe 是唯一一个收入超过他的经理的员工。 ±---------+ | Employee | ±---------+ | Joe | ±---------+ SELECT e1.name Employee FROM Employee e1 INNER JOIN Employee e2 ON e1.ManagerId=e2.Id WHERE e1.Salary&gt;e2.Salary; 写一个 SQL 查询，查找 Person 表中所有重复的电子邮箱。 示例： ±—±--------+ | Id | Email | ±—±--------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | ±—±--------+ 根据以上输入，你的查询应返回以下结果： ±--------+ | Email | ±--------+ | a@b.com | ±--------+ SELECT Email FROM Person GROUP BY Email HAVING COUNT(Email)&gt;1 下面这个倒是用来判断重复的好办法 select distinct(p1.Email) from Person p1 inner join Person p2 on p1.Email = p2.Email and p1.Id != p2.Id; 某网站包含两个表，Customers 表和 Orders 表。编写一个 SQL 查询，找出所有从不订购任何东西的客户。 Customers 表： ±—±------+ | Id | Name | ±—±------+ | 1 | Joe | | 2 | Henry | | 3 | Sam | | 4 | Max | ±—±------+ Orders 表： ±—±-----------+ | Id | CustomerId | ±—±-----------+ | 1 | 3 | | 2 | 1 | ±—±-----------+ 例如给定上述表格，你的查询应返回： ±----------+ | Customers | ±----------+ | Henry | | Max | ±----------+ SELECT Name Customers FROM Customers WHERE Customers.Id NOT IN (SELECT CustomerId FROM Orders); SELECT c.Name Customers FROM Customers c LEFT JOIN Orders O ON c.Id=o.CustomerId WHERE o.ID IS NULL Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id。 ±—±------±-------±-------------+ | Id | Name | Salary | DepartmentId | ±—±------±-------±-------------+ | 1 | Joe | 70000 | 1 | | 2 | Jim | 90000 | 1 | | 3 | Henry | 80000 | 2 | | 4 | Sam | 60000 | 2 | | 5 | Max | 90000 | 1 | ±—±------±-------±-------------+ Department 表包含公司所有部门的信息。 ±—±---------+ | Id | Name | ±—±---------+ | 1 | IT | | 2 | Sales | ±—±---------+ 编写一个 SQL 查询，找出每个部门工资最高的员工。对于上述表，您的 SQL 查询应返回以下行（行的顺序无关紧要）。 ±-----------±---------±-------+ | Department | Employee | Salary | ±-----------±---------±-------+ | IT | Max | 90000 | | IT | Jim | 90000 | | Sales | Henry | 80000 | ±-----------±---------±-------+ #DENSE_RANK() PARTITION BY 完美结合 SELECT Department,Employee,Salary FROM (SELECT e.Name Employee,e.Salary Salary,d.Name Department, DENSE_RANK() OVER(PARTITION BY e.DepartmentId ORDER BY e.Salary DESC) AS s_rank FROM Employee e INNER JOIN Department d ON e.DepartmentId=d.Id) t WHERE t.s_rank=1; 以后对于这种group by 后有字段不能取出来的，用这种方法 IN,或者是group by后需要当作条件的 select d.Name Department,e.Name Employee,Salary from Employee e join Department d on e.DepartmentId=d.Id where(e.DepartmentId , Salary) IN( select DepartmentId, max(salary) from Employee group by DepartmentId ); Employee 表包含所有员工信息，每个员工有其对应的工号 Id，姓名 Name，工资 Salary 和部门编号 DepartmentId 。 ±—±------±-------±-------------+ | Id | Name | Salary | DepartmentId | ±—±------±-------±-------------+ | 1 | Joe | 85000 | 1 | | 2 | Henry | 80000 | 2 | | 3 | Sam | 60000 | 2 | | 4 | Max | 90000 | 1 | | 5 | Janet | 69000 | 1 | | 6 | Randy | 85000 | 1 | | 7 | Will | 70000 | 1 | ±—±------±-------±-------------+ Department 表包含公司所有部门的信息。 ±—±---------+ | Id | Name | ±—±---------+ | 1 | IT | | 2 | Sales | ±—±---------+ 编写一个 SQL 查询，找出每个部门获得前三高工资的所有员工。例如，根据上述给定的表，查询结果应返回： ±-----------±---------±-------+ | Department | Employee | Salary | ±-----------±---------±-------+ | IT | Max | 90000 | | IT | Randy | 85000 | | IT | Joe | 85000 | | IT | Will | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | ±-----------±---------±-------+ #PARTITION BY 真好用 SELECT Department,Employee,Salary FROM (SELECT d.Name Department,e.Name Employee,e.Salary Salary, DENSE_RANK() OVER(PARTITION BY e.DepartmentId ORDER BY e.Salary DESC) AS s_rank FROM Employee e INNER JOIN Department d ON e.DepartmentId=d.Id) t WHERE t.s_rank&lt;=3; 自连接解法，这种通过自连接判断排名，一定要记得使用distinct select d.Name as Department,e.Name as Employee,e.Salary as Salary from Employee as e left join Department as d on e.DepartmentId = d.Id where e.Id in ( select e1.Id from Employee as e1 left join Employee as e2 on e1.DepartmentId = e2.DepartmentId and e1.Salary &lt; e2.Salary group by e1.Id having count(distinct e2.Salary) &lt;= 2 ) and e.DepartmentId in (select Id from Department) order by d.Id asc,e.Salary desc 编写一个 SQL 查询，来删除 Person 表中所有重复的电子邮箱，重复的邮箱里只保留 Id 最小 的那个。 ±—±-----------------+ | Id | Email | ±—±-----------------+ | 1 | john@example.com | | 2 | bob@example.com | | 3 | john@example.com | ±—±-----------------+ Id 是这个表的主键。 例如，在运行你的查询语句之后，上面的 Person 表应返回以下几行: ±—±-----------------+ | Id | Email | ±—±-----------------+ | 1 | john@example.com | | 2 | bob@example.com | ±—±-----------------+ #mysql删除时，先缓存再删除 DELETE FROM Person WHERE Id NOT IN (SELECT Id FROM (SELECT MIN(Id) Id FROM Person GROUP BY Email) t) 这种理解也很好 delete p1 from person p1 join person p2 on p1.email=p2.email and p1.id&gt;p2.id 表 Weather ±--------------±--------+ | Column Name | Type | ±--------------±--------+ | id | int | | recordDate | date | | temperature | int | ±--------------±--------+ id 是这个表的主键 该表包含特定日期的温度信息 编写一个 SQL 查询，来查找与之前（昨天的）日期相比温度更高的所有日期的 id 。 返回结果 不要求顺序 。 查询结果格式如下例： Weather ±—±-----------±------------+ | id | recordDate | Temperature | ±—±-----------±------------+ | 1 | 2015-01-01 | 10 | | 2 | 2015-01-02 | 25 | | 3 | 2015-01-03 | 20 | | 4 | 2015-01-04 | 30 | ±—±-----------±------------+ Result table: ±—+ | id | ±—+ | 2 | | 4 | ±—+ 2015-01-02 的温度比前一天高（10 -&gt; 25） 2015-01-04 的温度比前一天高（20 -&gt; 30） select date_add(@dt, interval 1 day); -- add 1 day select date_add(@dt, interval 1 hour); -- add 1 hour select date_add(@dt, interval 1 minute); -- ... select date_add(@dt, interval 1 second); select date_add(@dt, interval 1 microsecond); select date_add(@dt, interval 1 week); select date_add(@dt, interval 1 month); select date_add(@dt, interval 1 quarter); select date_add(@dt, interval 1 year); select date_sub('1998-01-01 00:00:00', interval '1 1:1:1' day_second); MySQL datediff(date1,date2)：两个日期相减 date1 - date2，返回天数。 select datediff('2008-08-08', '2008-08-01'); -- 7 select datediff('2008-08-01', '2008-08-08'); -- -7 select str_to_date('08/09/2008', '%m/%d/%Y'); -- 2008-08-09 select str_to_date('08/09/08' , '%m/%d/%y'); -- 2008-08-09 select str_to_date('08.09.2008', '%m.%d.%Y'); -- 2008-08-09 select str_to_date('08:09:30', '%h:%i:%s'); -- 08:09:30 select str_to_date('08.09.2008 08:09:30', '%m.%d.%Y %h:%i:%s'); -- 2008-08-09 08:09:30 SELECT a.id FROM Weather a INNER JOIN Weather b ON a.recordDate=DATE_ADD(b.recordDate,INTERVAL 1 day) WHERE a.temperature&gt;b.temperature; Trips 表中存所有出租车的行程信息。每段行程有唯一键 Id，Client_Id 和 Driver_Id 是 Users 表中 Users_Id 的外键。Status 是枚举类型，枚举成员为 (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’)。 ±—±----------±----------±--------±-------------------±---------+ | Id | Client_Id | Driver_Id | City_Id | Status |Request_at| ±—±----------±----------±--------±-------------------±---------+ | 1 | 1 | 10 | 1 | completed |2013-10-01| | 2 | 2 | 11 | 1 | cancelled_by_driver|2013-10-01| | 3 | 3 | 12 | 6 | completed |2013-10-01| | 4 | 4 | 13 | 6 | cancelled_by_client|2013-10-01| | 5 | 1 | 10 | 1 | completed |2013-10-02| | 6 | 2 | 11 | 6 | completed |2013-10-02| | 7 | 3 | 12 | 6 | completed |2013-10-02| | 8 | 2 | 12 | 12 | completed |2013-10-03| | 9 | 3 | 10 | 12 | completed |2013-10-03| | 10 | 4 | 13 | 12 | cancelled_by_driver|2013-10-03| ±—±----------±----------±--------±-------------------±---------+ Users 表存所有用户。每个用户有唯一键 Users_Id。Banned 表示这个用户是否被禁止，Role 则是一个表示（‘client’, ‘driver’, ‘partner’）的枚举类型。 ±---------±-------±-------+ | Users_Id | Banned | Role | ±---------±-------±-------+ | 1 | No | client | | 2 | Yes | client | | 3 | No | client | | 4 | No | client | | 10 | No | driver | | 11 | No | driver | | 12 | No | driver | | 13 | No | driver | ±---------±-------±-------+ 写一段 SQL 语句查出 2013年10月1日 至 2013年10月3日 期间非禁止用户的取消率。基于上表，你的 SQL 语句应返回如下结果，取消率（Cancellation Rate）保留两位小数。 取消率的计算方式如下：(被司机或乘客取消的非禁止用户生成的订单数量) / (非禁止用户生成的订单总数) ±-----------±------------------+ | Day | Cancellation Rate | ±-----------±------------------+ | 2013-10-01 | 0.33 | | 2013-10-02 | 0.00 | | 2013-10-03 | 0.50 | ±-----------±------------------+ SELECT t.Request_at 'Day', ROUND(SUM(IF(t.Status='completed',0,1))/COUNT(*),2) 'Cancellation Rate' FROM Trips t INNER JOIN Users u ON t.Client_id=u.Users_Id AND u.Banned='No' WHERE t.Request_at BETWEEN '2013-10-01' AND '2013-10-03' GROUP BY t.Request_at 活动表 Activity： ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | ±-------------±--------+ 表的主键是 (player_id, event_date)。 这张表展示了一些游戏玩家在游戏平台上的行为活动。 每行数据记录了一名玩家在退出平台之前，当天使用同一台设备登录平台后打开的游戏的数目（可能是 0 个）。 写一条 SQL 查询语句获取每位玩家 第一次登陆平台的日期。 SELECT player_id,MIN(event_date) first_login FROM Activity GROUP BY player_id; select distinct player_id,min(event_date) over(partition by player_id) as first_login from Activity Table: Activity ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | ±-------------±--------+ (player_id, event_date) 是这个表的两个主键 这个表显示的是某些游戏玩家的游戏活动情况 每一行是在某天使用某个设备登出之前登录并玩多个游戏（可能为0）的玩家的记录 请编写一个 SQL 查询，描述每一个玩家首次登陆的设备名称 SELECT a.player_id,a.device_id FROM (SELECT player_id,MIN(event_date) min_date FROM Activity GROUP BY player_id) t INNER JOIN Activity a ON t.player_id=a.player_id AND t.min_date=a.event_date 这种联合查询的写法注意点 SELECT player_id,device_id FROM activity WHERE (player_id,event_date) IN (SELECT player_id,MIN(event_date) FROM activity GROUP BY player_id) Table: Activity ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | ±-------------±--------+ （player_id，event_date）是此表的主键。 这张表显示了某些游戏的玩家的活动情况。 每一行是一个玩家的记录，他在某一天使用某个设备注销之前登录并玩了很多游戏（可能是 0 ）。 编写一个 SQL 查询，同时报告每组玩家和日期，以及玩家到目前为止玩了多少游戏。也就是说，在此日期之前玩家所玩的游戏总数。详细情况请查看示例。 棒棒棒，自己写出来了 SELECT a1.player_id,a1.event_date,SUM(a2.games_played) games_played_so_far FROM Activity a1 INNER JOIN Activity a2 ON a1.player_id=a2.player_id AND a1.event_date&gt;=a2.event_date GROUP BY a1.player_id,a1.event_date SELECT player_id,event_date,SUM(games_played) OVER(PARTITION BY player_id ORDER BY event_date ASC) games_played_so_far FROM Activity； Table: Activity ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | ±-------------±--------+ （player_id，event_date）是此表的主键。 这张表显示了某些游戏的玩家的活动情况。 每一行是一个玩家的记录，他在某一天使用某个设备注销之前登录并玩了很多游戏（可能是 0）。 编写一个 SQL 查询，报告在首次登录的第二天再次登录的玩家的比率，四舍五入到小数点后两位。换句话说，您需要计算从首次登录日期开始至少连续两天登录的玩家的数量，然后除以玩家总数。 自己写的说实话有点烂 SELECT ROUND(COUNT(DISTINCT t1.player_id)/(SELECT COUNT(DISTINCT t3.player_id) FROM Activity t3),2) fraction FROM Activity t1 INNER JOIN Activity t2 ON t1.player_id=t2.player_id AND DATE_ADD(t1.event_date,INTERVAL 1 DAY)=t2.event_date WHERE (t1.player_id,t1.event_date) IN( SELECT player_id,MIN(event_date) FROM Activity GROUP BY player_id ) 这个就很强，用最下日期+1，省去判断连续日期，首次登录连续日期，可以用MIN求出首次日期，再+1就是次日日期，判断palyer_id，加过后的日期在不在即可计数。 SELECT ROUND(COUNT(DISTINCT player_id)/(SELECT COUNT(distinct player_id) FROM Activity), 2) AS fraction FROM Activity WHERE (player_id,event_date) IN (SELECT player_id, Date(min(event_date)+1) FROM Activity GROUP BY player_id); select round(count(distinct(b.player_id))/(select count(distinct(player_id)) from activity),2) as fraction from activity b join (select player_id,min(event_date), min(event_date)+1 as date_2nd from activity group by 1 ) a on b.event_date = a.date_2nd and b.player_id = a.player_id Employee 表包含所有员工。Employee 表有三列：员工Id，公司名和薪水。 ±----±-----------±-------+ |Id | Company | Salary | ±----±-----------±-------+ |1 | A | 2341 | |2 | A | 341 | |3 | A | 15 | |4 | A | 15314 | |5 | A | 451 | |6 | A | 513 | |7 | B | 15 | |8 | B | 13 | |9 | B | 1154 | |10 | B | 1345 | |11 | B | 1221 | |12 | B | 234 | |13 | C | 2345 | |14 | C | 2645 | |15 | C | 2645 | |16 | C | 2652 | |17 | C | 65 | ±----±-----------±-------+ 请编写SQL查询来查找每个公司的薪水中位数。挑战点：你是否可以在不使用任何内置的SQL函数的情况下解决此问题。 求中位数的思路，先对各组数据进行排序，同时求出各组数据总数，然后再判断排名在总数/2与总数/2+1之间。 Employee 表包含所有员工和他们的经理。每个员工都有一个 Id，并且还有一列是经理的 Id。 ±-----±---------±----------±---------+ |Id |Name |Department |ManagerId | ±-----±---------±----------±---------+ |101 |John |A |null | |102 |Dan |A |101 | |103 |James |A |101 | |104 |Amy |A |101 | |105 |Anne |A |101 | |106 |Ron |B |101 | ±-----±---------±----------±---------+ 给定 Employee 表，请编写一个SQL查询来查找至少有5名直接下属的经理。 SELECT Name FROM Employee WHERE Id IN (SELECT ManagerId FROM Employee GROUP BY ManagerId HAVING COUNT(*)&gt;=5) Numbers 表保存数字的值及其频率。 ±---------±------------+ | Number | Frequency | ±---------±------------| | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | ±---------±------------+ 在此表中，数字为 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3，所以中位数是 (0 + 0) / 2 = 0。 ±-------+ | median | ±-------| | 0.0000 | ±-------+ 请编写一个查询来查找所有数字的中位数并将结果命名为 median 。 新增两列，将从后往前和从前往后的频数相加，两个数都需要大于等于总数一半，再取平均 错误答案 SELECT AVG(T.num) AS median FROM (SELECT n1.Number num,Sum(n2.Frequency) as f_sum FROM Numbers n1 INNER JOIN Numbers n2 on n1.Number&gt;=n2.Number GROUP BY n1.Number ORDER BY n1.Number ASC) T WHERE T.f_sum BETWEEN (SELECT SUM(Frequency)/2 FROM Numbers) AND (SELECT SUM(Frequency)/2+1 FROM Numbers) 表: Candidate ±----±--------+ | id | Name | ±----±--------+ | 1 | A | | 2 | B | | 3 | C | | 4 | D | | 5 | E | ±----±--------+ 表: Vote ±----±-------------+ | id | CandidateId | ±----±-------------+ | 1 | 2 | | 2 | 4 | | 3 | 3 | | 4 | 2 | | 5 | 5 | ±----±-------------+ id 是自动递增的主键， CandidateId 是 Candidate 表中的 id. 请编写 sql 语句来找到当选者的名字，上面的例子将返回当选者 B. #group by key order by count(key) SELECT Name FROM Candidate WHERE id=( SELECT CandidateId FROM Vote GROUP BY CandidateId ORDER BY COUNT(CandidateId) DESC LIMIT 1); 选出所有 bonus &lt; 1000 的员工的 name 及其 bonus。 Employee 表单 ±------±-------±----------±-------+ | empId | name | supervisor| salary | ±------±-------±----------±-------+ | 1 | John | 3 | 1000 | | 2 | Dan | 3 | 2000 | | 3 | Brad | null | 4000 | | 4 | Thomas | 3 | 4000 | ±------±-------±----------±-------+ empId 是这张表单的主关键字 Bonus 表单 ±------±------+ | empId | bonus | ±------±------+ | 2 | 500 | | 4 | 2000 | ±------±------+ empId 是这张表单的主关键字 SELECT e.name name,b.bonus bonus FROM Employee e LEFT OUTER JOIN Bonus b ON e.empId=b.empId WHERE b.bonus&lt;1000 or b.bonus IS NULL; 从 survey_log 表中获得回答率最高的问题，survey_log 表包含这些列：id, action, question_id, answer_id, q_num, timestamp。 id 表示用户 id；action 有以下几种值：“show”，“answer”，“skip”；当 action 值为 “answer” 时 answer_id 非空，而 action 值为 “show” 或者 “skip” 时 answer_id 为空；q_num 表示当前会话中问题的编号。 请编写 SQL 查询来找到具有最高回答率的问题。 这种group by 后又要取最值的问题，记得 order by 然后 limit 1 SELECT t.question_id survey_log FROM (SELECT question_id,COUNT(answer_id) num FROM survey_log GROUP BY question_id ORDER BY num DESC) AS t LIMIT 1; 这中group by后order by count的写法 SELECT question_id survey_log FROM survey_log WHERE answer_id IS NOT NULL GROUP BY question_id ORDER BY COUNT(answer_id) DESC LIMIT 1; 请你编写 SQL 语句，对于每个员工，查询他除最近一个月（即最大月）之外，剩下每个月的近三个月的累计薪水（不足三个月也要计算）。 结果请按 Id 升序，然后按 Month 降序显示。 示例： 输入： Id Month Salary 1 1 20 2 1 20 1 2 30 2 2 30 3 2 40 1 3 40 3 3 60 1 4 60 3 4 70 rows 2 preceding:将当前行和它前面的两行划为一个窗口，因此sum函数就作 用在这三行上面 SELECT Id id,Month month,SUM(Salary) OVER(PARTITION BY Id ORDER BY Month ASC ROWS 2 PRECEDING) Salary FROM Employee WHERE (Id,Month) NOT IN (SELECT Id,MAX(Month) FROM Employee GROUP BY Id) ORDER BY Id ASC,Month DESC; SELECT a.Id AS id, a.Month AS month,SUM(b.Salary) AS Salary FROM Employee a, Employee b WHERE a.Id = b.Id AND a.Month &gt;= b.Month AND a.Month &lt; b.Month+3 #这个限制在3个月内 AND (a.Id, a.Month) NOT IN (SELECT Id, MAX(Month) FROM Employee GROUP BY Id) GROUP BY a.Id, a.Month ORDER BY a.Id, a.Month DESC 一所大学有 2 个数据表，分别是 student 和 department ，这两个表保存着每个专业的学生数据和院系数据。 写一个查询语句，查询 department 表中每个专业的学生人数 （即使没有学生的专业也需列出）。 将你的查询结果按照学生人数降序排列。 如果有两个或两个以上专业有相同的学生数目，将这些部门按照部门名字的字典序从小到大排列。 student 表格如下： Column Name Type student_id Integer student_name String gender Character dept_id Integer 其中， student_id 是学生的学号， student_name 是学生的姓名， gender 是学生的性别， dept_id 是学生所属专业的专业编号。 department 表格如下： Column Name Type dept_id Integer dept_name String dept_id 是专业编号， dept_name 是专业名字。 区分count(*）和count(s.student_id)区别 前一个是有多少条不考虑值 后一个只算s.student_id这一列的值不会计算null SELECT d.dept_name,COUNT(s.student_id) student_number FROM department d LEFT JOIN student s ON d.dept_id=s.dept_id GROUP BY d.dept_name ORDER BY student_number DESC,d.dept_name ASC; 给定表 customer ，里面保存了所有客户信息和他们的推荐人。 ±-----±-----±----------+ | id | name | referee_id| ±-----±-----±----------+ | 1 | Will | NULL | | 2 | Jane | NULL | | 3 | Alex | 2 | | 4 | Bill | NULL | | 5 | Zack | 1 | | 6 | Mark | 2 | ±-----±-----±----------+ 写一个查询语句，返回一个编号列表，列表中编号的推荐人的编号都 不是 2。 select name from customer where ifnull(referee_id,0)!=2; SELECT name FROM customer WHERE referee_id!=2 OR referee_id IS NULL 写一个查询语句，将 2016 年 (TIV_2016) 所有成功投资的金额加起来，保留 2 位小数。 对于一个投保人，他在 2016 年成功投资的条件是： 他在 2015 年的投保额 (TIV_2015) 至少跟一个其他投保人在 2015 年的投保额相同。 他所在的城市必须与其他投保人都不同（也就是说维度和经度不能跟其他任何一个投保人完全相同）。 输入格式: 表 insurance 格式如下： Column Name Type PID INTEGER(11) TIV_2015 NUMERIC(15,2) TIV_2016 NUMERIC(15,2) LAT NUMERIC(5,2) LON NUMERIC(5,2) PID 字段是投保人的投保编号， TIV_2015 是该投保人在2015年的总投保金额， TIV_2016 是该投保人在2016年的投保金额， LAT 是投保人所在城市的维度， LON 是投保人所在城市的经度。 在表 orders 中找到订单数最多客户对应的 customer_number 。 数据保证订单数最多的顾客恰好只有一位。 表 orders 定义如下： Column Type order_number (PK) int customer_number int order_date date required_date date shipped_date date status char(15) comment char(200) 题目没看懂 SELECT customer_number FROM orders GROUP BY customer_number ORDER BY COUNT(order_number) DESC LIMIT 1; select customer_number from orders group by customer_number having count(order_number)&gt;=all( select count(order_number) from orders group by customer_number ) 这里有张 World 表 ±----------------±-----------±-----------±-------------±--------------+ | name | continent | area | population | gdp | ±----------------±-----------±-----------±-------------±--------------+ | Afghanistan | Asia | 652230 | 25500100 | 20343000 | | Albania | Europe | 28748 | 2831741 | 12960000 | | Algeria | Africa | 2381741 | 37100000 | 188681000 | | Andorra | Europe | 468 | 78115 | 3712000 | | Angola | Africa | 1246700 | 20609294 | 100990000 | ±----------------±-----------±-----------±-------------±--------------+ 如果一个国家的面积超过 300 万平方公里，或者人口超过 2500 万，那么这个国家就是大国家。 编写一个 SQL 查询，输出表中所有大国家的名称、人口和面积。 SELECT w.name,w.population,w.area FROM World w WHERE w.area&gt;3000000 OR population&gt;25000000 有一个courses 表 ，有: student (学生) 和 class (课程)。 请列出所有超过或等于5名学生的课。 例如，表： ±--------±-----------+ | student | class | ±--------±-----------+ | A | Math | | B | English | | C | Math | | D | Biology | | E | Math | | F | Computer | | G | Math | | H | Math | | I | Math | ±--------±-----------+ SELECT class FROM courses GROUP BY class HAVING COUNT(DISTINCT student)&gt;=5 在 Facebook 或者 Twitter 这样的社交应用中，人们经常会发好友申请也会收到其他人的好友申请。 表：FriendRequest ±---------------±--------+ | Column Name | Type | ±---------------±--------+ | sender_id | int | | send_to_id | int | | request_date | date | ±---------------±--------+ 此表没有主键，它可能包含重复项。 该表包含发送请求的用户的 ID ，接受请求的用户的 ID 以及请求的日期。 表：RequestAccepted ±---------------±--------+ | Column Name | Type | ±---------------±--------+ | requester_id | int | | accepter_id | int | | accept_date | date | ±---------------±--------+ 此表没有主键，它可能包含重复项。 该表包含发送请求的用户的 ID ，接受请求的用户的 ID 以及请求通过的日期。 写一个查询语句，求出好友申请的通过率，用 2 位小数表示。通过率由接受好友申请的数目除以申请总数。 提示： 通过的好友申请不一定都在表 friend_request 中。你只需要统计总的被通过的申请数（不管它们在不在表 FriendRequest 中），并将它除以申请总数，得到通过率 一个好友申请发送者有可能会给接受者发几条好友申请，也有可能一个好友申请会被通过好几次。这种情况下，重复的好友申请只统计一次。 如果一个好友申请都没有，通过率为 0.00 。 SELECT ROUND(IFNULL((SELECT COUNT(DISTINCT requester_id,accepter_id) FROM RequestAccepted)/ (SELECT COUNT(DISTINCT sender_id ,send_to_id ) FROM FriendRequest ),0),2) accept_rate 表：Stadium ±--------------±--------+ | Column Name | Type | ±--------------±--------+ | id | int | | visit_date | date | | people | int | ±--------------±--------+ visit_date 是表的主键 每日人流量信息被记录在这三列信息中：序号 (id)、日期 (visit_date)、 人流量 (people) 每天只有一行记录，日期随着 id 的增加而增加 编写一个 SQL 查询以找出每行的人数大于或等于 100 且 id 连续的三行或更多行记录。 返回按 visit_date 升序排列的结果表。 在 Facebook 或者 Twitter 这样的社交应用中，人们经常会发好友申请也会收到其他人的好友申请。 表 request_accepted 存储了所有好友申请通过的数据记录，其中， requester_id 和 accepter_id 都是用户的编号。 requester_id accepter_id accept_date 1 2 2016_06-03 1 3 2016-06-08 2 3 2016-06-08 3 4 2016-06-09 写一个查询语句，求出谁拥有最多的好友和他拥有的好友数目。对于上面的样例数据，结果为： id num 3 3 注意： 保证拥有最多好友数目的只有 1 个人。 好友申请只会被接受一次，所以不会有 requester_id 和 accepter_id 值都相同的重复记录。 SELECT id,COUNT(id) num FROM ((SELECT requester_id id FROM request_accepted) UNION ALL (SELECT accepter_id id FROM request_accepted)) sub GROUP BY id ORDER BY num DESC LIMIT 1; select id, sum(num) num from ((select requester_id id, count(*) num from request_accepted group by requester_id) union all (select accepter_id id, count(*) num from request_accepted group by accepter_id)) t3 group by id order by num desc limit 1 几个朋友来到电影院的售票处，准备预约连续空余座位。 你能利用表 cinema ，帮他们写一个查询语句，获取所有空余座位，并将它们按照 seat_id 排序后返回吗？ seat_id free 1 1 2 0 3 1 4 1 5 1 **连续值问题怎么处理?**首先是两张表自联结，然后两张表的顺序作差值等于1，取绝对值的话就不论前后了 SELECT DISTINCT a.seat_id FROM cinema a INNER JOIN cinema b ON abs(b.seat_id-a.seat_id)=1 WHERE b.free=1 AND a.free=1 ORDER BY a.seat_id ASC; 还有这种lag函数的用法,lag是提取之前的内容，lead是提取之后的内容 select seat_id from ( select seat_id , free , lag(free,1,999) over() pre_free, lead(free,1,999) over() next_free from cinema )tmp where free=1 and (pre_free=1 or next_free=1 ) order by seat_id 给定 3 个表： salesperson， company， orders。 输出所有表 salesperson 中，没有向公司 ‘RED’ 销售任何东西的销售员。 示例： 输入 表： salesperson ±---------±-----±-------±----------------±----------+ | sales_id | name | salary | commission_rate | hire_date | ±---------±-----±-------±----------------±----------+ | 1 | John | 100000 | 6 | 4/1/2006 | | 2 | Amy | 120000 | 5 | 5/1/2010 | | 3 | Mark | 65000 | 12 | 12/25/2008| | 4 | Pam | 25000 | 25 | 1/1/2005 | | 5 | Alex | 50000 | 10 | 2/3/2007 | ±---------±-----±-------±----------------±----------+ 表 salesperson 存储了所有销售员的信息。每个销售员都有一个销售员编号 sales_id 和他的名字 name 。 表： company ±--------±-------±-----------+ | com_id | name | city | ±--------±-------±-----------+ | 1 | RED | Boston | | 2 | ORANGE | New York | | 3 | YELLOW | Boston | | 4 | GREEN | Austin | ±--------±-------±-----------+ 表 company 存储了所有公司的信息。每个公司都有一个公司编号 com_id 和它的名字 name 。 表： orders ±---------±-----------±--------±---------±-------+ | order_id | order_date | com_id | sales_id | amount | ±---------±-----------±--------±---------±-------+ | 1 | 1/1/2014 | 3 | 4 | 100000 | | 2 | 2/1/2014 | 4 | 5 | 5000 | | 3 | 3/1/2014 | 1 | 1 | 50000 | | 4 | 4/1/2014 | 1 | 4 | 25000 | ±---------±---------±--------±---------±-------+ 表 orders 存储了所有的销售数据，包括销售员编号 sales_id 和公司编号 com_id 。 SELECT name FROM salesperson WHERE sales_id NOT IN (SELECT sales_id FROM orders o INNER JOIN company c ON o.com_id=c.com_id WHERE c.name='RED') 给定一个表 tree，id 是树节点的编号， p_id 是它父节点的 id 。 ±—±-----+ | id | p_id | ±—±-----+ | 1 | null | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 2 | ±—±-----+ 树中每个节点属于以下三种类型之一： 叶子：如果这个节点没有任何孩子节点。 根：如果这个节点是整棵树的根，即没有父节点。 内部节点：如果这个节点既不是叶子节点也不是根节点。 写一个查询语句，输出所有节点的编号和节点的类型，并将结果按照节点编号排序。 一个小学生 Tim 的作业是判断三条线段是否能形成一个三角形。 然而，这个作业非常繁重，因为有几百组线段需要判断。 假设表 triangle 保存了所有三条线段的长度 x、y、z ，请你帮 Tim 写一个查询语句，来判断每组 x、y、z 是否可以组成一个三角形？ 记一下这种 case then用法,也可以不指定变量，只在WHEN中判断。 SELECT x,y,z, CASE WHEN x+y&gt;z AND x+z&gt;y and y+z&gt;x then 'Yes' ELSE 'No' END triangle FROM triangle select x, y, z, if(x+y&gt;z and x+z&gt;y and y+z&gt;x, 'Yes', 'No') as triangle from triangle 表 point_2d 保存了所有点（多于 2 个点）的坐标 (x,y) ，这些点在平面上两两不重合。 写一个查询语句找到两点之间的最近距离，保留 2 位小数。 x y -1 -1 0 0 -1 -2 最近距离在点 (-1,-1) 和(-1,2) 之间，距离为 1.00 。所以输出应该为： shortest 1.00 SELECT ROUND(MIN(SQRT(POWER(p1.x-p2.x,2)+POWER(P1.y-p2.y,2))),2) shortest FROM point_2d p1,point_2d p2 WHERE p1.x!=p2.x OR p1.y!=p2.y select round(min(sqrt(pow(t1.x-t2.x,2)+pow(t1.y-t2.y,2))),2) shortest from point_2d as t1,point_2d as t2 where (t1.x,t1.y) != (t2.x,t2.y) 表 point 保存了一些点在 x 轴上的坐标，这些坐标都是整数。 写一个查询语句，找到这些点中最近两个点之间的距离。 x -1 0 2 最近距离显然是 ‘1’ ，是点 ‘-1’ 和 ‘0’ 之间的距离。所以输出应该如下： shortest 1 注意：每个点都与其他点坐标不同，表 table 不会有重复坐标出现。 SELECT MIN(ABS(p1.x-p2.x)) shortest FROM point p1,point p2 WHERE p1.x!=p2.x 在 facebook 中，表 follow 会有 2 个字段： followee, follower ，分别表示被关注者和关注者。 请写一个 sql 查询语句，对每一个关注者，查询关注他的关注者的数目。 比方说： ±------------±-----------+ | followee | follower | ±------------±-----------+ | A | B | | B | C | | B | D | | D | E | ±------------±-----------+ 应该输出： ±------------±-----------+ | follower | num | ±------------±-----------+ | B | 2 | | D | 1 | ±------------±-----------+ 解释： B 和 D 都在在 follower 字段中出现，作为被关注者，B 被 C 和 D 关注，D 被 E 关注。A 不在 follower 字段内，所以A不在输出列表中。 注意： 被关注者永远不会被他 / 她自己关注。 将结果按照字典序返回。 SELECT followee follower,COUNT(DISTINCT follower) num FROM follow WHERE followee IN (SELECT follower FROM follow) GROUP BY followee 给如下两个表，写一个查询语句，求出在每一个工资发放日，每个部门的平均工资与公司的平均工资的比较结果 （高 / 低 / 相同）。 表： salary id employee_id amount pay_date 1 1 9000 2017-03-31 2 2 6000 2017-03-31 3 3 10000 2017-03-31 4 1 7000 2017-02-28 5 2 6000 2017-02-28 6 3 8000 2017-02-28 employee_id 字段是表 employee 中 employee_id 字段的外键。 employee_id department_id 1 1 2 2 3 2 SELECT t1.pay_date pay_month,t2.department_id, CASE WHEN t1.avg_amount&gt;t2.avg_amount THEN 'lower' WHEN t1.avg_amount=t2.avg_amount THEN 'same' WHEN t1.avg_amount&lt;t2.avg_amount THEN 'higher' END comparison FROM (SELECT DATE_FORMAT(pay_date,'%Y-%m') pay_date,AVG(amount) avg_amount FROM salary GROUP BY DATE_FORMAT(pay_date,'%Y-%m') ) t1 INNER JOIN (SELECT DATE_FORMAT(s.pay_date,'%Y-%m') pay_date,e.department_id,AVG(s.amount) avg_amount FROM salary s INNER JOIN employee e ON s.employee_id=e.employee_id GROUP BY DATE_FORMAT(s.pay_date,'%Y-%m'),e.department_id ) t2 ON t1.pay_date=t2.pay_date 下面这个写的更好,OVER(PARTITION BY department_id,LEFT(pay_date,7)) SELECT pay_month,department_id,CASE WHEN DEP_AVG_Salary=Mon_AVG_Salary THEN 'same' WHEN DEP_AVG_Salary&gt;Mon_AVG_Salary THEN 'higher' ELSE 'lower' END 'comparison' FROM (SELECT DISTINCT(LEFT(pay_date,7)) as 'pay_month', e.department_id, AVG(amount) OVER(PARTITION BY e.department_id,LEFT(pay_date,7)) as 'DEP_AVG_Salary', AVG(amount) OVER(PARTITION BY LEFT(pay_date,7)) as 'Mon_AVG_Salary' FROM salary s LEFT JOIN employee e ON s.employee_id=e.employee_id) t ORDER BY pay_month DESC; 一所美国大学有来自亚洲、欧洲和美洲的学生，他们的地理信息存放在如下 student 表中。 name continent Jack America Pascal Europe Xi Asia Jane America 写一个查询语句实现对大洲（continent）列的 透视表 操作，使得每个学生按照姓名的字母顺序依次排列在对应的大洲下面。输出的标题应依次为美洲（America）、亚洲（Asia）和欧洲（Europe）。数据保证来自美洲的学生不少于来自亚洲或者欧洲的学生。 对于样例输入，它的对应输出是： America Asia Europe Jack Xi Pascal Jane **行列转换怎么弄呢？**先计算ROW_NUMBER() OVER(PARTITION BY continent ORDER BY name) 分组排序， 然后GROUP BY，MAX,再CASE操作。 SELECT MAX(CASE WHEN continent='America' THEN name END) AS America, MAX(CASE WHEN continent='Asia' THEN name END) AS Asia, MAX(CASE WHEN continent='Europe' THEN name END) AS Europe FROM (SELECT name,continent,ROW_NUMBER() OVER(PARTITION BY continent ORDER BY name) rk FROM student) T GROUP BY rk; 表 my_numbers 的 num 字段包含很多数字，其中包括很多重复的数字。 你能写一个 SQL 查询语句，找到只出现过一次的数字中，最大的一个数字吗？ ±–+ |num| ±–+ | 8 | | 8 | | 3 | | 3 | | 1 | | 4 | | 5 | | 6 | SELECT MAX(num) num FROM (SELECT num,COUNT(num) n_num FROM my_numbers GROUP BY num) t WHERE n_num=1; SELECT CASE WHEN COUNT(*)=1 THEN num ELSE NULL END num FROM my_numbers GROUP BY num ORDER BY num DESC LIMIT 1 某城市开了一家新的电影院，吸引了很多人过来看电影。该电影院特别注意用户体验，专门有个 LED显示板做电影推荐，上面公布着影评和相关电影描述。 作为该电影院的信息部主管，您需要编写一个 SQL查询，找出所有影片描述为非 boring (不无聊) 的并且 id 为奇数 的影片，结果请按等级 rating 排列。 例如，下表 cinema: ±--------±----------±-------------±----------+ | id | movie | description | rating | ±--------±----------±-------------±----------+ | 1 | War | great 3D | 8.9 | | 2 | Science | fiction | 8.5 | | 3 | irish | boring | 6.2 | | 4 | Ice song | Fantacy | 8.6 | | 5 | House card| Interesting| 9.1 | ±--------±----------±-------------±----------+ SELECT * FROM cinema WHERE description!='boring' AND id%2=1 ORDER BY rating DESC 给定一个 salary 表，如下所示，有 m = 男性 和 f = 女性 的值。交换所有的 f 和 m 值（例如，将所有 f 值更改为 m，反之亦然）。要求只使用一个更新（Update）语句，并且没有中间的临时表。 注意，您必只能写一个 Update 语句，请不要编写任何 Select 语句。 UPDATE salary SET sex=IF(sex='m','f','m') 感觉case可以处理的问题更多一点 UPDATE salary SET sex=(CASE sex WHEN 'm' THEN 'f' ELSE 'm' END) Customer 表： ±------------±--------+ | Column Name | Type | ±------------±--------+ | customer_id | int | | product_key | int | ±------------±--------+ product_key 是 Customer 表的外键。 Product 表： ±------------±--------+ | Column Name | Type | ±------------±--------+ | product_key | int | ±------------±--------+ product_key 是这张表的主键。 写一条 SQL 查询语句，从 Customer 表中查询购买了 Product 表中所有产品的客户的 id。 SELECT customer_id FROM Customer GROUP BY customer_id HAVING COUNT(DISTINCT product_key)=( SELECT COUNT(DISTINCT product_key) FROM Product ) ActorDirector 表： ±------------±--------+ | Column Name | Type | ±------------±--------+ | actor_id | int | | director_id | int | | timestamp | int | ±------------±--------+ timestamp 是这张表的主键. 写一条SQL查询语句获取合作过至少三次的演员和导演的 id 对 (actor_id, director_id) SELECT actor_id,director_id FROM ActorDirector GROUP BY actor_id,director_id HAVING COUNT(*)&gt;=3 销售表 Sales： ±------------±------+ | Column Name | Type | ±------------±------+ | sale_id | int | | product_id | int | | year | int | | quantity | int | | price | int | ±------------±------+ (sale_id, year) 是销售表 Sales 的主键. product_id 是关联到产品表 Product 的外键. 注意: price 表示每单位价格 产品表 Product： ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | product_id | int | | product_name | varchar | ±-------------±--------+ product_id 是表的主键. 写一条SQL 查询语句获取 Sales 表中所有产品对应的 产品名称 product_name 以及该产品的所有 售卖年份 year 和 价格 price 。 SELECT p.product_name,s.year,s.price FROM Sales s INNER JOIN Product p ON s.product_id=p.product_id 销售表：Sales ±------------±------+ | Column Name | Type | ±------------±------+ | sale_id | int | | product_id | int | | year | int | | quantity | int | | price | int | ±------------±------+ sale_id 是这个表的主键。 product_id 是 Product 表的外键。 请注意价格是每单位的。 产品表：Product ±-------------±--------+ | Column Name | Type | ±-------------±--------+ | product_id | int | | product_name | varchar | ±-------------±--------+ product_id 是这个表的主键。 编写一个 SQL 查询，按产品 id product_id 来统计每个产品的销售总量。 SELECT product_id,SUM(quantity) total_quantity FROM Sales GROUP BY product_id","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"sql","slug":"数据分析/sql","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/sql/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"sql","slug":"sql","permalink":"https://mz2sj.github.io/tags/sql/"},{"name":"刷题","slug":"刷题","permalink":"https://mz2sj.github.io/tags/%E5%88%B7%E9%A2%98/"}]},{"title":"怨念","slug":"怨念","date":"2020-06-22T10:32:52.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/06/22/怨念/","link":"","permalink":"https://mz2sj.github.io/2020/06/22/%E6%80%A8%E5%BF%B5/","excerpt":"","text":"这是远离坏心情的帖子… 啊,2020年的六月下旬.六月的雨你什么时候结束呀?明天和导师聊什么呢?说你自己什么也没干吗?呵呵.好好干吧. 我爱生活,生活爱我.","categories":[{"name":"再见坏心情","slug":"再见坏心情","permalink":"https://mz2sj.github.io/categories/%E5%86%8D%E8%A7%81%E5%9D%8F%E5%BF%83%E6%83%85/"}],"tags":[]},{"title":"01-java扫盲","slug":"01-java扫盲","date":"2020-06-17T15:06:01.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/06/17/01-java扫盲/","link":"","permalink":"https://mz2sj.github.io/2020/06/17/01-java%E6%89%AB%E7%9B%B2/","excerpt":"","text":"现在真的好迷茫呀，研究生的事让自己又烦又怕，学学java压压惊，老天保佑我吧。 关键字 被java语言赋予特定含义的字，组成关键字单词的字母全部小写，goto和const是保留字。 标识符 标识符就是给类、接口、方法、变量等起名字。不能以数字开头，不能是java中的关键字，标识符严格区分大小写。标识符的英文拼写中，常量每个字母都大写，用_隔开，例如STUDENT_MAX_AGE。 main方法 main方法是程序的主方法，是程序的入口，被jvm调用。 反码补码 计算机在操作时，采用数据对应的二进制的补码来计算。正数的原码最高位是0，负数的原码最高位是1，其他都是数值为。 正数的反码与原码相同，负数的反码与原码符号位相同，数值位取反。 正数的补码与原码相同，负数的补码是在反码的基础上加上1. 基本数据类型 整数 关键字 占用字节数 byte 1 short 2 int 4 long 8 浮点数 关键字 占用字节数 float 4 double 8 字符 关键字 占用字节数 char 2 布尔 关键字 占用字节数 boolean 1 布尔类型不参与转换 整数默认是int类型，浮点数默认是double类型。长整型后缀用L或者l标记，建议使用L。 单精度浮点数用F或者f标记，建议使用F。 默认转换 byte,short,char-int-long-float-double byte,short,char相互之间不能转换，它们参与运算首先转换成int类型。 关于精度的一些问题 如要声明一个常量为 float 型，则需在数字后面加 f 或 F 变量、常量相加 byte b1=3,b2=4,b; b=b1+b2; 错误，b1和b2是变量，变量的值会变化，不确定具体的值，所以默认使用int。byte、short、char相加会转换为int类型做运算，转为byte会降低精度。 b=3+4; 正确，常量相加，先做加法，然后看结果是否在赋值的数据类型范围内，在的话即正确 强制转换 从大的类型转换为小的类型 目标数据类型 变量=(目标数据类型)(被转换的数据)","categories":[{"name":"java","slug":"java","permalink":"https://mz2sj.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://mz2sj.github.io/tags/java/"}]},{"title":"05-CV入门之模型集成","slug":"05-CV入门之模型集成","date":"2020-06-02T02:13:51.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/06/02/05-CV入门之模型集成/","link":"","permalink":"https://mz2sj.github.io/2020/06/02/05-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/","excerpt":"","text":"集成学习方法 在机器学习中的集成学习可以在一定程度上提高预测精度，常见的集成学习方法有Stacking、Bagging和Boosting，同时这些集成学习方法与具体验证集划分联系紧密。 由于深度学习模型一般需要较长的训练周期，如果硬件设备不允许建议选取留出法，如果需要追求精度可以使用交叉验证的方法。 下面假设构建了10折交叉验证，训练得到10个CNN模型。 那么在10个CNN模型可以使用如下方式进行集成： 对预测的结果的概率值进行平均，然后解码为具体字符； 对预测的字符进行投票，得到最终字符。 除了上面这种方法外，我们也可以选择不同的模型，将不同模型的结果进行集成。 深度学习中的集成 Dropout Dropout可以作为训练深度神经网络的一种技巧。在每个训练批次中，通过随机让一部分的节点停止工作。同时在预测的过程中让所有的节点都其作用。 Dropout经常出现在在先有的CNN网络中，可以有效的缓解模型过拟合的情况，也可以在预测时增加模型的精度。 加入Dropout后的网络结构如下： # 定义模型 class SVHN_Model1(nn.Module): def __init__(self): super(SVHN_Model1, self).__init__() # CNN提取特征模块 self.cnn = nn.Sequential( nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)), nn.ReLU(), nn.Dropout(0.25), nn.MaxPool2d(2), nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)), nn.ReLU(), nn.Dropout(0.25), nn.MaxPool2d(2), ) # self.fc1 = nn.Linear(32*3*7, 11) self.fc2 = nn.Linear(32*3*7, 11) self.fc3 = nn.Linear(32*3*7, 11) self.fc4 = nn.Linear(32*3*7, 11) self.fc5 = nn.Linear(32*3*7, 11) self.fc6 = nn.Linear(32*3*7, 11) def forward(self, img): feat = self.cnn(img) feat = feat.view(feat.shape[0], -1) c1 = self.fc1(feat) c2 = self.fc2(feat) c3 = self.fc3(feat) c4 = self.fc4(feat) c5 = self.fc5(feat) c6 = self.fc6(feat) return c1, c2, c3, c4, c5, c6 TTA 测试集数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧，数据扩增不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增，对同一个样本预测三次，然后对三次结果进行平均。 def predict(test_loader, model, tta=10): model.eval() test_pred_tta = None # TTA 次数 for _ in range(tta): test_pred = [] with torch.no_grad(): for i, (input, target) in enumerate(test_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) output = np.concatenate([c0.data.numpy(), c1.data.numpy(), c2.data.numpy(), c3.data.numpy(), c4.data.numpy(), c5.data.numpy()], axis=1) test_pred.append(output) test_pred = np.vstack(test_pred) if test_pred_tta is None: test_pred_tta = test_pred else: test_pred_tta += test_pred return test_pred_tta Snapshot 本章的开头已经提到，假设我们训练了10个CNN则可以将多个模型的预测结果进行平均。但是假如只训练了一个CNN模型，如何做模型集成呢？ 在论文Snapshot Ensembles中，作者提出使用cyclical learning rate进行训练模型，并保存精度比较好的一些checkopint，最后将多个checkpoint进行模型集成。 由于在cyclical learning rate中学习率的变化有周期性变大和减少的行为，因此CNN模型很有可能在跳出局部最优进入另一个局部最优。在Snapshot论文中作者通过使用表明，此种方法可以在一定程度上提高模型精度，但需要更长的训练时间。 结果后处理 在不同的任务中可能会有不同的解决方案，不同思路的模型不仅可以互相借鉴，同时也可以修正最终的预测结果。 在本次赛题中，可以从以下几个思路对预测结果进行后处理： 统计图片中每个位置字符出现的频率，使用规则修正结果； 单独训练一个字符长度预测模型，用来预测图片中字符个数，并修正结果。","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"04-CV入门之模型训练与验证","slug":"04-CV入门之模型训练与验证","date":"2020-05-30T12:07:08.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/05/30/04-CV入门之模型训练与验证/","link":"","permalink":"https://mz2sj.github.io/2020/05/30/04-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/","excerpt":"","text":"数据集划分 在模型的训练过程中，模型只能利用训练数据来进行训练，模型并不能接触到测试集上的样本。因此模型如果将训练集学的过好，模型就会记住训练样本的细节，导致模型在测试集的泛化效果较差，这种现象称为过拟合（Overfitting）。与过拟合相对应的是欠拟合（Underfitting），即模型在训练集上的拟合效果较差。 如图所示：随着模型复杂度和模型训练轮数的增加，CNN模型在训练集上的误差会降低，但在测试集上的误差会逐渐降低，然后逐渐升高，而我们为了追求的是模型在测试集上的精度越高越好 。 解决上述问题最好的解决方法：构建一个与测试集尽可能分布一致的样本集（可称为验证集），在训练过程中不断验证模型在验证集上的精度，并以此控制模型的训练。 因为训练集和验证集是分开的，所以模型在验证集上面的精度在一定程度上可以反映模型的泛化能力。在划分验证集的时候，需要注意验证集的分布应该与测试集尽量保持一致，不然模型在验证集上的精度就失去了指导意义。 留出法 直接将训练集划分成两部分，新的训练集和验证集。这种划分方式的优点是最为直接简单；缺点是只得到了一份验证集，有可能导致模型在验证集上过拟合。留出法应用场景是数据量比较大的情况。 交叉验证法 将训练集划分成K份，将其中的K-1份作为训练集，剩余的1份作为验证集，循环K训练。这种划分方式是所有的训练集都是验证集，最终模型验证精度是K份平均得到。这种方式的优点是验证集精度比较可靠，训练K次可以得到K个有多样性差异的模型；CV验证的缺点是需要训练K次，不适合数据量很大的情况。 自助采样法 通过有放回的采样方式得到新的训练集和验证集，每次的训练集和验证集都是有区别的。这种划分方式一般适用于数据量较小的情况。 模型训练与验证 放下代码，其实之前代码已经写的差不多了 train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=10, shuffle=True, num_workers=10, ) val_loader = torch.utils.data.DataLoader( val_dataset, batch_size=10, shuffle=False, num_workers=10, ) model = SVHN_Model1() criterion = nn.CrossEntropyLoss (size_average=False) optimizer = torch.optim.Adam(model.parameters(), 0.001) best_loss = 1000.0 for epoch in range(20): print('Epoch: ', epoch) train(train_loader, model, criterion, optimizer, epoch) val_loss = validate(val_loader, model, criterion) # 记录下验证集精度 if val_loss &lt; best_loss: best_loss = val_loss torch.save(model.state_dict(), './model.pt') def train(train_loader, model, criterion, optimizer, epoch): # 切换模型为训练模式 model.train() for i, (input, target) in enumerate(train_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) loss = criterion(c0, data[1][:, 0]) + \\ criterion(c1, data[1][:, 1]) + \\ criterion(c2, data[1][:, 2]) + \\ criterion(c3, data[1][:, 3]) + \\ criterion(c4, data[1][:, 4]) + \\ criterion(c5, data[1][:, 5]) loss /= 6 optimizer.zero_grad() loss.backward() optimizer.step() def validate(val_loader, model, criterion): # 切换模型为预测模型 model.eval() val_loss = [] # 不记录模型梯度信息 with torch.no_grad(): for i, (input, target) in enumerate(val_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) loss = criterion(c0, data[1][:, 0]) + \\ criterion(c1, data[1][:, 1]) + \\ criterion(c2, data[1][:, 2]) + \\ criterion(c3, data[1][:, 3]) + \\ criterion(c4, data[1][:, 4]) + \\ criterion(c5, data[1][:, 5]) loss /= 6 val_loss.append(loss.item()) return np.mean(val_loss) torch.save(model_object.state_dict(), 'model.pt') model.load_state_dict(torch.load(' model.pt')) 模型调参 在参加本次比赛的过程中，我建议大家以如下逻辑完成： 1.初步构建简单的CNN模型，不用特别复杂，跑通训练、验证和预测的流程； 2.简单CNN模型的损失会比较大，尝试增加模型复杂度，并观察验证集精度； 3.在增加模型复杂度的同时增加数据扩增方法，直至验证集精度不变。 ok，有时间再修改。","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"03-CV入门之模型构建","slug":"03-CV入门之模型构建","date":"2020-05-26T13:15:59.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/26/03-CV入门之模型构建/","link":"","permalink":"https://mz2sj.github.io/2020/05/26/03-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/","excerpt":"","text":"CNN介绍 ​ 卷积神经网络（简称CNN）是一类特殊的人工神经网络，是深度学习中重要的一个分支。CNN在很多领域都表现优异，精度和速度比传统计算学习算法高很多。特别是在计算机视觉领域，CNN是解决图像分类、图像检索、物体检测和语义分割的主流模型。 ​ CNN是一种层次模型，输入的是原始的像素数据。CNN通过卷积（convolution）、池化（pooling）、非线性激活函数（non-linear activation function）和全连接层（fully connected layer）构成。 利用pytorch我们可以很方便实现。 #顺序依次是卷积、激活函数、池化 cnn=nn.Sequential( nn.Conv2d(3,16,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16,32,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), ) ​ 与传统机器学习模型相比，CNN具有一种端到端（End to End）的思路。在CNN训练的过程中是直接从图像像素到最终的输出，并不涉及到具体的特征提取和构建模型的过程，也不需要人工的参与。 模型构建 下面写一下代码的完整流程 #数据 import os,sys,glob,shutil,json import cv2 from PIL import Image import numpy as np import torch from torch.utils.data.dataset import Dataset import torchvision.transforms as transforms class SVHNDataset(Dataset): def __init__(self,img_path,img_label,transform=None): self.img_path=img_path self.img_label=img_label if transform is not None: self.transform=transform else: self.transform=None def __getitem__(self,index): img=Image.open(self.img_path[index]).convert('RGB') if self.transform is not None: img=self.transform(img) lbl=np.array(self.img_label[index],dtype=np.int) lbl=list(lbl)+(5-len(lbl))*[10] return img,torch.from_numpy(np.array(lbl[:6])) def __len__(self): return len(self.img_path) train_path=glob.glob('./data/mchar_train/*.png') train_path.sort() train_json=json.load(open('./data/mchar_train.json')) train_label=[train_json[x]['label'] for x in train_json] data=SVHNDataset(train_path,train_label,transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.2,0.2,0.2), transforms.RandomRotation(5), ])) train_loader=torch.utils.data.DataLoader( SVHNDataset(train_path,train_label, transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.3,0.3,0.2), transforms.RandomRotation(5), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ])), batch_size=10, shuffle=False, num_workers=10 ) #模型 class SVHN_Model1(nn.Module): def __init__(self): super(SVHN_Model1,self).__init__() self.cnn=nn.Sequential( nn.Conv2d(3,16,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16,32,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), ) self.fc1=nn.Linear(32*3*7,11) self.fc2=nn.Linear(32*3*7,11) self.fc3=nn.Linear(32*3*7,11) self.fc4=nn.Linear(32*3*7,11) self.fc5=nn.Linear(32*3*7,11) def forward(self,img): feat=self.cnn(img) feat=feat.view(feat.shape[0],-1) c1=self.fc1(feat) c2=self.fc2(feat) c3=self.fc3(feat) c4=self.fc4(feat) c5=self.fc5(feat) return c1,c2,c3,c4,c5 #使用预训练模型 class SVHN_Model2(nn.Module): def __init__(self): super(SVHN_Model2,self).__init__() model_conv=models.resnet18(pretrained=True) model_conv.avgpool=nn.AdaptivedAvgPool2d(1) model_conv=nn.Sequential(*list(model_conv.children())[:-1]) self.cnn=model_conv self.fc1=nn.Linear(512,11) self.fc2=nn.Linear(512,11) self.fc3=nn.Linear(512,11) self.fc3=nn.Linear(512,11) self.fc4=nn.Linear(512,11) self.fc5=nn.Linear(512,11) def forward(self,img): feat=self.cnn(img) feat=feat.view(feat.shape[0],-1) c1=self.fc1(feat) c2=self.fc2(feat) c3=self.fc3(feat) c4=self.fc4(feat) c5=self.fc5(feat) return c1,c2,c3,c4,c5 #pipeline model=SVHN_Model1() model.cuda() %%time criterion=nn.CrossEntropyLoss() optimizer=torch.optim.Adam(model.parameters(),0.005) loss_plot,c0_plot=[],[] for epoch in range(10): for data in train_loader: data[0]=data[0].cuda() data[1]=data[1].cuda() c0,c1,c2,c3,c4=model(data[0]) loss=criterion(c0,data[1][:,0].long())+\\ criterion(c1,data[1][:,1].long())+\\ criterion(c2,data[1][:,2].long())+\\ criterion(c3,data[1][:,3].long())+\\ criterion(c4,data[1][:,4].long()) loss/=5 optimizer.zero_grad() loss.backward() optimizer.step() loss_plot.append(loss.item()) c0_plot.append((c0.argmax(1)==data[1][:,0]).sum().item()*1.0/c0.shape[0]) print(epoch) 至此就完成了，这里的pipeline并不完整，只包含了训练的代码，还未包含模型的预测，模型的训练与验证下次再说。","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"},{"name":"CNN","slug":"CNN","permalink":"https://mz2sj.github.io/tags/CNN/"}]},{"title":"02-CV入门之数据读取与扩增","slug":"02-CV入门之数据读取与扩增","date":"2020-05-23T13:03:16.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/23/02-CV入门之数据读取与扩增/","link":"","permalink":"https://mz2sj.github.io/2020/05/23/02-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%89%A9%E5%A2%9E/","excerpt":"","text":"Pillow Pilow是python中进行图像处理的一个库，话不多说看代码。 from PIL import Image im=Image.open('./data/cat.png') im 自己截了colab里面的猫猫，很可爱呀 from PIL import Image,ImageFilter im=Image.open('./data/cat.png') im2=im.filter(ImageFilter.BLUR) im2.save('blur.jpg','jpeg') im2 可爱的猫猫变糊了 from PIL import Image im=Image.open('./data/cat.png') im.thumbnail((im.width//2,im.height//2)) im 可爱的猫猫变小了 对图像处理并不熟悉，我们要做的是清楚需要对图像处理做哪些工作，然后再去查找相关API Opencv 2 import cv2 import matplotlib.pyplot as plt img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB) plt.imshow(img) opencv读出来的是数组，需要由其他包可视化 img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) plt.imshow(img) 代码说的是转成灰度图像，不知道我怎么就变成了这个亚子 img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) edges=cv2.Canny(img,30,70) plt.imshow(edges) 给猫猫描描边 数据扩增 图像的数据扩增方法包括各种旋转以及裁剪以及对像素的操作。 以torchvision为例，常见的数据扩增方法包括： transforms.CenterCrop 对图片中心进行裁剪 transforms.ColorJitter 对图像颜色的对比度、饱和度和零度进行变换 transforms.FiveCrop 对图像四个角和中心进行裁剪得到五分图像 transforms.Grayscale 对图像进行灰度变换 transforms.Pad 使用固定值进行像素填充 transforms.RandomAffine 随机仿射变换 transforms.RandomCrop 随机区域裁剪 transforms.RandomHorizontalFlip 随机水平翻转 transforms.RandomRotation 随机旋转 transforms.RandomVerticalFlip 随机垂直翻转 新手用用torchvision足够啦 Dataset import os,sys,glob,shutil,json import cv2 from PIL import Image import numpy as np import torch from torch.utils.data.dataset import Dataset import torchvision.transforms as transforms class SVHNDataset(Dataset): def __init__(self,img_path,img_label,transform=None): self.img_path=img_path self.img_label=img_label if transform is not None: self.transform=transform else: self.transform=None def __getitem__(self,index): img=Image.open(self.img_path[index]).convert('RGB') if self.transform is not None: img=self.transform(img) lbl=np.array(self.img_label[index],dtype=np.int) lbl=list(lbl)+(5-len(lbl))*[10] return img,torch.from_numpy(np.array(lbl[:5])) def __len__(self): return len(self.img_path) pytorch自定义Dataset需要继承自pytorch定义的Dataset类，我们要实现其中的__init__()、len()、getitem()方法即可，对数据的操作主要放在__getitem__()方法中,比如之前提到的数据扩增我们也是放在__getitem__()里面进行操作。 train_path=glob.glob('./data/mchar_train/*.png') train_path.sort() train_json=json.load(open('./data/mchar_train.json')) train_label=[train_json[x]['label'] for x in train_json] data=SVHNDataset(train_path,train_label,transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.2,0.2,0.2), transforms.RandomRotation(5), ])) 需要注意的是Dataset每次只能返回一条数据，想要构成batch型数据，还需要进一步构建dataloader DataLoader dataloader的代码不需要我们重构，直接调用即可 ，但是如果我们想采用特殊的采样方法等定制化功能时，还需要我们对代码进行重构。 train_loader=torch.utils.data.DataLoader( SVHNDataset(train_path,train_label, transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.3,0.3,0.2), transforms.RandomRotation(5), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ])), batch_size=10, shuffle=False, num_workers=10 ) 我们可以通过 next(iter(train_loader)) 来查看单个batch的数据情况 收工！！！","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"},{"name":"pytorch","slug":"pytorch","permalink":"https://mz2sj.github.io/tags/pytorch/"}]},{"title":"01-CV入门之赛题理解","slug":"01-CV入门之赛题理解","date":"2020-05-20T13:53:20.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/20/01-CV入门之赛题理解/","link":"","permalink":"https://mz2sj.github.io/2020/05/20/01-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/","excerpt":"","text":"目标 识别图像中的字符串 为了简化难度，主办方提供了字符在图片中的字符框位置，我们只需识别字符框中的字符即可。 下面看读取数据的一个例子 import numpy as np #读取图片的起始结尾位置及标签 def parse_json(d): arr=np.array([ d['top'],d['height'],d['left'],d['width'],d['label'] ]) arr=arr.astype(int) return arr import cv2 img=cv2.imread('./data/mchar_train/000008.png') arr=parse_json(train_json['000008.png']) import matplotlib.pyplot as plt #显示图片 plt.figure(figsize=(10,10)) plt.subplot(1,arr.shape[1]+1,1) plt.imshow(img) 可以看到128三个数字，由于已经给我们分好了每个字符的范围，我们可以单独把每个字符画出来 for idx in range(arr.shape[1]): plt.subplot(1,arr.shape[1],idx+1) plt.imshow(img[arr[0,idx]:arr[0,idx]+arr[1,idx],arr[2,idx]:arr[2,idx]+arr[3,idx]]) plt.title(arr[4,idx]) plt.xticks([]);plt.yticks([]) 糊的不行 解题思路 定长字符识别 可以将赛题抽象为一个定长字符识别问题，在赛题数据集中大部分图像中字符个数为2-4个，最多的字符 个数为6个。 因此可以对于所有的图像都抽象为6个字符的识别问题，字符23填充为23XXXX，字符231填充为231XXX。 经过填充之后，原始的赛题可以简化了6个字符的分类问题。在每个字符的分类中会进行11个类别的分类，假如分类为填充字符，则表明该字符为空。 不定长字符识别 在字符识别研究中，有特定的方法来解决此种不定长的字符识别问题，比较典型的有CRNN字符识别模型。 在本次赛题中给定的图像数据都比较规整，可以视为一个单词或者一个句子 检测再识别 在赛题数据中已经给出了训练集、验证集中所有图片中字符的位置，因此可以首先将字符的位置进行识别，利用物体检测的思路完成。 需要用到目标检测模型，那是后话啦，先把眼前的弄好。","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"},{"name":"比赛","slug":"CV/比赛","permalink":"https://mz2sj.github.io/categories/CV/%E6%AF%94%E8%B5%9B/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"04-爬虫","slug":"04-爬虫","date":"2020-04-27T08:57:35.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/27/04-爬虫/","link":"","permalink":"https://mz2sj.github.io/2020/04/27/04-%E7%88%AC%E8%99%AB/","excerpt":"","text":"chrome监控ajax请求 浏览器中输入腾讯新闻的网址，发现不断向下拉滚动条就可以新闻会不断出现，初步判断新出现的新闻是通过ajax加载的。打开chrome的开发者工具，进入network中的js界面，将js脚本按名称排序，继续向下拉动滚动条，可以发现新出现的请求有规律。 rcd?这个请求随着滚动条的拖动不断增加，我们把前几条请求单独拿出来分析。 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=0&amp;expIds=&amp;callback=__jp1 https://pacaio.match.qq.com/irs/rcd?cid=4&amp;token=9513f1a78a663e1d25b46a826f248c3c&amp;ext=&amp;page=0&amp;expIds=&amp;callback=__jp2 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=1&amp;expIds=20200427A0I0UN|20200427002954|20200402012032|20200427A0CB27|20200427A0H4VM|20200427A0GJP8|20200427002536|20200426005210|20200427A0CW43|20200427A0H11D&amp;callback=__jp4 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=2&amp;expIds=20200427A0BLNR|20200427A08JMI|20200426005565|20200427A0BBTS|20200427A088DC|20200427A05064|20200427A04L6I|20200427A066PB|20200427V05K49&amp;callback=__jp5 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=3&amp;expIds=20200427V01DY9|20200427000076|20200426A0O227|20200427A00E00|20200427A02TN7|20200426A0PR2B|20200426A0PLML|20200426A0HWYG|20200426A0Q1GM|20200426A0PGG8&amp;callback=__jp6 出去开头的请求规律不规范外，可以发现请求的一些规律，第三条之后的请求：cid=137,page=1 2 3...,callback=__jp4 5 6,从第二条开始看的话，page=0 1 2 3 又综合分析一下，经过自己的尝试我们可以构造出下面的请求 url='https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page={}'.format(i) 只需要更改i就可以获取不同页面的内容，并且是直接返回json，省去解析的麻烦。 不用selenium就可以爬取新闻 下面放上自己的代码 requests爬取腾讯新闻 import time import requests url_fmt='https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page={}' title_list=[] tags_list=[] keywords_list=[] vurl_list=[] intro_list=[] category_list=[] source_list=[] bimg_list=[] for page in range(100): url=url_fmt.format(page) res=requests.get(url) data=res.json()['data'] for new in data: try: title=new['title'] tags=new['tags'] keywords=new['keywords'] vurl=new['vurl'] intro=new['intro'] category=new['category'] source=new['source'] bimg=new['bimg'] except: continue title_list.append(title) tags_list.append(tags) keywords_list.append(keywords) vurl_list.append(vurl) intro_list.append(intro) category_list.append(category) source_list.append(source) bimg_list.append(bimg) time.sleep(2) import pandas as pd news=pd.DataFrame({'title':title_list, 'tags':tags_list, 'keywords':keywords_list, 'vurl':vurl_list, 'intro':intro_list, 'category':category_list, 'source':source_list, 'bimg':bimg_list}) news.to_csv('news.csv') 大功告成！！！ selenium模拟拖动滚动条 可以让selenium模拟拖动滚动条使新闻更新，在这里发现展示的新闻数量是有限的，拖动到一定程度将不能再拖动。 import time from selenium import webdriver from lxml import etree #模拟打开界面 driver=webdriver.Chrome(r'C:\\Users\\MZ\\Downloads\\chromedriver_win32\\chromedriver.exe') driver.get('https://news.qq.com/') driver.maximize_window() time.sleep(3) current_window_1=driver.current_window_handle print(current_window_1) #无法拖动时退出拖动，使用chrome自带copy xpath 构造解析式 number_before=0 for i in range(50): driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight-100)\") html=driver.page_source tree=etree.HTML(html) number_after=len(tree.xpath('/html/body/div/div[4]/div[2]/div/div/ul[2]/li'))-1 if number_before==number_after: break time.sleep(1) number_before=number_after html=driver.page_source print(number_after) news_number=len(tree.xpath('/html/body/div/div[4]/div[2]/div/div/ul[2]/li'))-1 title_fmt='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/img/@alt' img_fmt='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/img/@src' news_src='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/@href' title_list=[] img_list=[] src_list=[] for number in range(news_number): try: title=tree.xpath(title_fmt.format(number))[0] img=tree.xpath(img_fmt.format(number))[0] src=tree.xpath(news_src.format(number))[0] except: continue title_list.append(title) img_list.append(img) src_list.append(src) news=pd.DataFrame({'title':title_list,'url':src_list,'img':img_list}) news.to_csv('news.csv') news.head(10)","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"腾讯新闻爬虫","slug":"腾讯新闻爬虫","permalink":"https://mz2sj.github.io/tags/%E8%85%BE%E8%AE%AF%E6%96%B0%E9%97%BB%E7%88%AC%E8%99%AB/"},{"name":"ajax","slug":"ajax","permalink":"https://mz2sj.github.io/tags/ajax/"}]},{"title":"03-爬虫","slug":"03-爬虫","date":"2020-04-25T14:00:44.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/25/03-爬虫/","link":"","permalink":"https://mz2sj.github.io/2020/04/25/03-%E7%88%AC%E8%99%AB/","excerpt":"","text":"proxy ip 代理 代理主要用来解决ip频繁访问被服务器封号的问题，在requests中使用代理很简单.proxies格式是一个字典：{‘http’: ‘[http://IP:port‘,‘https’:'https://IP:port‘}](http://xn--ip:port%2Chttps:'https-mc9nd6a//IP:port‘}),我们将它传入requests中的get方法就可以使用 requests.get(url, headers = headers, proxies = proxies, timeout = 10) 我们可以从网上提供的一些代理池中获取有效的代理，具体代码如下： from bs4 import BeautifulSoup import requests import re import json def open_proxy_url(url): user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' headers = {'User-Agent': user_agent} try: r = requests.get(url, headers = headers, timeout = 10) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: print('无法访问网页' + url) def get_proxy_ip(response): proxy_ip_list = [] soup = BeautifulSoup(response, 'html.parser') proxy_ips = soup.find(id = 'ip_list').find_all('tr') for proxy_ip in proxy_ips: if len(proxy_ip.select('td')) &gt;=8: ip = proxy_ip.select('td')[1].text port = proxy_ip.select('td')[2].text protocol = proxy_ip.select('td')[5].text if protocol in ('HTTP','HTTPS','http','https'): proxy_ip_list.append(f'{protocol}://{ip}:{port}') return proxy_ip_list def open_url_using_proxy(url, proxy): user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' headers = {'User-Agent': user_agent} proxies = {} if proxy.startswith(('HTTPS','https')): proxies['https'] = proxy else: proxies['http'] = proxy try: r = requests.get(url, headers = headers, proxies = proxies, timeout = 10) r.raise_for_status() r.encoding = r.apparent_encoding return (r.text, r.status_code) except: print('无法访问网页' + url) print('无效代理IP: ' + proxy) return False def check_proxy_avaliability(proxy): url = 'http://www.baidu.com' result = open_url_using_proxy(url, proxy) VALID_PROXY = False if result: text, status_code = result if status_code == 200: r_title = re.findall('&lt;title&gt;.*&lt;/title&gt;', text) if r_title: if r_title[0] == '&lt;title&gt;百度一下，你就知道&lt;/title&gt;': VALID_PROXY = True if VALID_PROXY: check_ip_url = 'https://jsonip.com/' try: text, status_code = open_url_using_proxy(check_ip_url, proxy) except: return print('有效代理IP: ' + proxy) with open('valid_proxy_ip.txt','a') as f: f.writelines(proxy) try: source_ip = json.loads(text).get('ip') print(f'源IP地址为：{source_ip}') print('='*40) except: print('返回的非json,无法解析') print(text) else: print('无效代理IP: ' + proxy) if __name__ == '__main__': proxy_url = 'https://www.xicidaili.com/' proxy_ip_filename = 'proxy_ip.txt' text = open(proxy_ip_filename, 'r').read() proxy_ip_list = get_proxy_ip(text) for proxy in proxy_ip_list: check_proxy_avaliability(proxy) selenium selenium是什么：一个自动化测试工具（大家都是这么说的） selenium应用场景：用代码的方式去模拟浏览器操作过程（如：打开浏览器、在输入框里输入文字、回车等），在爬虫方面很有必要 selenium模拟浏览器进行操作，需要电脑安装相应的浏览器和浏览器驱动 1、导入模块： from selenium import webdriver # 启动浏览器需要用到 from selenium.webdriver.common.keys import Keys # 提供键盘按键支持（最后一个K要大写） 2、创建一个WebDriver实例： driver = webdriver.Chrome(\"chromedriver驱动程序路径\") 3、打开一个页面: driver.get(\"http://www.python.org\") # 这个时候chromedriver会打开一个Chrome浏览器窗口，显示的是网址所对应的页面 4、关闭页面 driver.close() # 关闭浏览器一个Tab # or driver.quit() # 关闭浏览器窗口 高级 查找元素 在打开页面和关闭页面中间，就是各种操作！而查找元素这一点，和爬虫常见的HTML页面解析，定位到具体的某个元素基本一样，只不过，调用者是driver element = driver.find_element_by_name(\"q\") 高级 页面交互 找到元素后，就是进行“交互”，如键盘输入（需提前导入模块） element.send_keys(“some text”） # 往一个可以输入对象中输入“some text” #甚至 element.send_keys(Keys.RETURN） # 模拟键盘回车 #一般来说，这种方式输入后会一直存在，而要清空某个文本框中的文字，就需要： element.clear() # 清空element对象中的文字 session和cookie Session 是会话的意思，会话是产生在服务端的，用来保存当前用户的会话信息，而 Cookies 是保存在客户端（浏览器），有了 Cookie 以后，客户端（浏览器）再次访问服务端的时候，会将这个 Cookie 带上，这时，服务端可以通过 Cookie 来识别本次请求到底是谁在访问。 可以简单理解为 Cookies 中保存了登录凭证，我们只要持有这个凭证，就可以在服务端保持一个登录状态。 在爬虫中，有时候遇到需要登录才能访问的网页，只需要在登录后获取了 Cookies ，在下次访问的时候将登录后获取到的 Cookies 放在请求头中，这时，服务端就会认为我们的爬虫是一个正常登录用户。 session 那么，Cookies 是如何保持会话状态的呢？ 在客户端（浏览器）第一次请求服务端的时候，服务端会返回一个请求头中带有 Set-Cookie 字段的响应给客户端（浏览器），用来标记是哪一个用户，客户端（浏览器）会把这个 Cookies 给保存起来。 我们来使用工具 PostMan 来访问下某东的登录页，看下返回的响应头： 当我们输入好用户名和密码时，客户端会将这个 Cookies 放在请求头一起发送给服务端，这时，服务端就知道是谁在进行登录操作，并且可以判断这个人输入的用户名和密码对不对，如果输入正确，则在服务端的 Session 记录一下这个人已经登录成功了，下次再请求的时候这个人就是登录状态了。 如果客户端传给服务端的 Cookies 是无效的，或者这个 Cookies 根本不是由这个服务端下发的，或者这个 Cookies 已经过期了，那么接下里的请求将不再能访问需要登录后才能访问的页面。 所以， Session 和 Cookies 之间是需要相互配合的，一个在服务端，一个在客户端。 cookie 我们登陆一个网站，Chrome 中按 F12 打开开发者工具，选择 Application 标签，点开 Cookies 这一栏。 Name：这个是 Cookie 的名字。一旦创建，该名称便不可更改。 Value：这个是 Cookie 的值。 Domain：这个是可以访问该 Cookie 的域名。例如，如果设置为 .jd.com ，则所有以 jd.com ，结尾的域名都可以访问该Cookie。 Max Age：Cookie 失效的时间，单位为秒，也常和 Expires 一起使用。 Max Age 如果为正数，则在 Max Age 秒之后失效，如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会保存该 Cookie 。 Path：Cookie 的使用路径。如果设置为 /path/ ，则只有路径为 /path/ 的页面可以访问该 Cookie 。如果设置为 / ，则本域名下的所有页面都可以访问该 Cookie 。 Size：Cookie 的大小。 HTTPOnly：如果此项打勾，那么通过 JS 脚本将无法读取到 Cookie 信息，这样能有效的防止 XSS 攻击，窃取 Cookie 内容，可以增加 Cookie 的安全性。 Secure：如果此项打勾，那么这个 Cookie 只能用 HTTPS 协议发送给服务器，用 HTTP 协议是不发送的。 那么有的网站为什么这次关闭了，下次打开的时候还是登录状态呢？ 这就要说到 Cookie 的持久化了，其实也不能说是持久化，就是 Cookie 失效的时间设置的长一点，比如直接设置到 2099 年失效，这样，在浏览器关闭后，这个 Cookie 是会保存在我们的硬盘中的，下次打开浏览器，会再从我们的硬盘中将这个 Cookie 读取出来，用来维持用户的会话状态。 第二个问题产生了，服务端的会话也会无限的维持下去么，当然不会，这就要在 Cookie 和 Session 上做文章了， Cookie 中可以使用加密的方式将用户名记录下来，在下次将 Cookies 读取出来由请求发送到服务端后，服务端悄悄的自己创建一个用户已经登录的会话，这样我们在客户端看起来就好像这个登录会话是一直保持的。 重要概念 当我们关闭浏览器的时候会自动销毁服务端的会话，这个是错误的，因为在关闭浏览器的时候，浏览器并不会额外的通知服务端说，我要关闭了，你把和我的会话销毁掉吧。 因为服务端的会话是保存在内存中的，虽然一个会话不会很大，但是架不住会话多啊，硬件毕竟是会有限制的，不能无限扩充下去的，所以在服务端设置会话的过期时间就非常有必要。 当然，有没有方式能让浏览器在关闭的时候同步的关闭服务端的会话，当然是可以的，我们可以通过脚本语言 JS 来监听浏览器关闭的动作，当浏览器触发关闭动作的时候，由 JS 像服务端发起一个请求来通知服务端销毁会话。 由于不同的浏览器对 JS 事件的实现机制不一致，不一定保证 JS 能监听到浏览器关闭的动作，所以现在常用的方式还是在服务端自己设置会话的过期时间 模拟登陆163 import time from selenium import webdriver from selenium.webdriver.common.by import By \"\"\" 使用selenium进行模拟登陆 1.初始化ChromDriver 2.打开163登陆页面 3.找到用户名的输入框，输入用户名 4.找到密码框，输入密码 5.提交用户信息 \"\"\" name = '*' passwd = '*' driver = webdriver.Chrome('./chromedriver') driver.get('https://mail.163.com/') # 将窗口调整最大 driver.maximize_window() # 休息5s time.sleep(5) current_window_1 = driver.current_window_handle print(current_window_1) button = driver.find_element_by_id('lbNormal') button.click() driver.switch_to.frame(driver.find_element_by_xpath(\"//iframe[starts-with(@id, 'x-URS-iframe')]\")) email = driver.find_element_by_name('email') #email = driver.find_element_by_xpath('//input[@name=\"email\"]') email.send_keys(name) password = driver.find_element_by_name('password') #password = driver.find_element_by_xpath(\"//input[@name='password']\") password.send_keys(passwd) submit = driver.find_element_by_id(\"dologin\") time.sleep(15) submit.click() time.sleep(10) print(driver.page_source) driver.quit() 这次的内容有点多，很多都是点了一下，并没有深入介绍，想要熟练掌握，还要多加练习，多啃文档。 丁香园自动化登陆 import requests from bs4 import BeautifulSoup from selenium import webdriver from selenium.webdriver.common.by import By from lxml import etree import time name='1876180****' passwd='dxy123456' driver=webdriver.Chrome(r'C:\\Users\\MZ\\Downloads\\chromedriver_win32\\chromedriver.exe') driver.get('http://www.dxy.cn/bbs/thread/626626#626626') driver.maximize_window() time.sleep(5) current_window_1=driver.current_window_handle print(current_window_1) #点击登陆按钮 button=driver.find_element_by_xpath('//*[@id=\"J-eventBanner\"]/div/div/a[2]') button.click() #点击账号密码登陆 button=driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[1]/a[2]') button.click() email=driver.find_element_by_id('username') email.send_keys(name) password=driver.find_element_by_name('password') password.send_keys(passwd) submit=driver.find_element_by_xpath('//*[@id=\"user\"]/div[1]/div[3]/button') time.sleep(3) submit.click() time.sleep(10) print(driver.page_source) 至此即可获得登陆后的界面信息，抽取评论应该和上次所用方法一样，有时间再做补充。","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"selenium","slug":"selenium","permalink":"https://mz2sj.github.io/tags/selenium/"},{"name":"cookie","slug":"cookie","permalink":"https://mz2sj.github.io/tags/cookie/"},{"name":"session","slug":"session","permalink":"https://mz2sj.github.io/tags/session/"},{"name":"代理","slug":"代理","permalink":"https://mz2sj.github.io/tags/%E4%BB%A3%E7%90%86/"}]},{"title":"02-爬虫","slug":"02-爬虫","date":"2020-04-23T11:49:12.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/23/02-爬虫/","link":"","permalink":"https://mz2sj.github.io/2020/04/23/02-%E7%88%AC%E8%99%AB/","excerpt":"","text":"正则表达式 正则表达式语法 . 表示任何单个字符 [ ] 字符集，对单个字符给出取值范围 ，如[abc]表示a、b、c，[a‐z]表示a到z单个字符 [^ ] 非字符集，对单个字符给出排除范围 ，如[^abc]表示非a或b或c的单个字符 * 前一个字符0次或无限次扩展，如abc* 表示 ab、abc、abcc、abccc等 + 前一个字符1次或无限次扩展 ，如abc+ 表示 abc、abcc、abccc等 ? 前一个字符0次或1次扩展 ，如abc? 表示 ab、abc | 左右表达式任意一个 ，如abc|def 表示 abc、def {m} 扩展前一个字符m次 ，如ab{2}c表示abbc {m,n} 扩展前一个字符m至n次（含n） ，如ab{1,2}c表示abc、abbc ^ 匹配字符串开头 ，如^abc表示abc且在一个字符串的开头 $ 匹配字符串结尾 ，如abc$表示abc且在一个字符串的结尾 ( ) 分组标记，内部只能使用 | 操作符 ，如(abc)表示abc，(abc|def)表示abc、def，使用分组除了整体匹配的符号外，单独的分组也会匹配出来。 \\d 数字，等价于[0‐9] \\w 单词字符，等价于[A‐Za‐z0‐9_] re库 re.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象 re.search(pattern, string, flags=0) re.match() 从一个字符串的开始位置起匹配正则表达式，返回match对象 re.match(pattern, string, flags=0) re.findall() 搜索字符串，以列表类型返回全部能匹配的子串 re.findall(pattern, string, flags=0) re.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型 re.split(pattern, string, maxsplit=0, flags=0) re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 re.finditer(pattern, string, flags=0) re.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 re.sub(pattern, repl, string, count=0, flags=0) flags : 正则表达式使用时的控制标记： re.I --&gt; re.IGNORECASE : 忽略正则表达式的大小写，[A‐Z]能够匹配小写字符 re.M --&gt; re.MULTILINE : 正则表达式中的^操作符能够将给定字符串的每行当作匹配开始 re.S --&gt; re.DOTALL : 正则表达式中的.操作符能够匹配所有字符，默认匹配除换行外的所有字符 淘宝商品爬虫 import requests import re def getHTMLText(url): try: kv={'cookie':'thw=cn; cna=+xjxFvvjZFUCAbegU4BJw+f0; hng=CN%7Czh-CN%7CCNY%7C156; _samesite_flag_=true; cookie2=18ea7aba395e98b25e202aca290dfc51; t=063bd64499346909d85799069ecdfb0a; _tb_token_=e7b0d70737676; sgcookie=EQbeuix2KIOgXG2FFLZOJ; unb=2657451481; uc3=vt3=F8dBxGR2VD%2BIzyEg%2Beo%3D&amp;lg2=UtASsssmOIJ0bQ%3D%3D&amp;id2=UU6kVNSBaZ7V0Q%3D%3D&amp;nk2=oHW%2BtqmuGFM%3D; csg=4349f769; lgc=%5Cu5B5F%5Cu95471023; cookie17=UU6kVNSBaZ7V0Q%3D%3D; dnk=%5Cu5B5F%5Cu95471023; skt=490ef5758fe425d4; existShop=MTU4NzU0MjE2Mg%3D%3D; uc4=nk4=0%40oicDEdY%2Fq4hj7fRzY6bY5rGJXw%3D%3D&amp;id4=0%40U2xpViK2NwPK%2B6Z6pycbkB5Z8GcI; tracknick=%5Cu5B5F%5Cu95471023; _cc_=VFC%2FuZ9ajQ%3D%3D; _l_g_=Ug%3D%3D; sg=313; _nk_=%5Cu5B5F%5Cu95471023; cookie1=WqUIbiciTOhdw8TeNfR1ltACceU5jKLQIv2L5E78Zzo%3D; enc=5kK7r49vDFT6xXh5WeuK224BSMneKyQf4H%2F7R%2FSyjHeMJIz8BwUqOX967CJBECgM1E03U6Y1ORAfflhD%2FmV1Mg%3D%3D; JSESSIONID=F55DD890B8CDAD1F8644BE9DABD3A503; tfstk=cEb1BFvrHAD1s1bZ8tNebz9sepTVa6yBtl9O1MBYuf-uj7C6psAjzL_IQiDnJ3dC.; uc1=cookie16=VT5L2FSpNgq6fDudInPRgavC%2BQ%3D%3D&amp;cookie21=Vq8l%2BKCLjhS4UhJVbhgU&amp;cookie15=V32FPkk%2Fw0dUvg%3D%3D&amp;existShop=false&amp;pas=0&amp;cookie14=UoTUPcllKBuWuA%3D%3D; mt=ci=75_1; v=0; isg=BPDwL_0CWmxemQaV6E8w83tQwb5COdSDUALMBOpBvMsepZBPkkmkE0aX-a3FLoxb; l=eBxryhwuQJlCKyvkBOfaFurza77OSIRYYuPzaNbMiT5POS5B5QzGWZX3qAT6C3GVh6ByR3JMwUXJBeYBqQAonxv92j-la_kmn', 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'} r=requests.get(url,timeout=30,headers=kv) r.raise_for_status() r.encoding=r.apparent_encoding return r.text except: return '爬取失败' def parsePage(glist,html): try: price_list=re.findall('\\\"view_price\\\":\\\"\\d+\\.\\d+\\\"',html) name_list=re.findall(r'\\\"raw_title\\\":\\\"[a-zA-Z0-9_\\w\\s\\\\\\u3010\\u3011\\-*]+\\\"',html) for i in range(len(price_list)): price=eval(price_list[i].split(\":\")[1]) name=eval(name_list[i].split(\":\")[1]) # print(price,name) glist.append([price,name]) except: print('解析失败') def printGoodList(glist): tplt=\"{0:^4}\\t{1:6}\\t{2:^10}\" print(tplt.format('序号',\"商品价格\",'商品名称')) count=0 for g in glist: count+=1 print(tplt.format(count,g[0],g[1])) goods_name='书包' start_url='https://s.taobao.com/search?q='+goods_name info_list=[] page=3 count=0 for i in range(page): count+=1 try: url=start_url+\"&amp;s\"+str(44*i) html=getHTMLText(url) parsePage(info_list,html) print(\"\\r爬取页面当前进度:{:.2f}%\".format(count*100/page),end=\"\") except: continue 正则表达式的问题主要出现在匹配商品名称上面，自己的表达式为\\&quot;raw_title\\&quot;:\\&quot;[a-zA-Z0-9_\\w\\s\\\\\\u3010\\u3011\\-*]+\\&quot;,主要出现的问题是商品名称会出现许多特殊符号，比如【-，等等，自己想用.*来匹配，但是.会匹配所有字符，产生越界问题。参考答案给的解决方案是\\&quot;raw_title\\&quot;\\:\\&quot;.*?\\&quot;,在.*后面加上问号就可以解决越界问题。 BeautifulSoup BeautifulSoup的基本元素 Beautiful Soup库的理解： Beautiful Soup库是解析、遍历、维护“标签树”的功能库，对应一个HTML/XML文档的全部内容 BeautifulSoup类的基本元素: Tag 标签，最基本的信息组织单元，分别用&lt;&gt;和标明开头和结尾； Name 标签的名字，…的名字是'p'，格式：.name; Attributes 标签的属性，字典形式组织，格式：.attrs; NavigableString 标签内非属性字符串，&lt;&gt;…中字符串，格式：.string; Comment 标签内字符串的注释部分，一种特殊的Comment类型; # 导入bs4库 from bs4 import BeautifulSoup import requests # 抓取页面 r = requests.get('https://python123.io/ws/demo.html') # Demo网址 demo = r.text # 抓取的数据 demo # 解析HTML页面 soup = BeautifulSoup(demo, 'html.parser') # 抓取的页面数据；bs4的解析器 # 有层次感的输出解析后的HTML页面 print(soup.prettify()) 标签 直接使用tag名获得 soup.a soup.title 返回的string对象可以继续调用BeautifulSoup的层级标签功能 标签的名字 soup.a.name soup.a.parent.name soup.p.parent.name 可以用来查看层级标签的名字，当层级标签比较复杂时可以使用。 标签的属性 tag = soup.a print(tag.attrs) print(tag.attrs['class']) print(type(tag.attrs)) 注意，返回的是关于标签的一个列表 string print(soup.a.string) print(type(soup.a.string)) 返回标签内的非属性字符串。 prettify() print(soup.prettify()) 返回层次化的标签结构 基于bs4库的HTML内容遍历方法 HTML基本格式:&lt;&gt;…构成了所属关系，形成了标签的树形结构 标签树的下行遍历 .contents 子节点的列表，将``所有儿子节点存入列表 .children 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点 .descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历 标签树的上行遍 .parent 节点的父亲标签 .parents 节点先辈标签的迭代类型，用于循环遍历先辈节点 标签树的平行遍历 .next_sibling 返回按照HTML文本顺序的下一个平行节点标签 .previous_sibling 返回按照HTML文本顺序的上一个平行节点标签 .next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 .previous_siblings 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 标签树的下行遍历 import requests from bs4 import BeautifulSoup r=requests.get('http://python123.io/ws/demo.html') demo=r.text soup=BeautifulSoup(demo,'html.parser') print(soup.contents)# 获取整个标签树的儿子节点 print(soup.body.contents)#返回标签树的body标签下的节点 print(soup.head)#返回head标签 for child in soup.body.children:#遍历儿子节点，不会遍历儿子节点下的节点 print(child) for child in soup.body.descendants:#遍历子孙节点，遍历父节点下面所有节点 print(child) 标签树的上行遍历 soup.title.parent soup.title.parent for parent in soup.a.parents: # 遍历先辈的信息 if parent is None: print(parent) else: print(parent.name) 标签树的平行遍历 注意： 标签树的平行遍历是有条件的 平行遍历发生在同一个父亲节点的各节点之间 标签中的内容也构成了节点,连续的字符串也是节点 print(soup.a.next_sibling)#a标签的下一个标签 print(soup.a.next_sibling.next_sibling)#a标签的下一个标签的下一个标签 print(soup.a.previous_sibling)#a标签的前一个标签 print(soup.a.previous_sibling.previous_sibling)#a标签的前一个标签的前一个标签 for sibling in soup.a.next_siblings:#遍历后续节点 print(sibling) for sibling in soup.a.previous_siblings:#遍历之前的节点 print(sibling) 基于bs4库的html查找方法 &lt;&gt;.find_all(name, attrs, recursive, string, **kwargs) 参数： ∙ name : 对标签名称的检索字符串 ∙ attrs: 对标签属性值的检索字符串，可标注属性检索 ∙ recursive: 是否对子孙全部检索，默认True ∙ string: &lt;&gt;…&lt;/&gt;中字符串区域的检索字符串 简写： (..) 等价于.find_all(…) soup(…) 等价于 soup.find_all(…) 扩展方法： &lt;&gt;.find() 搜索且只返回一个结果，同.find_all()参数 &lt;&gt;.find_parents() 在先辈节点中搜索，返回列表类型，同.find_all()参数 &lt;&gt;.find_parent() 在先辈节点中返回一个结果，同.find()参数 &lt;&gt;.find_next_siblings() 在后续平行节点中搜索，返回列表类型，同.find_all()参数 &lt;&gt;.find_next_sibling() 在后续平行节点中返回一个结果，同.find()参数 &lt;&gt;.find_previous_siblings() 在前序平行节点中搜索，返回列表类型，同.find_all()参数 &lt;&gt;.find_previous_sibling() 在前序平行节点中返回一个结果，同.find()参数 import requests from bs4 import BeautifulSoup r = requests.get('http://python123.io/ws/demo.html') demo = r.text soup = BeautifulSoup(demo,'html.parser') soup # name : 对标签名称的检索字符串 soup.find_all('a') soup.find_all(['a', 'p']) # attrs: 对标签属性值的检索字符串，可标注属性检索 soup.find_all(\"p\",\"course\") soup.find_all(id=\"link\") # 完全匹配才能匹配到 # recursive: 是否对子孙全部检索，默认True soup.find_all('p',recursive=False) # string: &lt;&gt;…&lt;/&gt;中字符串区域的检索字符串 soup.find_all(string = \"Basic Python\") # 完全匹配才能匹配到 好大学排名爬虫给你 爬取url：http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html 爬取思路： 从网络上获取大学排名网页内容 提取网页内容中信息到合适的数据结构（二维数组）-排名，学校名称，总分 利用数据结构展示并输出结果 # 导入库 import requests from bs4 import BeautifulSoup import bs4 自己和官方代码比起来就比较简陋了 r=requests.get('http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html') r.raise_for_status() r.encoding=r.apparent_encoding #自动获取编码，可以省好多事 demo=r.text demo soup=BeautifulSoup(demo,'html.parser') print('{0:^8}\\t{1:^18}\\t{2:^18}'.format(\"排名\",\"学校名称\",\"总分\")) for tr in trs: rank=tr.find_all('td')[0].string school_name=tr.find_all('td')[1].string score=tr.find_all('td')[4].string print('{0:^8}\\t{1:^18}\\t{2:^18}'.format(rank,school_name,score)) Xpath Xpath 语法 XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。 在XPath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。 XML文档是被作为节点树来对待的。 XPath使用路径表达式在XML文档中选取节点。节点是通过沿着路径选取的。下面列出了最常用的路径表达式： nodename 选取此节点的所有子节点。 / 从根节点选取。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选取当前节点。 … 选取当前节点的父节点。 @ 选取属性。 /text() 提取标签下面的文本内容 如： /标签名 逐层提取 /标签名 提取所有名为&lt;&gt;的标签 //标签名[@属性=“属性值”] 提取包含属性为属性值的标签 @属性名 代表取某个属性名的属性值 详细学习：https://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html 使用lxml解析 导入库：from lxml import etree lxml将html文本转成xml对象 tree = etree.HTML(html) 用户名称：tree.xpath(’//div[@class=“auth”]/a/text()’) 回复内容：tree.xpath(’//td[@class=“postbody”]’) 因为回复内容中有换行等标签，所以需要用string()来获取数据。 string()的详细见链接：https://www.cnblogs.com/CYHISTW/p/12312570.html Xpath中text()，string()，data()的区别如下： text()仅仅返回所指元素的文本内容。 string()函数会得到所指元素的所有节点文本内容，这些文本讲会被拼接成一个字符串。 data()大多数时候，data()函数和string()函数通用，而且不建议经常使用data()函数，有数据表明，该函数会影响XPath的性能。 丁香园评论爬虫 chrome的开发工具里面是有copy Xpath这个选项的 # 导入库 from lxml import etree import requests url = \"http://www.dxy.cn/bbs/thread/626626#626626\" req = requests.get(url) html = req.text tree = etree.HTML(html) tree user = tree.xpath('//div[@class=\"auth\"]/a/text()') # print(user) content = tree.xpath('//td[@class=\"postbody\"]') results = [] for i in range(0, len(user)): # print(user[i].strip()+\":\"+content[i].xpath('string(.)').strip()) # print(\"*\"*80) # 因为回复内容中有换行等标签，所以需要用string()来获取数据 results.append(user[i].strip() + \": \" + content[i].xpath('string(.)').strip()) # 打印爬取的结果 for i,result in zip(range(0, len(user)),results): print(\"user\"+ str(i+1) + \"-\" + result) print(\"*\"*100) 比较让自己困惑的是text()和string(.)的用法，博客上介绍说text()返回的是匹配到的所有元素包含文字的列表，而string(.)返回的是合并后的结果。而且string(.)的用法要在获取上级元素后，单独对每个元素调用.xpath(‘string(.)’)。后来我才发现，如果文字中内嵌有其他标签，比如br等，就会将文字分割成单独的各个列表元素，和上一层及的元素并列产生问题。因此我们可以对同一层级的元素使用string(.),这样不管同级元素是否有下级元素，都会统一成一个元素。 收工~~~","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://mz2sj.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"bs4","slug":"bs4","permalink":"https://mz2sj.github.io/tags/bs4/"},{"name":"Xpath","slug":"Xpath","permalink":"https://mz2sj.github.io/tags/Xpath/"}]},{"title":"01-爬虫入门","slug":"01-爬虫","date":"2020-04-21T10:10:41.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/21/01-爬虫/","link":"","permalink":"https://mz2sj.github.io/2020/04/21/01-%E7%88%AC%E8%99%AB/","excerpt":"","text":"HTTP请求 对于http请求，记录一下常用的几种： GET：向指定的资源发出“显示”请求。GET方法应该只用于读取数据 ，查询字符串（名称/值对）是在 GET 请求的 URL 中发送的 。 POST：向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求文本中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。 查询字符串（名称/值对）是在 POST 请求的 HTTP 消息主体中发送的 PUT：向指定资源位置上传输最新内容。 DELETE：请求服务器删除Request-URL所标识的资源。 requests.get 实现基本的requests应用，爬取python之禅 import requests url='https://www.python.org/dev/peps/pep-0020/' res=request.get(url) text=res.text 此时返回的text是html原生文本，想要获取对应内容就需要进行匹配了。 with open('zon_of_python.txt','w') as f: f.write(text[text.find('&lt;pre')+28:text.find('&lt;/pre&gt;')-1]) print(text[text.find('&lt;pre')+28:text.find('&lt;/pre')-1]) Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! python自带的urllib也能实现上述功能 import urllib url = 'https://www.python.org/dev/peps/pep-0020/' res=urllib.request.urlopen(url).read().decode('utf-8') print(res[res.find('&lt;pre')+28:res.find('&lt;/pre')-1]) urllib明显要繁琐些。 request.post request.get只是从网站上爬取已有数据，当我们需要和网站进行数据交互时，就要用到post方法。 import requests def translate(word): url=\"http://fy.iciba.com/ajax.php?a=fy\" data={ 'f':'auto', 't':'atuo', 'w':word, } headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'} response=requests.post(url,data=data,headers=headers) json_data=response.json() return json_data def run(word): result=tranlate(word)['content']['out'] print(result) return result def main(): with open('zon_of_python.txt') as f: zh=[run(word) for word in f] with open('zon_of_python_zh-CN.txt','w') as g: for i in zh: g.write(i+'\\n') if __name__=='__main__': main() data内容和格式的确定需要从chrome的检查工具network中的Headers获取。 爬取豆瓣电影top250 首先要找到我们要爬取的东西在网页的位置，借助检查工具，可以找到对用位置的代码： 借助匹配规则就可以拿到想要的东西（ps：还不会匹配呀，应该是要根据html元素的层级关系来匹配）。借助检查工具中的Copy Xpath功能，可以找到电影名和图片超链接的位置。 获取电影名和图片超链接的Xpath公式为 //*[@id=\"content\"]/div/div[1]/ol/li[23]/div/div[1]/a/img/@alt //*[@id=\"content\"]/div/div[1]/ol/li[23]/div/div[1]/a/img/@src 其中23是电影对应的当前页次序，范围是1-25，刚开始以为是电影的排名还出错了。 电影分布在好几个页面，所以还要找出翻页的规律，借助检查工具点击页码，找到对应位置的代码： 可以发现其中url对应的数字规律，0 25 50… from lxml import etree import time dic={} headers={ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36' } def get_name_imgpath(text): html = etree.HTML(text) for i in range(1,26): name = html.xpath('//*[@id=\"content\"]/div/div[1]/ol/li[{}]/div/div[1]/a/img/@alt'.format(i)) img_path=html.xpath('//*[@id=\"content\"]/div/div[1]/ol/li[{}]/div/div[1]/a/img/@src'.format(i)) print(name[0],img_path[0]) dic.update({name[0]:img_path[0]}) print('crawl start') for page in range(0,226,25): text=requests.get('https://movie.douban.com/top250?start={}&amp;filter='.format(page),headers=headers).text get_name_imgpath(text) time.sleep(1) print('crawl finish') print(dic) 程序运行成功 收工！！！","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://mz2sj.github.io/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"04-二手车价格预测之模型融合","slug":"04-二手车价格预测之模型融合","date":"2020-04-04T11:39:31.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/04/04-二手车价格预测之模型融合/","link":"","permalink":"https://mz2sj.github.io/2020/04/04/04-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/","excerpt":"","text":"虽然这是最后一次打卡的内容，但是自己离掌握这些内容还差得很远，还要继续努力呀！温故而知新。 ensemble的方法 简单加权融合 回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）； 分类：投票（Voting) 综合：排序融合(Rank averaging)，log融合 stacking/blending 构建多层模型，并利用预测结果再拟合预测 boosting/bagging 在xgboost，Adaboost，GBDT中使用，多树的提升方法 Stacking 理论介绍 先训练几个学习器，在这些学习器预测数据的基础上再训练新的学习器。 将个体学习器结合在一起的时候使用的方法叫做结合策略。对于分类问题，我们可以使用投票法来选择输出最多的类。对于回归问题，我们可以将分类器输出的结果求平均值。 在stacking方法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器（meta-learner），次级学习器用于训练的数据叫做次级训练集。次级训练集是在训练集上用初级学习器得到的。 对于训练集和测试集分布不那么一致的情况，用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这里我们一般有两种方法：次级模型尽量选择简单的线性模型 利用K折交叉验证。 代码示例 简单加权平均 ## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值 test_pre1 = [1.2, 3.2, 2.1, 6.2] test_pre2 = [0.9, 3.1, 2.0, 5.9] test_pre3 = [1.1, 2.9, 2.2, 6.0] # y_test_true 代表第模型的真实值 y_test_true = [1, 3, 2, 6] import numpy as np import pandas as pd ## 定义结果的加权平均函数 def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]): Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3) return Weighted_result from sklearn import metrics # 各模型的预测结果计算MAE print('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1)) print('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2)) print('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3)) w = [0.3,0.4,0.3] # 定义比重权值 Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w) print('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre)) 即使是这样对多个预测结果的简单加权，也可以给实验结果带来提升 ## 定义结果的加权平均函数 def Mean_method(test_pre1,test_pre2,test_pre3): Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1) return Mean_result Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3) print('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre)) ## 定义结果的加权平均函数 def Median_method(test_pre1,test_pre2,test_pre3): Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1) return Median_result Median_pre = Median_method(test_pre1,test_pre2,test_pre3) print('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre)) stacking融合（回归） from sklearn import linear_model def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression()): model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values,y_train_true) Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values) return Stacking_result ## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值 train_reg1 = [3.2, 8.2, 9.1, 5.2] train_reg2 = [2.9, 8.1, 9.0, 4.9] train_reg3 = [3.1, 7.9, 9.2, 5.0] # y_test_true 代表第模型的真实值 y_train_true = [3, 8, 9, 5] test_pre1 = [1.2, 3.2, 2.1, 6.2] test_pre2 = [0.9, 3.1, 2.0, 5.9] test_pre3 = [1.1, 2.9, 2.2, 6.0] # y_test_true 代表第模型的真实值 y_test_true = [1, 3, 2, 6] model_L2= linear_model.LinearRegression() Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true, test_pre1,test_pre2,test_pre3,model_L2) print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre)) 我们需要注意的一点是，对于第二层Stacking的模型不宜选取的过于复杂 分类模型融合 from sklearn.datasets import make_blobs from sklearn import datasets from sklearn.tree import DecisionTreeClassifier import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from xgboost import XGBClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons from sklearn.metrics import accuracy_score,roc_auc_score from sklearn.model_selection import cross_val_score from sklearn.model_selection import StratifiedKFold Voting 投票机制 分为软投票和硬投票两种 ''' 硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。 ''' iris = datasets.load_iris() x=iris.data y=iris.target x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7, colsample_bytree=0.6, objective='binary:logistic') clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4, min_samples_leaf=63,oob_score=True) clf3 = SVC(C=0.1) # 硬投票 eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard') for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']): scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy') print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) Accuracy: 0.97 (+/- 0.02) [XGBBoosting] Accuracy: 0.33 (+/- 0.00) [Random Forest] Accuracy: 0.95 (+/- 0.03) [SVM] Accuracy: 0.94 (+/- 0.04) [Ensemble] ''' 软投票：和硬投票原理相同，增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。 ''' x=iris.data y=iris.target x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic') clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4, min_samples_leaf=63,oob_score=True) clf3 = SVC(C=0.1, probability=True) # 软投票 eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1]) clf1.fit(x_train, y_train) for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']): scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy') print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) 分类的Stacking、Blending融合 ''' 5-Fold Stacking ''' from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier import pandas as pd #创建训练的数据集 data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] #模型融合中使用到的各个单模型 clfs = [LogisticRegression(solver='lbfgs'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] #切分一部分数据作为测试集 X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020) dataset_blend_train = np.zeros((X.shape[0], len(clfs))) dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs))) #5折stacking n_splits = 5 skf = StratifiedKFold(n_splits) skf = skf.split(X, y) for j, clf in enumerate(clfs): #依次训练各个单模型 dataset_blend_test_j = np.zeros((X_predict.shape[0], 5)) for i, (train, test) in enumerate(skf): #5-Fold交叉训练，使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。 X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test] clf.fit(X_train, y_train) y_submission = clf.predict_proba(X_test)[:, 1] dataset_blend_train[test, j] = y_submission dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1] #对于测试集，直接用这k个模型的预测值均值作为新的特征。 dataset_blend_test[:, j] = dataset_blend_test_j.mean(1) print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j])) clf = LogisticRegression(solver='lbfgs') clf.fit(dataset_blend_train, y) y_submission = clf.predict_proba(dataset_blend_test)[:, 1] print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission))) Blending，其实和Stacking是一种类似的多层模型融合的形式 其主要思路是把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。 在第一层，我们在这70%的数据上训练多个模型，然后去预测那30%数据的label，同时也预测test集的label。 在第二层，我们就直接用这30%数据在第一层预测的结果做为新特征继续训练，然后用test集第一层预测的label做特征，用第二层训练的模型做进一步预测 其优点在于： 1.比stacking简单（因为不用进行k次的交叉验证来获得stacker feature） 2.避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集 缺点在于： 1.使用了很少的数据（第二阶段的blender只使用training set10%的量） 2.blender可能会过拟合 3.stacking使用多次的交叉验证会比较稳健 ''' Blending ''' #创建训练的数据集 #创建训练的数据集 data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] #模型融合中使用到的各个单模型 clfs = [LogisticRegression(solver='lbfgs'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] #切分一部分数据作为测试集 X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020) #切分训练数据集为d1,d2两部分 X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020) dataset_d1 = np.zeros((X_d2.shape[0], len(clfs))) dataset_d2 = np.zeros((X_predict.shape[0], len(clfs))) for j, clf in enumerate(clfs): #依次训练各个单模型 clf.fit(X_d1, y_d1) y_submission = clf.predict_proba(X_d2)[:, 1] dataset_d1[:, j] = y_submission #对于测试集，直接用这k个模型的预测值作为新的特征。 dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1] print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j])) #融合使用的模型 clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30) clf.fit(dataset_d1, y_d2) y_submission = clf.predict_proba(dataset_d2)[:, 1] print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission))) 分类的Stacking融合(利用mlxtend) !pip install mlxtend import warnings warnings.filterwarnings('ignore') import itertools import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import StackingClassifier from sklearn.model_selection import cross_val_score from mlxtend.plotting import plot_learning_curves from mlxtend.plotting import plot_decision_regions # 以python自带的鸢尾花数据集为例 iris = datasets.load_iris() X, y = iris.data[:, 1:3], iris.target clf1 = KNeighborsClassifier(n_neighbors=1) clf2 = RandomForestClassifier(random_state=1) clf3 = GaussianNB() lr = LogisticRegression() sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr) label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier'] clf_list = [clf1, clf2, clf3, sclf] fig = plt.figure(figsize=(10,8)) gs = gridspec.GridSpec(2, 2) grid = itertools.product([0,1],repeat=2) clf_cv_mean = [] clf_cv_std = [] for clf, label, grd in zip(clf_list, label, grid): scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy') print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label)) clf_cv_mean.append(scores.mean()) clf_cv_std.append(scores.std()) clf.fit(X, y) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X=X, y=y, clf=clf) plt.title(label) plt.show() 其他方法 将特征放进模型中预测，并将预测结果变换并作为新的特征加入原有特征中再经过模型预测结果 （Stacking变化） def Ensemble_add_feature(train,test,target,clfs): # n_flods = 5 # skf = list(StratifiedKFold(y, n_folds=n_flods)) train_ = np.zeros((train.shape[0],len(clfs*2))) test_ = np.zeros((test.shape[0],len(clfs*2))) for j,clf in enumerate(clfs): '''依次训练各个单模型''' # print(j, clf) '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。''' # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test] clf.fit(train,target) y_train = clf.predict(train) y_test = clf.predict(test) ## 新特征生成 train_[:,j*2] = y_train**2 test_[:,j*2] = y_test**2 train_[:, j+1] = np.exp(y_train) test_[:, j+1] = np.exp(y_test) # print(\"val auc Score: %f\" % r2_score(y_predict, dataset_d2[:, j])) print('Method ',j) train_ = pd.DataFrame(train_) test_ = pd.DataFrame(test_) return train_,test_ from sklearn.model_selection import cross_val_score, train_test_split from sklearn.linear_model import LogisticRegression clf = LogisticRegression() data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3) x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test) #模型融合中使用到的各个单模型 clfs = [LogisticRegression(), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs) clf = LogisticRegression() # clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30) clf.fit(New_train, y_train) y_emb = clf.predict_proba(New_test)[:, 1] print(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb))) 本次赛题 import pandas as pd import numpy as np import warnings import matplotlib import matplotlib.pyplot as plt import seaborn as sns warnings.filterwarnings('ignore') %matplotlib inline import itertools import matplotlib.gridspec as gridspec from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier # from mlxtend.classifier import StackingClassifier from sklearn.model_selection import cross_val_score, train_test_split # from mlxtend.plotting import plot_learning_curves # from mlxtend.plotting import plot_decision_regions from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_split from sklearn import linear_model from sklearn import preprocessing from sklearn.svm import SVR from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA import lightgbm as lgb import xgboost as xgb from sklearn.model_selection import GridSearchCV,cross_val_score from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error ## 数据读取 Train_data = pd.read_csv('data/used_car_train_20200313.csv', sep=' ') TestA_data = pd.read_csv('data/used_car_testA_20200313.csv', sep=' ') print(Train_data.shape) print(TestA_data.shape) numerical_cols = Train_data.select_dtypes(exclude = 'object').columns print(numerical_cols) feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','regDate','price']] X_data = Train_data[feature_cols] Y_data = Train_data['price'] X_test = TestA_data[feature_cols] print('X train shape:',X_data.shape) print('X test shape:',X_test.shape) def Sta_inf(data): print('_min',np.min(data)) print('_max:',np.max(data)) print('_mean',np.mean(data)) print('_ptp',np.ptp(data)) print('_std',np.std(data)) print('_var',np.var(data)) print('Sta of label:') Sta_inf(Y_data) X_data = X_data.fillna(-1) X_test = X_test.fillna(-1) def build_model_lr(x_train,y_train): reg_model = linear_model.LinearRegression() reg_model.fit(x_train,y_train) return reg_model def build_model_ridge(x_train,y_train): reg_model = linear_model.Ridge(alpha=0.8)#alphas=range(1,100,5) reg_model.fit(x_train,y_train) return reg_model def build_model_lasso(x_train,y_train): reg_model = linear_model.LassoCV() reg_model.fit(x_train,y_train) return reg_model def build_model_gbdt(x_train,y_train): estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100) param_grid = { 'learning_rate': [0.05,0.08,0.1,0.2], } gbdt = GridSearchCV(estimator, param_grid,cv=3) gbdt.fit(x_train,y_train) print(gbdt.best_params_) # print(gbdt.best_estimator_ ) return gbdt def build_model_xgb(x_train,y_train): model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\ colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror' model.fit(x_train, y_train) return model def build_model_lgb(x_train,y_train): estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100) param_grid = { 'learning_rate': [0.01, 0.05, 0.1], } gbm = GridSearchCV(estimator, param_grid) gbm.fit(x_train, y_train) return gbm XGBoost五折交叉回归验证 ## xgb xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\ colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror' scores_train = [] scores = [] ## 5折交叉验证方式 sk=StratifiedKFold(n_splits=5,shuffle=True,random_state=0) for train_ind,val_ind in sk.split(X_data,Y_data): train_x=X_data.iloc[train_ind].values train_y=Y_data.iloc[train_ind] val_x=X_data.iloc[val_ind].values val_y=Y_data.iloc[val_ind] xgr.fit(train_x,train_y) pred_train_xgb=xgr.predict(train_x) pred_xgb=xgr.predict(val_x) score_train = mean_absolute_error(train_y,pred_train_xgb) scores_train.append(score_train) score = mean_absolute_error(val_y,pred_xgb) scores.append(score) print('Train mae:',np.mean(score_train)) print('Val mae',np.mean(scores)) 划分数据集，并用多种方法训练和预测 ## Split data with val x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,test_size=0.3) ## Train and Predict print('Predict LR...') model_lr = build_model_lr(x_train,y_train) val_lr = model_lr.predict(x_val) subA_lr = model_lr.predict(X_test) print('Predict Ridge...') model_ridge = build_model_ridge(x_train,y_train) val_ridge = model_ridge.predict(x_val) subA_ridge = model_ridge.predict(X_test) print('Predict Lasso...') model_lasso = build_model_lasso(x_train,y_train) val_lasso = model_lasso.predict(x_val) subA_lasso = model_lasso.predict(X_test) print('Predict GBDT...') model_gbdt = build_model_gbdt(x_train,y_train) val_gbdt = model_gbdt.predict(x_val) subA_gbdt = model_gbdt.predict(X_test) 一般比赛中效果最为显著的方法 print('predict XGB...') model_xgb = build_model_xgb(x_train,y_train) val_xgb = model_xgb.predict(x_val) subA_xgb = model_xgb.predict(X_test) print('predict lgb...') model_lgb = build_model_lgb(x_train,y_train) val_lgb = model_lgb.predict(x_val) subA_lgb = model_lgb.predict(X_test) print('Sta inf of lgb:') Sta_inf(subA_lgb) 加权融合 def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]): Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3) return Weighted_result ## Init the Weight w = [0.3,0.4,0.3] ## 测试验证集准确度 val_pre = Weighted_method(val_lgb,val_xgb,val_gbdt,w) MAE_Weighted = mean_absolute_error(y_val,val_pre) print('MAE of Weighted of val:',MAE_Weighted) ## 预测数据部分 subA = Weighted_method(subA_lgb,subA_xgb,subA_gbdt,w) print('Sta inf:') Sta_inf(subA) ## 生成提交文件 sub = pd.DataFrame() sub['SaleID'] = X_test.index sub['price'] = subA sub.to_csv('./sub_Weighted.csv',index=False) ## 与简单的LR（线性回归）进行对比 val_lr_pred = model_lr.predict(x_val) MAE_lr = mean_absolute_error(y_val,val_lr_pred) print('MAE of lr:',MAE_lr) Stacking融合 ## 第一层 train_lgb_pred = model_lgb.predict(x_train) train_xgb_pred = model_xgb.predict(x_train) train_gbdt_pred = model_gbdt.predict(x_train) Strak_X_train = pd.DataFrame() Strak_X_train['Method_1'] = train_lgb_pred Strak_X_train['Method_2'] = train_xgb_pred Strak_X_train['Method_3'] = train_gbdt_pred Strak_X_val = pd.DataFrame() Strak_X_val['Method_1'] = val_lgb Strak_X_val['Method_2'] = val_xgb Strak_X_val['Method_3'] = val_gbdt Strak_X_test = pd.DataFrame() Strak_X_test['Method_1'] = subA_lgb Strak_X_test['Method_2'] = subA_xgb Strak_X_test['Method_3'] = subA_gbdt ## level2-method model_lr_Stacking = build_model_lr(Strak_X_train,y_train) ## 训练集 train_pre_Stacking = model_lr_Stacking.predict(Strak_X_train) print('MAE of Stacking-LR:',mean_absolute_error(y_train,train_pre_Stacking)) ## 验证集 val_pre_Stacking = model_lr_Stacking.predict(Strak_X_val) print('MAE of Stacking-LR:',mean_absolute_error(y_val,val_pre_Stacking)) ## 预测集 print('Predict Stacking-LR...') subA_Stacking = model_lr_Stacking.predict(Strak_X_test) subA_Stacking[subA_Stacking&lt;10]=10 ## 去除过小的预测值 sub = pd.DataFrame() sub['SaleID'] = X_test.index sub['price'] = subA_Stacking sub.to_csv('./sub_Stacking.csv',index=False) print('Sta inf:') Sta_inf(subA_Stacking) 经验总结 结果层面的融合，这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。 特征层面的融合，这个层面其实感觉不叫融合，准确说可以叫分割，很多时候如果我们用同种模型训练，可以把特征进行切分给不同的模型，然后在后面进行模型或者结果融合有时也能产生比较好的效果。 模型层面的融合，模型层面的融合可能就涉及模型的堆叠和设计，比如加Staking层，部分模型的结果作为特征输入等，这些就需要多实验和思考了，基于模型层面的融合最好不同模型类型要有一定的差异，用同种模型不同的参数的收益一般是比较小的。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"比赛","slug":"数据分析/比赛","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%AF%94%E8%B5%9B/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"03-二手车价格预测之建模调参","slug":"03-二手车价格预测之建模调参","date":"2020-04-01T13:22:06.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/01/03-二手车价格预测之建模调参/","link":"","permalink":"https://mz2sj.github.io/2020/04/01/03-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/","excerpt":"","text":"今天要总结的是建模调参部分，可谓重中之重，DataWhale官方的参考代码十分详细，够自己消化好一阵子了。 数据处理 作者向我们分享了一种减少内存空间占用的方法，其思想是对于每个Series将数据类型转化为合适的精度，对于非数值型数据转化为category类型数据。 def reduce_mem_usage(df): \"\"\" iterate through all the columns of a dataframe and modify the data type to reduce memory usage. \"\"\" start_mem = df.memory_usage().sum() print('Memory usage of dataframe is {:.2f} MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype('category') end_mem = df.memory_usage().sum() print('Memory usage after optimization is: {:.2f} MB'.format(end_mem)) print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem)) return df 可以看出，放弃不必要的精度确实能节省不少内存空间。 简单建模与分析 对于数值数据的预测，首先采用了最简单的线性回归模型。 from sklearn.linear_model import LinearRegression model = LinearRegression(normalize=True) model = model.fit(train_X, train_y) sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True) 值得一提的是，我们可以根据线性模型对应的权重来判断属性的重要程度，权重越大，属性与预测结果越相关。 from matplotlib import pyplot as plt subsample_index=np.random.randint(low=0,high=len(train_y),size=50) plt.scatter(train_X['v_9'][subsample_index],train_y[subsample_index],color='black') plt.scatter(train_X['v_9'][subsample_index],model.predict(train_X.loc[subsample_index]),color='blue') plt.xlabel('v_9') plt.ylabel('price') plt.legend(['True Price','Predicted Price'],loc='upper right') print('The predicted price is obvious different from true price') plt.show() 但是我们选取权重最大的属性对数据进行预测后发现和真实数据差别偏大，那么很可能是模型除了问题 对我们要预测的数据作图 import seaborn as sns print('It is clear to see the price shows a exponential distribution') plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train_y) plt.subplot(1,2,2) sns.distplot(train_y[train_y&lt;np.quantile(train_y,0.9)]) 数据呈长尾分布，我们对其进行log变换 train_y_ln = np.log(train_y + 1) import seaborn as sns print('The transformed price seems like normal distribution') plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train_y_ln) plt.subplot(1,2,2) sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, 0.9)]) 长尾现象明显缓解，再作预测，选取最相关的权重。 plt.scatter(train_X['v_9'][subsample_index],train_y[subsample_index],color='black') plt.scatter(train_X['v_9'][subsample_index],np.exp(model.predict(train_X.loc[subsample_index])),color='blue') plt.xlabel('v_9') plt.ylabel('price') plt.legend(['True Price','Predicted Price'],loc='upper right') print('The predicted price seems normal after np.log(transforming') plt.show() 这时拟合效果要好得多。 交叉验证 交叉验证将数据分成若干份，每次取一份作验证集，其余的作训练集，这样可以将模型所有数据都机型训练，并且对模型泛化结果进行检验。 from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_absolute_error, make_scorer def log_transfer(func): def wrapper(y, yhat): result = func(np.log(y), np.nan_to_num(np.log(yhat))) return result return wrapper scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error))) print('AVG:', np.mean(scores)) scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error)) print('AVG:', np.mean(scores)) scores = pd.DataFrame(scores.reshape(1,-1)) scores.columns = ['cv' + str(x) for x in range(1, 6)] scores.index = ['MAE'] scores z在这里就捕将数据输出一一列出了。 但是使用交叉验证时也要考虑到数据的真实情况，比如数据与时间相关时，那么把后时间靠后的数据作为训练集，时间靠前的作为验证集，颠倒了顺序，不合逻辑。在实际建模时，要注意考虑这一点。 绘制学习率验证曲线 可以使我们清楚的看出训练数据和验证数据得分（损失）的下降情况。 from sklearn.model_selection import learning_curve,validation_curve def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )): plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel('Training example') plt.ylabel('score') train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error)) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#区域 plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\") plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label=\"Training score\") plt.plot(train_sizes, test_scores_mean,'o-',color=\"g\", label=\"Cross-validation score\") plt.legend(loc=\"best\") return plt plot_learning_curve(LinearRegression(),'linear_model',train_X[:1000],train_y_ln[:1000],ylim=(0.0,0.5),cv=5,n_jobs=1) 多种模型对比 在交叉验证的基础上，使用多个模型 from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso models = [LinearRegression(), Ridge(), Lasso()] result = dict() for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') 还可以通过带正则化的线性回归模型进行特征筛选。 model = Ridge().fit(train_X, train_y_ln) print('intercept:'+ str(model.intercept_)) sns.barplot(abs(model.coef_), continuous_feature_names) model = Lasso().fit(train_X, train_y_ln) print('intercept:'+ str(model.intercept_)) sns.barplot(abs(model.coef_), continuous_feature_names) 非线性模型 前面说了那么多线性回归的模型，下面介绍非线性模型 from sklearn.linear_model import LinearRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.neural_network import MLPRegressor from xgboost.sklearn import XGBRegressor from lightgbm.sklearn import LGBMRegressor models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), MLPRegressor(solver='lbfgs', max_iter=100), XGBRegressor(n_estimators = 100, objective='reg:squarederror'), LGBMRegressor(n_estimators = 100)] result = dict() for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') result = pd.DataFrame(result) result.index = ['cv' + str(x) for x in range(1, 6)] result 可以看出随机森林还是顶的。 模型调参 贪心调参 ## LGB的参数集合： objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair'] num_leaves = [3,5,10,15,20,40, 55] max_depth = [3,5,10,15,20,40, 55] bagging_fraction = [] feature_fraction = [] drop_rate = [] best_obj = dict() for obj in objective: model = LGBMRegressor(objective=obj) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_obj[obj] = score best_leaves = dict() for leaves in num_leaves: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_leaves[leaves] = score best_depth = dict() for depth in max_depth: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0], max_depth=depth) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_depth[depth] = score sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())]) 网格调参 from sklearn.model_selection import GridSearchCV parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth} model = LGBMRegressor() clf = GridSearchCV(model, parameters, cv=5) clf = clf.fit(train_X, train_y) model = LGBMRegressor(objective='regression', num_leaves=55, max_depth=15) np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) 网格调参比较好理解，我们事先输入我们认为合理的参数，对所有参数的组合进行实验，找出得分最高的参数组合。 贝叶斯调参 from bayes_opt import BayesianOptimization def rf_cv(num_leaves, max_depth, subsample, min_child_samples): val = cross_val_score( LGBMRegressor(objective = 'regression_l1', num_leaves=int(num_leaves), max_depth=int(max_depth), subsample = subsample, min_child_samples = int(min_child_samples) ), X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error) ).mean() return 1 - val rf_bo = BayesianOptimization( rf_cv, { 'num_leaves': (2, 100), 'max_depth': (2, 100), 'subsample': (0.1, 1), 'min_child_samples' : (2, 100) } ) 对于贝叶斯调参和贪心调参，自己确实不够了解，还需要再专门学习一下。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"比赛","slug":"比赛","permalink":"https://mz2sj.github.io/tags/%E6%AF%94%E8%B5%9B/"}]},{"title":"02-二手车价格预测之特征工程","slug":"02-二手车价格预测之特征工程","date":"2020-03-28T11:32:28.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/03/28/02-二手车价格预测之特征工程/","link":"","permalink":"https://mz2sj.github.io/2020/03/28/02-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","excerpt":"","text":"今天要做的笔记是特征工程，感觉这次官方给的代码不是特别全。最近在忙作业和kaggle的比赛，后面自己有时间会再完善一下。自己将按照特征工程的常见任务进行介绍。 异常处理 数据收集的过程中可能会产生脏数据，导致产生离群值，数据分布明显不合理，这时候我们要对数据进行异常处理。常见的异常处理方法有： 通过箱型图删除异常值 官方提供了相关代码 def outliers_proc(data,col_name,scale=3): def box_plot_outliers(data_ser,box_scale): iqr=box_scale*(data_ser.quantile(0.75)-data_ser.quantile(0.25)) val_low=data_ser.quantile(0.25)-iqr val_up=data_ser.quantile(0.75)+iqr rule_low=(data_ser&lt;val_low) rule_up=(data_ser&gt;val_up) return (rule_low,rule_up),(val_low,val_up) data_n=data.copy() data_series=data_n[col_name] rule,value=box_plot_outliers(data_series,box_scale=scale) index=np.arange(data_series.shape[0])[rule[0]|rule[1]] print('Delete number is :{}'.format(len(index))) data_n=data_n.drop(index) data_n.reset_index(drop=True,inplace=True) print('Now column number is :{}'.format(data_n.shape[0])) index_low=np.arange(data_series.shape[0])[rule[0]] outliers=data_series.iloc[index_low] print('Description of data less than the lower bound is :') print(pd.Series(outliers).describe()) index_up=np.arange(data_series.shape[0])[rule[1]] outliers=data_series.iloc[index_up] print('Description of data larger than the upper bound is:') print(pd.Series(outliers).describe()) fig,ax=plt.subplots(1,2,figsize=(10,7)) sns.boxplot(y=data[col_name],data=data,palette='Set1',ax=ax[0]) sns.boxplot(y=data_n[col_name],data=data_n,palette='Set1',ax=ax[1]) return data_n 可以看出异常值得到了有效剔除，不过这段代码不是太懂，有时间还要再研究一下。 BOX-COX 转换（处理有偏分布） 待完善 ，我们希望数据尽量符合正态分布，除了这种方法，常见的还有对数据取log、倒数等。 长尾截断 对此自己持怀疑态度，网上也没找到相关代码。如果数据的特征少时，截断一部分数据还能接受，如果符合长尾的特则很多再进行长尾阶段似乎显得不合适。 特征归一化/标准化 sklearn有专门的数据归一化和标注化的包，当然自己写也不难 归一化 def max_min(x): return (x-np.min(x))/(np.max(x)-np.min(x)) from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() x = scaler.fit_transform(x) 标准化 def standard(x): return (x-np.mean(x))/(np.var(x)) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() x = scaler.fit_transform(x) 幂率分布 这种情况我还没遇到过，在此记录下 log(1+x1+median)log(\\frac{1+x}{1+median})log(1+median1+x​) 数据分桶 通俗理解下，就是将数据分段。举个例子年龄段的分桶 等频分桶 待完善 等距分桶 bin=[i*10 for i in range(31)] data['power_bin']=pd.cut(data['power'],bin,labels=False) data[['power_bin','power']].head() Best-KS分桶 待完善 卡方分桶 待完善 缺失值处理 对于树模型，模型本身已经有了对于缺失值的处理机制，这时候我们可以不对数据进行处理。 其他的常见方法是对缺失值进行填充，常见的有平均数、中位数、众数等等。 分箱，对含有缺失值的数据进行分箱，缺失的数据单独为一类，但是如果还想利用原来未缺失的数据值，分箱并未对缺失值数据进行填充。 特征构造 构造统计量 如各个类别的数据求均值、方差、极值、中位数等等 train_gb=train.groupby('brand') all_info={} for kind,kind_data in train_gb: info={} kind_data=kind_data[kind_data['price']&gt;0] info['brand_amount']=len(kind_data) info['brand_price_max']=kind_data.price.max() info['brand_price_median']=kind_data.price.median() info['brand_price_min']=kind_data.price.min() info['brand_price_sum']=kind_data.price.sum() info['brand_price_std']=kind_data.price.std() info['brand_price_average']=round(kind_data.price.sum()/(len(kind_data)+1),2) all_info[kind]=info brand_fe=pd.DataFrame(all_info).T.reset_index().rename(columns={'index':'brand'}) data=data.merge(brand_fe,how='left',on='brand') 时间特征 求相对时间、节假日等，在本项目中，汽车的使用时间很明显就会对销售价格产生影响。 ata['used_time']=(pd.to_datetime(data['creatDate'],format='%Y%m%d',errors='coerce')- pd.to_datetime(data['regDate'],format='%Y%m%d',errors='coerce')).dt.days 地理特征 前提是你得知道数据中所包含的地理信息， data['city']=data['regionCode'].apply(lambda x:str(x))[:-3] 除了以上的特征构造方式外，还有特征组合、对数据的非线性变换（log、根值、平方）等等。私以为构造可解释的特征组合还比较有说服力，胡乱组合的特征可能就有点碰运气了。 特征筛选 过滤式 通过相关系数选择发、互信息法、卡方检验、Relief、方差选择法选择特征，相关系数法比较好理解，越相关的特征当然更值得选择。 data_numeric=data[['power','kilometer','brand_amount','brand_price_average','brand_price_max','brand_price_median']] correlation=data_numeric.corr() f,ax=plt.subplots(figsize=(7,7)) plt.title('Correlation of Numeric Features with Price',y=1,size=16) sns.heatmap(correlation,square=True,vmax=0.8) u1s1,热力图可以直接看出哪些特征比较相关，不过这个图似乎少了price这个变量 [ 包裹式 把学习器的性能作为特征子集的评价准则，类似于auto-ml？或者是logistic回归的变量系数，系数越大越相关。利用包帮我们自助寻找特征 from mlxtend.feature_selection import SequentialFeatureSelector as SFS from sklearn.linear_model import LinearRegression sfs=SFS(LinearRegression(), k_features=10, forward=True, floating=False, scoring='r2', cv=0) x=data.drop(['price'],axis=1) x=x.fillna(0) y=y.fillna(0) y=data['price'] sfs.fit(x,y) sfs.k_feature_names_ from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs import matplotlib.pyplot as plt fig1=plot_sfs(sfs.get_metric_dict(),kind='std_dev') plt.grid() plt.show() 嵌入式 结合过滤式和包裹式，在训练过程进行特征选择。其实就是把数据输入算法中，让算法自己取选择特征呗。 降维 降维自己还是有点了解的，对于高维度数据可以降到低维度。在数据可视化也常用到。常用的降维算法有PCA、t-SNE、LDA、LSA等等。 这次的内容好多啊，感觉给的笔记也不是很全，还需要自己再多做总结。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"特征工程","slug":"数据分析/特征工程","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"01-二手车价格销售价格预测之EDA","slug":"01-二手车价格销售价格预测之EDA","date":"2020-03-24T08:59:22.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/03/24/01-二手车价格销售价格预测之EDA/","link":"","permalink":"https://mz2sj.github.io/2020/03/24/01-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%94%80%E5%94%AE%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8BEDA/","excerpt":"","text":"写在前面 这次参加了DataWhale组织的二手车销售价值预测组队学习，开始第一部分的学习——数据分析EDA。主要从数据分析流程和软件包工具两方面来介绍自己的收获。 数据分析流程 总结了一下，EDA主要做的工作就是发现数据缺失值和查看数据分布情况以及预测目标与其他因变量的相关性分析。 缺失值分析 对于表格数据，使用DataFrame的info()函数就可以直观的看到数据的缺失情况，但是这个只针对数值型（numeric）数据，缺失数据会以null值的形式呈现，直接调用DataFrame的isnull()函数配合sum()就可以对缺失值数据进行统计。比较坑的是object类型数据，有些缺失值数据并不会用null值表示，像下面这种： “-”明显代表缺失值数据，但被字符串代替了。如何发现这种缺失情况，需要我们对相应的数据调用value_counts()函数，或者更直观点观察看一部分数据看有无异常情况。比如类别数据中，有的类别只出现了一次，和其他类别压根不是一个数量级，那么很有可能是异常值，可以考虑将其删去。 数据分布 首先是预测值的数据分布情况，其次是其他数据（或者说是自变量）的分布情况。对于数值型数据我们可以用正太分布、对数正态分布进行拟合，其他分布自己还不太了解。对于类别特征，我们要查看各种类别对应的数量，可以用柱状图来可视化。也可以查看不同类别的数据对应的目标预测值（price）的分布情况，可以用来分析的图表有箱型图、小提琴图。箱型图可以帮助了解数据的特征范围，也可用来查看离群点。小提琴图在箱型图的基础上显示了目标数据值的分布情况。 相关性分析 主要是预测值和自变量之间的相关性分析，还有各个自变量之间的相关性分析。常用散点图来绘制，看数据之间是不是有相关性关系。如果预测值和某个自变量有很强的线性关系，那么毫无疑问这个变量对我们的预测是十分有用的。如果有两个自变量它们之间具有很明显的线性关系，那么我们可以只保留其中一个。相关性分析中常用到的图表有热力图、散点图等等。 软件工具技能包 以前只知道用pandas读数据，matplotlib绘图，这次学到了一些新知识，在此记录。 missingno 见名知意，这个包是用来处理缺失值的。 missingno.matrix(dataframe) missingno.bar(dataframe) 可以直观的让我们看出哪些数据有缺失值 pandas DataFrame.skew() DataFrame.kurt() 查看数值型数据的偏度和峰度 DataFrame.corr() 计算数据的相关性 Series.astype('category') Series.cat.add_category(list) astype将某个Seiries转成category型数据 add_category为category型数据添加新的类别 pd.melt(dataframe,id_vars,value_vars,var_name,value_name) 将列名转为列数据，id_vars是保留的列，value_vars是要转换的列，var_name、value_name对应保留和转换后的列名，盗用一张图表示 seaborn 有中文的文档，查阅起来十分方便 以前绘图使用的都是matplotlib，今天见识到了seaborn的强大，并且绘制出来的图也十分好看 seaborn.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, color=None, vertical=False, norm_hist=False, axlabel=None, label=None, ax=None) 灵活绘制单变量观测值分布图 ,fit可以配合scipy使用，查看数据是否符合某种数学分布 seaborn.heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels='auto', yticklabels='auto', mask=None, ax=None, **kwargs) 可以将pandas处理的corr数据放进去，绘制出热力图，选择不同的主题就很舒服 seaborn.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None) diag_kind选择对角线图的类型，可选 {‘auto’, ‘hist’, ‘kde’} 三种类型 seaborn.regplot(x, y, data=None, x_estimator=None, x_bins=None, x_ci='ci', scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, dropna=True, x_jitter=None, y_jitter=None, label=None, color=None, marker='o', scatter_kws=None, line_kws=None, ax=None) 注意和之前的不同，由于是两个变量之间的回归，所以x,y值得是变量名，data才是DataFrame数据 seaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, notch=False, ax=None, **kwargs) 同样x、y是字符串，data是数据 seaborn.violinplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, bw='scott', cut=2, scale='area', scale_hue=True, gridsize=100, width=0.8, inner='box', split=False, dodge=True, orient=None, linewidth=None, color=None, palette=None, saturation=0.75, ax=None, **kwargs) 小提琴图的功能与箱型图类似。 它显示了一个（或多个）分类变量多个属性上的定量数据的分布，从而可以比较这些分布。与箱形图不同，其中所有绘图单元都与实际数据点对应，小提琴图描述了基础数据分布的核密度估计。 seaborn.FacetGrid(data, row=None, col=None, hue=None, col_wrap=None, sharex=True, sharey=True, height=3, aspect=1, palette=None, row_order=None, col_order=None, hue_order=None, hue_kws=None, dropna=True, legend_out=True, despine=True, margin_titles=False, xlim=None, ylim=None, subplot_kws=None, gridspec_kws=None, size=None) 来了难点了，以前的只要代数据就可以，这个网格稍微复杂点 col_wrap 指定网络的列数，col指定列维度，row指定行维度，hue指定第三个维度。在指定维度上，会绘制对应类别的数据。 之后可以调用FacetGrid对象的map方法。像下面的小栗子 g = sns.FacetGrid(tips, col=\"time\") g.map(plt.hist, \"tip\"); col对应的维度是time，而time有Lunch和Dinner两类数据，就会分别在两列绘制对应的数句。map方法传入要绘制的图类型函数plt.hist,并传递参数名称。 g = sns.FacetGrid(tips, col=\"sex\", hue=\"smoker\") g.map(plt.scatter, \"total_bill\", \"tip\", alpha=.7) g.add_legend(); 这是传入第三维度的例子 好啦，这次就都这里了，快到deadline了，上传下我的博客。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Regression","slug":"数据分析/Regression","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/Regression/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"EDA","slug":"EDA","permalink":"https://mz2sj.github.io/tags/EDA/"},{"name":"回归","slug":"回归","permalink":"https://mz2sj.github.io/tags/%E5%9B%9E%E5%BD%92/"},{"name":"比赛","slug":"比赛","permalink":"https://mz2sj.github.io/tags/%E6%AF%94%E8%B5%9B/"}]},{"title":"04-spelling-correction","slug":"04-spelling-correction","date":"2020-02-16T05:42:05.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/02/16/04-spelling-correction/","link":"","permalink":"https://mz2sj.github.io/2020/02/16/04-spelling-correction/","excerpt":"","text":"今天要介绍的是拼写检查，主要基于语言模型。 Spelling Tasks 拼写检查任务主要包括两个方面，一方面是拼写错误侦测，另一方面是拼写错误修正。 拼写错误分为两种： 1.Non-word Errors：比如漏写导致的错误，graffe-&gt;giraffe。这类错误产生的词不存在。可以通过词典来进行侦测。找出与这些词相近的词，有两种方法，最小编辑路径法，信道法（Highest noisy channel probability）。 2.Real-word Errors：一种是打印错误，如 three-&gt;there顺序颠倒。另一种是认知错误，如piece-&gt;peace,too-&gt;two，发音相同导致的错误。这类错误产生的词是真实存在的，要先找到与这些词发音拼写相近的词形成候选集，在通过信道模型和分类器选择最好的候选词。 The Nosiy Channel Model of Spelling The intuition of Noisy Model is that the words we see may be wrong words were produced through Noisy Channel。我们观察到的拼写错误的词，可以建立一个概率模型找到正确的词。 w^=argmax⁡w∈VP(w∣x)=argmax⁡w∈VP(x∣w)P(w)P(x)=argmax⁡w∈VP(x∣w)P(w)\\begin{aligned} \\hat{w} &amp;=\\underset{w \\in V}{\\operatorname{argmax}} P(w | x) \\\\ &amp;=\\underset{w \\in V}{\\operatorname{argmax}} \\frac{P(x | w) P(w)}{P(x)} \\\\ &amp;=\\underset{w \\in V}{\\operatorname{argmax}} P(x | w) P(w) \\end{aligned} w^​=w∈Vargmax​P(w∣x)=w∈Vargmax​P(x)P(x∣w)P(w)​=w∈Vargmax​P(x∣w)P(w)​ V是词表 Non-word spellig error example 比如acress这个词我们要找到他的拼写正确的词 首先要产生候选集，候选集的产生又两种，一种是基于拼写上的相近，另一种是基于发音上的相近。 最小编辑距离又有四种类型：插入，删除，替换，相邻位置词的替换。插入删除 编辑距离为1，替换编辑距离为2 大部分拼写错误的编辑距离都在1以内，几乎所有拼写错误都包含在编辑距离2. 会用到两种模型，一种是Language Model，另一种是Channel Model Probability，即正确的词产生某种拼写错误的概率。 P(x∣w)=P(x | w)=P(x∣w)= probability of the edit www是正确词 xxx是拼写错误的词 deletion/insertion/substitution/transposition) 拼写错误的四种情况可以产生对应的混淆矩阵 del[x,y]: count(xy typed as x) ins[x,y]: count(x typed as xy) sub[x,y]: count(x typed as y) trans[x,y]: count(xy typed as yx) 基于语料进行统计，比如有多少次xy被拼成了x啊，x被拼成了xy 基于上图的混淆矩阵我们就可以计算对应错误的概率 P(x∣w)={del⁡[wi−1,wi]count⁡[wi−1,wi], if deletion ins⁡[wi−1,xi]count⁡[wi−1], if insertion sub⁡[xi,wi]count⁡[wi], if substitution trans⁡[wi,wi+1]count⁡[wi,wi+1], if transposition P(x | w)=\\left\\{\\begin{array}{cl} {\\frac{\\operatorname{del}\\left[w_{i-1}, w_{i}\\right]}{\\operatorname{count}\\left[w_{i-1}, w_{i}\\right]},} &amp; {\\text { if deletion }} \\\\ {\\frac{\\operatorname{ins}\\left[w_{i-1}, x_{i}\\right]}{\\operatorname{count}\\left[w_{i-1}\\right]},} &amp; {\\text { if insertion }} \\\\ {\\frac{\\operatorname{sub}\\left[x_{i}, w_{i}\\right]}{\\operatorname{count}\\left[w_{i}\\right]},} &amp; {\\text { if substitution }} \\\\ {\\frac{\\operatorname{trans}\\left[w_{i}, w_{i+1}\\right]}{\\operatorname{count}\\left[w_{i}, w_{i+1}\\right]},} &amp; {\\text { if transposition }} \\end{array}\\right. P(x∣w)=⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​count[wi−1​,wi​]del[wi−1​,wi​]​,count[wi−1​]ins[wi−1​,xi​]​,count[wi​]sub[xi​,wi​]​,count[wi​,wi+1​]trans[wi​,wi+1​]​,​ if deletion if insertion if substitution if transposition ​ 这些概率可以用来对应候选集各个单词的概率，错误词对应的可能的真实词的概率 再加上language model 我们就可以找到最大概率的词，当然啦这只是基于当前词unigram 再看bigram “a stellar and versatile acress whose combination of sass and glamour…” •P(actress|versatile)=.000021 P(whose|actress) = .0010 •P(across|versatile) =.000021 P(whose|across) = .000006 基于bigram计算出对应前后两个词组成序列的概率 •P(“versatile actress whose”) = .000021*.0010 = 210 x10-10 •P(“versatile across whose”) = .000021*.000006 = 1 x10-10 再乘上对应的编辑距离概率，就能得出候选集各个词的概率 Real-word spelling errors 就是这个词在字典中是存在的，但是并不是我们要的那个词。 •The design an construction of the system… •Can they lave him my messages? 第一步也是产生候选集，for each word in sentence 因为这些词是真实存在与字典中的，所以词自身也是在候选集里，其他的还包括单一编辑距离词，发音相近的词等。 真实存在词拼写检查流程比较明显的特点就是它是基于整个句子，对每个词都产生候选集，将所有词进行组合，选择概率最大的那个序列路径的词 •Given a sentence w1,w2,w3,…,wn •Generate a set of candidates for each word wi •Candidate(w1) = {w1, w’1 , w’’1 , w’’’1 ,…} •Candidate(w2) = {w2, w’2 , w’’2 , w’’’2 ,…} •Candidate(wn) = {wn, w’n , w’’n , w’’’n ,…} •Choose the sequence W that maximizes P(W) 下面这张图应该就比较好理解了 很明显，two of the应该是计算后概率最大的路径。 也可以简化计算，每次只替换路径中的一个词，一般来说拼写错误只会拼错一个吧。 和non-word error相比,real-word error多了一个概率计算，no error $$\\mathrm{P}(\\mathrm{w} | \\mathrm{w})$$ 那么这个词本身就是正确的概率是多少呢？需要根据具体应用而定。假设你要做的应用，十个词就有一个是错误的，那么就是0.90，20个词里面有1个是错误的就是0.95. 看一下，现面的例子 在unigram模型中，thew很有可能就是the拼错的，如果假设语料的拼写错误概率是0.9的情况下 State-of-the-art Systems 不再简单的把先验概率乘上channel model产生的错误概率，将独立性假设转为概率不相同 w^=argmax⁡w∈VP(x∣w)P(w)λ\\hat{w}=\\underset{w \\in V}{\\operatorname{argmax}} P(x | w) P(w)^{\\lambda} w^=w∈Vargmax​P(x∣w)P(w)λ 针对non-word error，产生了上面的公式，λ\\lambdaλ可以通过development test set 中学习出来。 Phonetic error model 简单的来说就是加上发音来修正拼写错误，原来的拼写错误都是基于字母的拼写错误，现在把发音因素也考虑到拼写错误。比如下面的规则： •“Drop duplicate adjacent letters, except for C.” •“If the word begins with ‘KN’, ‘GN’, ‘PN’, ‘AE’, ‘WR’, drop the first letter.” “Drop ‘B’ if after ‘M’ and if it is at the end of the 去掉这些字母后，根据发音来进行spelling correction？？？ 接着找到和拼写错误词发音相差1-2的最小编辑错误候选词 计算概率时对不同的编辑错误候选给与不同的权重。 Improvement to channel model 更丰富的编辑距离，不仅仅是删除插入替换等 •ent–&gt;ant •ph–&gt;f •le–&gt;al 将发音错误考虑到channel里面 channel model 除了基于语料统计出插入删除替换产生的channel model，还可以考虑其他因素 •The source letter •The target letter •Surrounding letters •The position in the word •Nearby keys on the keyboard •Homology on the keyboard •Pronunciations •Likely morpheme transformations classifier-based methods for real-word spelling correction 除了基于language model 、channel model的生成式模型，我们还可以利用特征输入分类器来进行拼写错误检查。 比如下面的例子 whether/weather 判断这个拼写错误到底是哪一个，我们考虑下面这些特征： 附近10个词内是否有cloudy，有的话那么很可能就是weather天气 后面 是不是接的 to VERB，是的话那么很有可能就是whether 是否 后面接的是不是 or not，是的话很有可能就是or not 是否","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"03-language-modeling","slug":"03-language-modeling","date":"2020-02-14T09:46:04.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/14/03-language-modeling/","link":"","permalink":"https://mz2sj.github.io/2020/02/14/03-language-modeling/","excerpt":"","text":"今天要学习的是语言模型，好多东西呀！ 概念 老师介绍了一大堆，我的理解语言模型就是对语言的合理性进行建模。这种建模可以是一个完整的句子的可能性，也可以是一句话基于之前的词预测出下一个词的可能性。数学公式表达如下： P(W)=P(w1,W2,W3,W4,W5…Wn)\\mathrm{P}(\\mathrm{W})=\\mathrm{P}\\left(\\mathrm{w}_{1}, \\mathrm{W}_{2}, \\mathrm{W}_{3}, \\mathrm{W}_{4}, \\mathrm{W}_{5} \\dots \\mathrm{W}_{\\mathrm{n}}\\right)P(W)=P(w1​,W2​,W3​,W4​,W5​…Wn​) 一个句子出现的概率 P(wn∣w1,W2…Wn−1)\\mathrm{P}\\left(\\mathrm{w}_{\\mathrm{n}} | \\mathrm{w}_{1}, \\mathrm{W}_{2} \\dots \\mathrm{W}_{\\mathrm{n}-1}\\right)P(wn​∣w1​,W2​…Wn−1​) 基于之前词预测下一词 语言模型的计算 那么语言模型该如何计算呢？我们可以通过链式法则来计算一句话所有单词的联合概率 P(w1w2⋯wn)=∏iP(wi∣w1w2⋯wi−1)P\\left(w_{1} w_{2} \\cdots w_{n}\\right)=\\prod_{i} P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) P(w1​w2​⋯wn​)=i∏​P(wi​∣w1​w2​⋯wi−1​) 举个例子 P(′′its water is so transparent&quot; ′′)=\\mathrm{P}\\left(^{\\prime \\prime} \\mathrm{its} \\text { water is so transparent&quot; }^{\\prime \\prime}\\right)=P(′′its water is so transparent&quot; ′′)=P(its)×P( water ∣ its )×P( is lits water )\\quad \\mathrm{P}(\\mathrm{its}) \\times \\mathrm{P}(\\text { water } | \\text { its }) \\times \\mathrm{P}(\\text { is lits water })P(its)×P( water ∣ its )×P( is lits water )×P( so lits water is )×\\quad \\times \\mathrm{P}(\\text { so lits water is }) \\times×P( so lits water is )×P(transparent) its water is so))) 这样一来，每个字都是依赖于之前所有的字，通过链式法则从第一个字一直传递到最后一个字，形成完整的一句话的概率。 但是随着句子长度增长，条件概率的计算变得不现实 P( the ∣ its water is so transparent that )=Count(itswaterissotransparentthatthe)/Count(itswaterissotransparentthat)P(\\text { the } | \\text { its water is so transparent that })= Count (its water is so transparent that the) /Count (its water is so transparent that) P( the ∣ its water is so transparent that )=Count(itswaterissotransparentthatthe)/Count(itswaterissotransparentthat) 我们通过对序列计数来计算条件概率，当句子过长时，句子序列数量增长，有些句子序列我们根本就没见过，会导致分母部分概率为0，导致无法计算。由此引入了马尔科夫假设，即一个事务的状态只与之前的状态有关。 整个序列的概率可以用如下公式估计 P(w1w2⋯wn)≈∏P(wi∣wi−k⋯wi−1)P\\left(w_{1} w_{2} \\cdots w_{n}\\right) \\approx \\prod P\\left(w_{i} | w_{i-k} \\cdots w_{i-1}\\right) P(w1​w2​⋯wn​)≈∏P(wi​∣wi−k​⋯wi−1​) 单个单词的条件概率估计 P(wi∣w1w2⋯wi−1)≈P(wi∣wi−k⋯wi−1)P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) \\approx P\\left(w_{i} | w_{i-k} \\cdots w_{i-1}\\right) P(wi​∣w1​w2​⋯wi−1​)≈P(wi​∣wi−k​⋯wi−1​) 当当前词的概率只与当前词有关，则是Unigram Model P(w1w2⋯wn)≈∏iP(wi)P\\left(w_{1} w_{2} \\cdots w_{n}\\right) \\approx \\prod_{i} P\\left(w_{i}\\right) P(w1​w2​⋯wn​)≈i∏​P(wi​) 当当前词概率与前一个词有关时，则是Bigram Model P(wi∣w1w2⋯wi−1)≈P(wi∣wi−1)P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) \\approx P\\left(w_{i} | w_{i-1}\\right) P(wi​∣w1​w2​⋯wi−1​)≈P(wi​∣wi−1​) 由此可以继续推出3-gram、4-gram Model等n-gram model模型。 n-gram模型基于条件独立假设，忽略了语言之间的依赖性，但我们仍使用它。 预测N-gram 概率 举个bigram例子 P(wi∣wi−1)=c(wi−1,wi)c(wi−1)P\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)}{c\\left(w_{i-1}\\right)} P(wi​∣wi−1​)=c(wi−1​)c(wi−1​,wi​)​ 在实际应用中，我们会加上句子的开始&lt;s&gt;和&lt;/s&gt;结尾符。 &lt;s&gt; I am Sam &lt;/s&gt; &lt;s&gt; Sam I am &lt;/s&gt; &lt;s&gt; I do not like green eggs and ham &lt;/s&gt; P(I∣&lt;s&gt;)=23=.67P( Sam ∣&lt;s)=13=.33P(am∣I)=23=.67P(&lt;/s&gt;∣ Sam )=12=0.5P( Sam ∣am)=12=.5P(do∣I)=13=.33\\begin{array}{ll} {P(\\mathrm{I} |&lt;\\mathrm{s}&gt;)=\\frac{2}{3}=.67} &amp; {P(\\text { Sam } |&lt;\\mathrm{s})=\\frac{1}{3}=.33 \\quad P(\\mathrm{am} | \\mathrm{I})=\\frac{2}{3}=.67} \\\\ {P(&lt;/ \\mathrm{s}&gt;| \\text { Sam })=\\frac{1}{2}=0.5} &amp; {P(\\text { Sam } | \\mathrm{am})=\\frac{1}{2}=.5 \\quad P(\\mathrm{do} | \\mathrm{I})=\\frac{1}{3}=.33} \\end{array} P(I∣&lt;s&gt;)=32​=.67P(&lt;/s&gt;∣ Sam )=21​=0.5​P( Sam ∣&lt;s)=31​=.33P(am∣I)=32​=.67P( Sam ∣am)=21​=.5P(do∣I)=31​=.33​ 基于语料我们可以形成对应bigram表 这种概率计算出来的概率会很小，可能会带来underflow问题，由此引出log函数将连乘转化为累加 log⁡(p1×p2×p3×p4)=log⁡p1+log⁡p2+log⁡p3+log⁡p4\\log \\left(p_{1} \\times p_{2} \\times p_{3} \\times p_{4}\\right)=\\log p_{1}+\\log p_{2}+\\log p_{3}+\\log p_{4} log(p1​×p2​×p3​×p4​)=logp1​+logp2​+logp3​+logp4​ 评价语言模型 perplexity 除了可以根据任务评价语言模型外，最直接的语言模型评价方法就是perplexity 一个好的语言模型应该能给真实的句子更高的概率，我们可以据此来评价语言模型。将模型在测试集上的概率的倒数n次幂后作为perplexity，bigrams公式如下 PP⁡(W)=∏i=1N1P(wi∣wi−1)N\\operatorname{PP}(W)=\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} | w_{i-1}\\right)}} PP(W)=Ni=1∏N​P(wi​∣wi−1​)1​​ 困惑度越低代表模型越好 Smoothing 平滑 当训练集和测试集很相似的时候，n-grams模型可以取得很好效果。但是现实中，训练集和测试集常有差别，导致在测试集中出现许多在训练集中没见过的东西。没见过的东西概率是0，那么测试集中原本是正确的句子可能被预测为错误，而且概率为0，导致无法计算困惑度。 劫富济贫 将一些概率较高的gram分配给一些未出现过的gram Add-one estimation 对所有词都加一，那么分母就要机上词表大小V PAdd-1(wi∣wi−1)=c(wi−1,wi)+1c(wi−1)+VP_{\\text {Add-1}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+1}{c\\left(w_{i-1}\\right)+V} PAdd-1​(wi​∣wi−1​)=c(wi−1​)+Vc(wi−1​,wi​)+1​ Add-one方法显得十分粗糙，所以并没有用在N-grams中，但在文本分类和0的数目不是很多的任务中会用到。 也可以不加1，加k，由此引出下列模型 PAdd −k(wi∣wi−1)=c(wi−1,wi)+kc(wi−1)+kVPAdd-k(wi∣wi−1)=c(wi−1,wi)+m(1V)c(wi−1)+m\\begin{aligned} &amp;P_{\\text {Add }-k}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+k}{c\\left(w_{i-1}\\right)+k V}\\\\ &amp;P_{\\text {Add-k}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m\\left(\\frac{1}{V}\\right)}{c\\left(w_{i-1}\\right)+m} \\end{aligned} ​PAdd −k​(wi​∣wi−1​)=c(wi−1​)+kVc(wi−1​,wi​)+k​PAdd-k​(wi​∣wi−1​)=c(wi−1​)+mc(wi−1​,wi​)+m(V1​)​​ 保证分子加上的和和分母加上的相等即可，也可加上当前词的概率 PAdd-k(wi∣wi−1)=c(wi−1,wi)+m(1V)c(wi−1)+mPUnigramPrior (wi∣wi−1)=c(wi−1,wi)+mP(wi)c(wi−1)+m\\begin{aligned} &amp;P_{\\text {Add-k}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m\\left(\\frac{1}{V}\\right)}{c\\left(w_{i-1}\\right)+m}\\\\ &amp;P_{\\text {UnigramPrior }}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m P\\left(w_{i}\\right)}{c\\left(w_{i-1}\\right)+m} \\end{aligned} ​PAdd-k​(wi​∣wi−1​)=c(wi−1​)+mc(wi−1​,wi​)+m(V1​)​PUnigramPrior ​(wi​∣wi−1​)=c(wi−1​)+mc(wi−1​,wi​)+mP(wi​)​​ 模型的融合 Backoff 回退 试一试那种语言模型好，谁好用谁。例如计算trigram时，预料中没有这个trigram，就用bigram来代替，bigram没有就用unigram代替。 Interpolation 插值 将 unigram，bigram，trigram混用，通常会表现的更好。 简单的interpolation P^(wn∣wn−1wn−2)=λ1P(wn∣wn−1wn−2)+λ2P(wn∣wn−1)+λ3P(wn)\\begin{aligned} \\hat{P}\\left(w_{n} | w_{n-1} w_{n-2}\\right)=&amp; \\lambda_{1} P\\left(w_{n} | w_{n-1} w_{n-2}\\right) \\\\ &amp;+\\lambda_{2} P\\left(w_{n} | w_{n-1}\\right) \\\\ &amp;+\\lambda_{3} P\\left(w_{n}\\right) \\end{aligned} P^(wn​∣wn−1​wn−2​)=​λ1​P(wn​∣wn−1​wn−2​)+λ2​P(wn​∣wn−1​)+λ3​P(wn​)​ ∑iλi=1\\sum_{i} \\lambda_{i}=1 i∑​λi​=1 基于上文的interpolation P^(wn∣wn−2wn−1)=λ1(wn−2n−1)P(wn∣wn−2wn−1)+λ2(wn−2n−1)P(wn∣wn−1)+λ3(wn−2n−1)P(wn)\\begin{aligned} \\hat{P}\\left(w_{n} | w_{n-2} w_{n-1}\\right)=&amp; \\lambda_{1}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n} | w_{n-2} w_{n-1}\\right) \\\\ &amp;+\\lambda_{2}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n} | w_{n-1}\\right) \\\\ &amp;+\\lambda_{3}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n}\\right) \\end{aligned} P^(wn​∣wn−2​wn−1​)=​λ1​(wn−2n−1​)P(wn​∣wn−2​wn−1​)+λ2​(wn−2n−1​)P(wn​∣wn−1​)+λ3​(wn−2n−1​)P(wn​)​ 我们要找到能使概率最大的lambda log⁡P(w1…wn∣M(λ1…λk))=∑ilog⁡PM(λ1…λk)(wi∣wi−1)\\log P\\left(w_{1} \\ldots w_{n} | M\\left(\\lambda_{1} \\ldots \\lambda_{k}\\right)\\right)=\\sum_{i} \\log P_{M\\left(\\lambda_{1} \\ldots \\lambda_{k}\\right)}\\left(w_{i} | w_{i-1}\\right) logP(w1​…wn​∣M(λ1​…λk​))=i∑​logPM(λ1​…λk​)​(wi​∣wi−1​) 对于大规模的n-gram模型，如果某个词序列个数为0，导致n-gram为0，则可以降低n-gram阶数，计算之前的n-gram概率。 S(wi∣wi−k+1i−1)={count⁡(wi−k+1i)count⁡(wi−k+1i−1) if count⁡(wi−k+1i)&gt;00.4S(wi∣wi−k+2i−1) otherwise S\\left(w_{i} | w_{i-k+1}^{i-1}\\right)=\\left\\{\\begin{array}{cc} {\\frac{\\operatorname{count}\\left(w_{i-k+1}^{i}\\right)}{\\operatorname{count}\\left(w_{i-k+1}^{i-1}\\right)}} &amp; {\\text { if } \\operatorname{count}\\left(w_{i-k+1}^{i}\\right)&gt;0} \\\\ {0.4 S\\left(w_{i} | w_{i-k+2}^{i-1}\\right)} &amp; {\\text { otherwise }} \\end{array}\\right. S(wi​∣wi−k+1i−1​)=⎩⎨⎧​count(wi−k+1i−1​)count(wi−k+1i​)​0.4S(wi​∣wi−k+2i−1​)​ if count(wi−k+1i​)&gt;0 otherwise ​ 高级平滑 Absolute Discounting Interpolation PAbsoluteDiscounting (wi∣wi−1)=c(wi−1,wi)−dc(ui)+λ(wdi−1)P(w)P_{\\text {AbsoluteDiscounting }}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)-d}{c\\left(u_{i}\\right)}+\\lambda\\left(\\stackrel{d}{w}_{i-1}\\right) P(w) PAbsoluteDiscounting ​(wi​∣wi−1​)=c(ui​)c(wi−1​,wi​)−d​+λ(wdi−1​)P(w) 将我们频率出现较高的词减去一部分，分给较低的一部分。d通常取0.75或者比较小的数 Kneser-Ney Smotting I used to eat Chinese food with ______ instead of knife and fork. 当我们在一句话中预测未知词，我们除了要考虑词的unigram，还要考虑这个词的bigram，它是否适合和别的词组成词，即一种连续性。 PCONTTNUATION (w)=∣{wi−1:c(wi−1,w)&gt;0}∣∣{(wj−1,wj):c(wj−1,wj)&gt;0}P_{\\text {CONTTNUATION }}(w)=\\frac{\\left|\\left\\{w_{i-1}: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right|}{|\\left\\{\\left(w_{j-1}, w_{j}\\right): c\\left(w_{j-1}, w_{j}\\right)&gt;0\\right\\}} PCONTTNUATION ​(w)=∣{(wj−1​,wj​):c(wj−1​,wj​)&gt;0}∣{wi−1​:c(wi−1​,w)&gt;0}∣​ 上面时w这个词在语料中产生的bigram数量，下面是语料中所有词的bigram数量 PCONTTNUATON (w)=∣{wi−1:c(wi−1,w)&gt;0}∣∑w′∣{wi−1′:c(wi−1′,w′)&gt;0}∣P_{\\text {CONTTNUATON }}(w)=\\frac{\\left|\\left\\{w_{i-1}: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right|}{\\sum_{w^{\\prime}}\\left|\\left\\{w_{i-1}^{\\prime}: c\\left(w_{i-1}^{\\prime}, w^{\\prime}\\right)&gt;0\\right\\}\\right|} PCONTTNUATON ​(w)=∑w′​∣∣​{wi−1′​:c(wi−1′​,w′)&gt;0}∣∣​∣{wi−1​:c(wi−1​,w)&gt;0}∣​ 将PCONTINUATION (w)P_{\\text {CONTINUATION }}(w)PCONTINUATION ​(w)引入之前定义的abosolute discounting PKN(wi∣wi−1)=max⁡(c(wi−1,wi)−d,0)c(wi−1)+λ(wi−1)PCONTINUATION(wi)P_{K N}\\left(w_{i} | w_{i-1}\\right)=\\frac{\\max \\left(c\\left(w_{i-1}, w_{i}\\right)-d, 0\\right)}{c\\left(w_{i-1}\\right)}+\\lambda\\left(w_{i-1}\\right) P_{\\text {CONTINUATION}}\\left(w_{i}\\right) PKN​(wi​∣wi−1​)=c(wi−1​)max(c(wi−1​,wi​)−d,0)​+λ(wi−1​)PCONTINUATION​(wi​) max是为了使分子都大于0 λ(wi−1)=dc(wi−1)∣{w:c(wi−1,w)&gt;0}∣\\lambda\\left(w_{i-1}\\right)=\\frac{d}{c\\left(w_{i-1}\\right)}\\left|\\left\\{w: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right| λ(wi−1​)=c(wi−1​)d​∣{w:c(wi−1​,w)&gt;0}∣ λ\\lambdaλ取值，只有当c(wi−1,wi)c\\left(w_{i-1}, w_{i}\\right)c(wi−1​,wi​)&gt;0时才会减去d，这里又乘上减去d的次数。 平滑算法太复杂啦，今天先到这儿，我还要再理解理解。","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"02 basic text processing","slug":"02-basic-text-processing","date":"2020-02-12T12:45:27.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/12/02-basic-text-processing/","link":"","permalink":"https://mz2sj.github.io/2020/02/12/02-basic-text-processing/","excerpt":"","text":"今天学习正则表达式的使用，后面还有一个小作业要做，开始啦！ 正则表达式 Disjunctions 析取 我也不知道中文该怎么说，暂定析取吧。disjunction用 [] 表示，表示选取括号中表示的一个字符。下面举个例子 Pattern Matches [wW]oodchunk Woodchunk,woodchunk [123456789] any digit(由于匹配是贪心的，所以只要是数字都会匹配) 像数字每次都要把十个阿拉伯数字都写出来太麻烦了，所以有了range用法 Pattern Matches [A-Z] An uppeer case letter [a-z] A lowwer case letter [0-9] A single digit Negation in Disjunction 析取取反 翻译忽略， ^ carat就是取反的意思，在disjunction中经常会用到 ^ 只有在析取 [] 中作为第一个字符才表示取反，也就是说**^** 与 [] 常常连用 Pattern Matcher Examples [^A-Z] Not an upper case letter Hello World [^Ss] Neither S nor s Say [e] Neither e nor ^ ^eergg a^b The pattern a carat b,as a signal tobe matched a^b More Disjunction 取或 用 **|**来表示，表示取或的意思，或者理解成linux的管道？在前面的基础上继续进行运算。 Pattern Matches hello|world 匹配hello或者world a|b|c 等同于[abc] [Hh]ello|[Ww]orld 匹配大小写开头的hello或者world 值得注意的是 | 判断的是符号之前或之后的连续字符是否匹配，而不是只看前面若干个字符匹配情况。 ？ * +. Patter Matches Instances colou?r optional previous char,?号前面的字符可有可无 color colour oo*h! 0 or more of previous char,0次或者更多次 oh!,ooh!,oooh! o+h! 1 or more of previous char,1次或者更多次 oh!,ooh! baa+ baa,baaa beg.n 1 char,单个字符，可以是任意字符 beggn，beg1n，beg2n Anchors ^$ 锚 说白了，就是表明位置。^ 后的字符表示该字符在序列开始处，$ 前的字符表示该字符在结尾处。 Pattern Matches ^[A-Z] Palo Alto 只匹配前面的一个大写P ^[^A-Za-z] **1 “**Hello” 实验证明只能匹配一个字符并不能匹配引号，ppt上有误 \\.$ The end. 以.结尾 .$ 匹配结尾的任意一个字符 The end**？** The end**！** 文本序列化 Text Normalization 文本规范化 引出几个概念 lemma 词根：cat 和 cats的词根是相同的 wordform 词形：cat 和 cats的词形式不同的 对于词的类型和数目也有讲究，比如一句简单的英语 they lay back on the San Fancisco grass and looked at the stars and their 关于词的数目，San Fancisco是算一个词还是两个词呢 关于词的类型，they和their算不算同一类词呢？ 都是问题 我们通产用N表示token的数目，用V表示词典词的大小，也就是词类型数目 不同的语言在token的过程中会遇到不同的问题，这里就不一一例举了。 课程中还专门介绍了中文分词方法，最大正向匹配法。最大正向匹配法的思想很简单，就是拿词典中最长的词去遍历语句，匹配成功则分词，不成功则换长度更短的词继续匹配，直至匹配结束。 下面介绍集中常见的文本规范化方法。 Word Normalization Case folding 大小写转换，文本处理中我们通常会将大写转化为小写，但是这也会带来一些问题。例如机器翻译中，大写US和小写us完全不是一个意思。但是在信息检索中，我们习惯小写，如果出现了大写也会同时检索小写的结果。 Lemmatization 词性还原，根据词典找到词初始的词形式，比如单复数，所有格，时态的还原 am，are，is --&gt; be Morphology 形态还原，将词分为词干和词缀，在信息检索中就常会用到Stemming。保留词的词干用来检索， 例如compressed和compression都能通过它们的词干compress检索到。 英语中会有专门的规则用来获取词的词干，比如下面这条。 （*v*)ing 去掉ing后的词干，词干中含有元音 walking --&gt;walk a是元音 sing --&gt;sing s不是元音，所以词干不是s 秀一波正则表达式，.*[aoeiu].*ing$，就是上面我们说的匹配所有类似于walking这样的词 Sentence Segmentation and Decision Trees 句子的划分有时候也是一个问题。英语中的句号.有时会被当作其他用途，并不是真正表示一句话结束，例如%0.2中的.就不是表示句子结束,所以我们要建立一个分类器来判断哪里是句子的开始哪里句子结束了. 例如决策树来判断句子是否结束. 我们需要手工选取许多特征输入决策树让决策树判断","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://mz2sj.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"01 introduction to nlp","slug":"01-introduction-to-nlp","date":"2020-02-12T08:10:18.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/12/01-introduction-to-nlp/","link":"","permalink":"https://mz2sj.github.io/2020/02/12/01-introduction-to-nlp/","excerpt":"","text":"写在前面 研究生读了半年了，越来越迷茫了，没有认认真真做好一件事。今天，立帖为证，我一定要将这门课认真修完，勤做笔记，完成作业。话不多说，开始吧！附上课程链接。 nlp的应用领域 Question Answering 问答 Information Extraction 信息抽取 Sentiment Analysis 情感分析 机器翻译 自然语言在垃圾邮件侦测、词性标注、命名实体识别上已经取得了较好的表现，在情感分类、指代消解、句法分析、机器翻译、信息抽取也有所进步，尤其是现在的神经机器翻译，比之前的统计机器翻译取得了长足的进步，不过这门课开设时间已经比较早了，那时候深度学习还没有大规模应用到nlp中。但在问答、摘要、对话系统、改述（Paraphrase）等方面表现还有待提高。 歧义让nlp变得更加困难 举个例子，这是纽约时报的一条新闻头条 Fed raises interest rates 既可以理解为Fed这个人将利率interest rates 提高了，也可以理解为Fed这个人对rates这个东西起了兴趣raise interest。 当给出更多信息时，可以帮助我们消除这种模糊性。 Fed raises interest rates 0.5% 这时候我们就知道了新闻真实想表达的意思是利率提高了%0.5，而不是Fed来了兴趣. 其他的困难 非标准英语 比如推特上的用语可能就很不规范 切分问题 一个词不同的切分会带来不同的问题 俚语 类似于我们的成语,不能通过字面意思来判断他的意思 新词 类似于给力这种新词汇 外部知识 知识的不确定性 实体名 有些词可能是专有名词,不能按字面意思理解 写在后面 第一篇差不多就到这儿了,完事开头难,加油写下去哦!","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"使用torchtext处理中文文本","slug":"使用torchtext处理中文文本","date":"2020-01-08T11:45:08.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/08/使用torchtext处理中文文本/","link":"","permalink":"https://mz2sj.github.io/2020/01/08/%E4%BD%BF%E7%94%A8torchtext%E5%A4%84%E7%90%86%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC/","excerpt":"","text":"torchtext是pytorch中专门用来处理文本的包，使用torchtext可以灵活的帮我们生成batch和词向量。网上介绍的大多是基于英文的版本，自己在中文文本上进行了尝试，将之前的英文文本分类改成中文文本分类。下面开始介绍。 数据格式 text label 又要写博客了，好开心啊 0 程序出bug了，唉 1 数据格式可以是csv、tsv、json文件，自己比较喜欢处理csv、tsv文件两者是一样的，json文件还没怎么接触。数据的内容如上所示，包括文本列和标签列，一般会分为train、dev、test三个集合。本文用到train.tsv、dev.tsv、test.tsv。 Field 我也不知道这个该叫啥，针对我们要处理的不同数据分别创建Field对象。 import torchtext.data as data import jieba def tokenizer(x): return list(jieba.cut(x)) text_field=data.Field(tokenize=tokenizer,fix_length=60) label_field=data.LabelField(dtype=torch.long) 针对我们需要处理的text列和label列，我们创建了两个Field对象，分别用来处理对应列。因为我们需要对text进行分词，所以需要指定分词器，我这里用的结巴。分词器的返回结果需要是关于词语的列表，可以根据自己的需求指定。fix_length指定单个序列的长度，torchtext会自动帮我们进行padding和裁剪操作。label列是分类的类别列，torchtext相应提供了LabelField，需要指定数据类型。看别人的代码，使用Field也能用，自己还没实验过。 Field中还有其他参数 def generate_bigrams(x): n_grams = set(zip(*[x[i:] for i in range(2)])) for n_gram in n_grams: x.append(' '.join(n_gram)) return x text_field=data.Field(tokenize=tokenizer,fix_length=60,include_lengths=True, preprocessing=generate_bigrams) include_lengths,返回的minibatch中包含padded后的text和序列长度，序列长度对于我们处理变长序列有用。这样操作后，在后面的每个batch中，batch.text返回的就是一个（text，text_length）的元组。 preprocessing,添加预处理函数，tokenize后的结果返回的是列表类型数据，将这个列表数据再通过preprocessing指定的函数进行处理。 TabularDataset 针对我们需要处理的表格数据，torch提供了TabularDataset帮助我们按行进行划分，按照表格对应列的顺序指定Field对象构成元组fields=[('text', text_field)，('label', label_field), ] ,label和text是别名，可以任意，我们后面可以通过这个别名获取batch的数据。除此之外还要指定数据的路径，格式，以及是否跳过表头等超参数。 train,dev,test=data.TabularDataset.splits(path,train='trian.tsv',validation='dev.tsv',test='test.tsv',skip_header=True,fields=[('label',label_field),('text',text_field)],format='tsv') 生成的数据格式如下 构建映射和导入词向量 前面将数据按行进行划分了，但是并没有建立词和索引的对应关系，也没有建立索引和词向量的对应关系，下一步就是建立词和索引和向量的关系 import torchtext.vocab as vocab vectors=vocab.Vectors('embedding_path','cachepath') text_field.build_vocab(train,dev,test,vectors,unk_init=torch.Tensor.normal_) label_field.build_vocab(train,dev,test) torchtext.vocab提供了一个缓存词向量的方法，在第一次读取词向量时将词向量缓存到指定目录，下次再读取词向量时直接从缓存目录中读取，这样就不用每次加载词向量了，加快了读取速度。未知词和padding词词语和索引对应关系为：{:0,:1}。其索引可以通过如下 方式获取： PAD_IDX=text_field.vocab.stoi[text_field.pad_token] UNK_IDX=text_field.vocab.stoi[text_field.unk_token] 未知词和padding词通常对我们的任务没有帮助，但unk_init=torch.Tensor.normal_却正太分布初始化了，获取了这两个词的索引可以帮助我们在后面的embedding层将其词向量重置为0 建立text中词和索引、词向量的映射关系时，需要输入要处理的数据、词向量，指定未识别词的初始化方式。 建立label中类别和索引的关系就比较简单了，输入之前生成的数据就好了。看一眼生成的词表和词向量 生成batch 前面我们已经将每行数据进行了分词，建立了词表、索引等对应关系，接下来就是生成batch啦 train_iter,dev_iter,test_iter=data.BucketIterator.splits( (train,dev,test),batch_sizes=(64,len(dev),len(test)), sort_key=lambda x:len(x.text),sort_within_batch,repeat=False, shuffle=True,device=device ) 首先指定要处理的数据（train，dev，test），接着指定各个数据对应要生成 的batch的batch_size大小，注意这里是多个数据集的batch_size,参数是batch_sizes=（64，len（dev),len(test)）,可以看到这里对dev和test我们的batchsize大小就是他们所有数据，所以这些数据只会生成一个batch。通常对训练集和验证集的数据要按数据长度进行排序，我们要指定排序的键sort_key，这里使用了匿名函数，text就是我们之前指定的别名。还包括其他一些参数，不一一介绍。至此我们就生成了text和label的batch集合，两者是合在一起的，我们可以通过batch的别名获得各自值。sort_within_batch,在一个batch内对数据进行排序。 embedding 如何获得词表对应的embedding呢？ pretrained_embeddings=text_field.vocab.vectors 可以和torch的embedding模块结合使用 embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False) embedded=embedding(batch.text) embedding的输入需要是之前处理过的batch数据，torchtext会自动帮我们处理好对应关系。 那么我们如何处理边长序列呢？通常为了数据的通用性，我们对较短的数据会进行padding，让所有序列具有相同的长度。当使用RNN等网络时，经过padding的数据产生的hidden state、outputs对于我们并无帮助，我们主需要真是序列长产生的hidden state和outputs即可。torchtext提供了pack_padded_sequence和pad_packed_sequence两个方法，对embedded的结果进行处理加入序列长度。 embedded=self.embedding(text) #添加了padding的embedding结果 packed_embedded=nn.utils.pack_padded_embedding(embedded,text_lengths) packed_output,(hidden,cell)=self.rnn(packed_output) #没有padding的输出 output,output_lengths=nn.utils.rnn.pad_packed_sequence(packed_output) #返回输出，和没有padding的句子长度 对于unk和pad词我们可以将其词向量初始化为0向量 model.embedding.weight.data[UNK_IDX]=torch.zeros(EMBEDDING_DIM) model.embedding.weight.data[PAD_IDX]=torch.zeros(EMBEDDING_DIM) 其实在调用field.build_coab时会自动将pad和unk初始化为0 还可以换种方式指定embedding embedding=nn.Embedding(vocab_size,embedding_dim,pad_idx=pad_idx) 先指定好embedding的形状，再在模型搭建好后指定embedding embedding=model.embedding.weight.data.copy_(pretrained_embeddings) 收工~","categories":[],"tags":[{"name":"torchtext","slug":"torchtext","permalink":"https://mz2sj.github.io/tags/torchtext/"}]},{"title":"TextCNN","slug":"TextCNN","date":"2020-01-07T11:50:28.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/07/TextCNN/","link":"","permalink":"https://mz2sj.github.io/2020/01/07/TextCNN/","excerpt":"","text":"学了点pytorch，最近在知乎刷到了一个repo主要是介绍文本分类的，感觉还不错，打算用来入门，自己将会比较细致的实现代码，分析代码的结构。天下大事必做于易，天下大事必做于细。寒假来了，自己基础较差，只能多花点时间了。 基础网络 作者自己没有直接调用torch的api，而是在其基础上进行实现。一些模块继承自nn.Module层，并且实现了初始化和前向计算。嘿呀，等我熟练了，我也要自己做一个工具箱。不多说上代码 Conv1d import torch.nn.Functional as F class Conv1d(nn.Module): def __init__(self,in_channels,out_channels,filter_sizes): super(Conv1d,self).__init__() self.convs=nn.ModuleList([ nn.Conv1d(in_channels,out_channels,kernel_size=fs) for fs in filter_sizes ]) self.init_params() def init_params(self): for m in self.convs: nn.init.xavier_uniform_(m.weight.data) nn.init.constant_(m.bias.data,0.1) def forward(self,x): return [F.relu(conv(x))for conv in self.convs] 让我们来细细品味这段代码，继承自 nn.Module 这不用说了，表明这个模块可以和torch自带的模块混用。__init__函数的参数有讲究，除了必传的self外，还传入了一些指定卷积层形状的参数，例如通道数：in_channels、out_channels,还有卷积核集合。这里传入的是卷积核集合，而不是单单一个卷积核需要注意。接下来是继承自父类的初始化，super()第一个参数一般为函数名，第二个是self,后面再接__init__()。在__init__()函数中放置要用到的模块或参数等，不涉及到具体的操作，在初始化层调用了后面定义的参数 初始化函数。 第二部分是初始化层，init_params()。初始化层比较好理解，初始化init层的参数就好。前面要初始化的只有ModuleList中的卷积层，卷积的函数改天专门总结一下。卷积层参数分为weight和bias两部分,bias部分初始化为0。 第三部分是前向传播，forward()。参数有输入x，由于有多个卷积核，将多个卷积核的卷积结果放到列表中。再调用F.relu()activation。不得不说torch.nn.Functional 超好用。 Linear Linear层和Conv1d结构上差不多，不过Linear层的初始化参数将卷积层的channel换成了feature，并且少了卷积核。代码如下： class Linear(nn.Module): def __init__(self,in_features,out_features): super(Linear,self).__init__() self.linear=nn.Linear(in_features=in_features,out_features=out_features) self.init_params() def init_parmas(self): nn.init.kaiming_normal_(self.linear.weight) nn.init.constant_(self,linear.bias,0) def forward(self,x): x=self.linear(x) return x TextCNN 到了最最关键的环节了，TextCNN层: class TextCNN(nn.Module): def __init__(self,embedding_dim,n_filters,filter_sizes,output_dim,dropout, pretrained_embeddings): super(TextCNN,self).__init__() #导入预训练的embedding frezee指定权重是否更新 self.embedding=nn.Embedding.from_pretrained(pretrained_embeddings,freeze=False) #卷积核参数 embedding维数，卷积核个数，卷积核大小 self.convs=Cov1d(embedding_dim,n_filters,filter_sizes) #前向运算 输入维度 预测输出 self.fc=Linear(len(filter_sizes)*fliters,output_dim) self.dropout=nn.Dropout(dropout) def forward(self,x): #text:[sent_len,batch_size] 两个参数分别是行数列数 text,_=x #维度换位 [batch_size,sent_len] text=text.permute(1,0) # [batch_size,sent_len,embedding_len] embedded=self.embedding(text) #[batch_size,emb_dim,sent_len] embedded=embedded.permute(0,2,1) conved=self.convs(embedded) ##[batch_size,n_filters,sent_len-filter_sizes[n]+1] pooled=[F.max_pool1d(conv,conv.shape[2]) for conv in conved] cat=self.dropout(torch.cat(pooled,dim=1)) cat=cat.reshae((cat.shape[0],-1)) return self.fc(cat) TextCNN层中的张量维度变换比较复杂，我是一下子反应不过来的，后面自己会debug彻底弄清楚每一步是怎么卷积的，维度怎么变换的。 argparse 代码还有一个比较值得借鉴的点就是使用了argparse模块对各种参数进行管理，不是一个个手动去定义变量，更加清晰。 import argparse def get_args(data_dir, cache_dir, embedding_folder, model_dir, log_dir): parser = argparse.ArgumentParser(description='SST') # data_util parser.add_argument('--model_name', default='TextCNN', type=str, help='参数所属模型名') parser.add_argument('--seed', default=47, type=int, help='随机种子') parser.add_argument('--data_path', default=data_dir, type=str, help='SST2数据集位置') parser.add_argument('--cache_path', default=cache_dir, type=str, help='数据集地址') parser.add_argument('--sequence_length', default=60, type=int, help='句子长度') # 输出文件名 parser.add_argument('--model_dir', default=model_dir + 'TextCNN/', type=str, help='输出模型地址') parser.add_argument('--log_dir', default=log_dir + 'TextCNN/', type=str, help='日志文件地址') parser.add_argument('--do_train', action='store_true', help='Whether to run training') parser.add_argument('--print_step', default=100, type=int, help='多少步存储一次模型') # 优化参数 parser.add_argument('--batch_size', default=64, type=int) parser.add_argument('--epoch_num', default=5, type=int) parser.add_argument('--dropout', default=0.4, type=float) # 模型参数 parser.add_argument('--output_dim', default=2, type=int) # TextCNN参数 parser.add_argument('--filter_num', default=200, type=int, help='filter数量') parser.add_argument('--filter_sizes', default='1 2 3 4 5', type=str, help='filter的size') # word embdding parser.add_argument('--glove_word_file', default=embedding_folder + 'glove.6B.50d.txt', type=str, help='path of embedding file') parser.add_argument('--glove_words_size', default=int(2.2e6), type=int, help='Corpus size for Glove') parser.add_argument('--glove_word_dim', default=50, type=int, help='word embedding size(default:300)') config = parser.parse_args([]) return config 注意的一点就是parse_args()在notebook中需要加入[]作为参数，否则会报错。 device 指定设备device，这个很容易理解 def get_device(): device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') n_gpu=torch.cuda.device_count() if torch.cuda.is_available(): print('device is cuda,# cuda is:',n_gpu) else: print('device is cpu,not recommend') return device,n_gpu 统计gpu数代码：torch.cuda.device_count() 指定device代码：torch.device('cuda:0') 加载数据集 对api还不太熟悉，这里也比较复杂。 def load_sst2(path,text_field,label_field,batch_size,device,embedding_file,cache_dir): train,dev,test=data.TabularDataset.splits( path=path,train='train.tsv',validationj='dev.tsv', test='test.tsv',format='tsv',skip_header=True, fields=[('text',text_field),('label',label_field)] ) print('the size of train:{},dev:{},test:{}'.format( len(train.examples), len(dev.examples), len(test.examples) )) #将词向量放到缓存目录中，下次直接从换从目录中加载 vectors=vocab.Vectors(embedding_file,cache_dir) #使用之前构建的词向量映射文本序列，将子序列转化为数字序列 text_field.build_vocab( train,dev,test,max_size=25000, vectors=vectors,unk_init=torch.Tensor.normal_ ) label_field.build_vocab(train,dev,test) train_iter,dev_iter,test_iter=data.BucketIterator.splits( (train,dev,test),batch_sizes=(batch_size,len(dev),len(test)), sort_key=lambda x:len(x.text),sort_within_batch=True,repeat=False, shuffle=True,device=device ) return train_iter,dev_iter,test_iter 得总结一下套路，看起来懵懵的 训练 def train(epoch_num,model,train_dataloader,dev_dataloader,optimizer,criterion, label_list,out_model_file,log_dir,print_step,data_type='word'): #首先转为训练模式 model.train() writer=SummaryWriter(log_dir=log_dir+time.strftime('%H%M%S',time.gmtime())) global_step=0 best_dev_loss=float('inf') for epoch in range(int(epoch_num)): print(f'--------------Epoch:{epoch+1:02}--------------') epoch_loss=0 train_step=0 all_preds=np.array([],dtype=int) all_labels=np.array([],dtype=int) for step,batch in enumerate(tqdm(train_dataloader,desc='Iteration')): #首先梯度归零 optimzier.zero_grad() logits=model(batch.text) #计算损失 loss=criterion(logits.view(-1,len(label_list)),batch.label) labels=batch.label.detach().cpu().numpy() preds=np.argmax(logits.detach().cpu().numpy(),axis=1) #计算梯度 loss.backward() optimizer.step() global_step+=1 epoch_loss+=loss.item() train_steps+=1 all_preds=np.append(all_preds,preds) all_labels=np.append(all_labels,labels) #打印 if global_step%print_step==0: train_loss=epoch_loss/train_steps train_acc,train_report=classification_metric( all_preds,all_labels,label_list) dev_loss,dev_acc,dev_report=evaluate( model,dev_dataloader,criterion,label_list,data_type ) c=global_step/print_step writer.add_scalar('loss/train',train_loss,c) writer.add_scalar('loss/dev',dev_loss,c) writer.add_scalar('acc/train',train_acc,c) writer.add_scalar('acc/dev',dev_acc,c) for label in label_list: writer.add_scalar(label+':'+'f1/train', train_report[label]['f1-score'],c) writer.add_scalar(label+':'+'f1/dev', dev_report[label]['f1-score'],c) print_list=['macro avg','weighted avg'] for label in print_list: writer.add_scalar(label+':'+'f1/train', train_report[label]['f1-score'],c) writer.add_scalar(label+':'+'f1/dev', dev_report[label]['f1-score'],c) if dev_loss&lt;best_dev_loss: best_dev_loss=dev_loss torch.save(model.state_dict(),out_model_file) model.train() writer.close() 训练函数有几个点总结，传入参数有：迭代次数、model、训练集、验证集，optimzer、损失函数、打印的epoch数，等等。 evaluate 用于在训练过程中对训练结果进行测试 def evaluate(model,iterator,criterion,label_list,data_type='word'): model.eval() epoch_loss=0 all_preds=np.array([],dtype=int) all_labels=np.array([],dtype=int) with torch.no_grad(): for batch in iterator: if data_type=='word': with torch.no_grad(): logits=model(batch.text) elif data_type=='highway': with torch.no_grad(): logits=model(batch.text_word,batch.text_char) loss=criterion(logits.view(-1,len(label_list)),batch.label) labels=batch.label.detach().cpu().numpy() preds=np.argmax(logits.detach().cpu().numpy(),axis=1) all_preds=np.append(all_preds,preds) all_labels=np.append(all_labels,labels) epoch_loss+=loss.item() acc,report=classification_metric(all_preds,all_labels,label_list) return epoch_loss/len(iterator),acc,report 测试过程中记得要切换到eval模式，并且关闭梯度追踪with torch.no_grad()。 main函数 def main(config): if not os.path.exists(config.model_dir): os.makedirs(config.model_dir) if not os.path.exists(config.log_dir): os.makedirs(config.log_dir) print('\\t \\t \\t the model name is {}'.format(config.model_name)) device, n_gpu = get_device() # 指定torch 随机种子 torch.manual_seed(config.seed) # 指定numpy 随机种子 np.random.seed(config.seed) torch.manual_seed(config.seed) if n_gpu &gt; 0: # 指定cuda随机种子 torch.cuda.manual_seed_all(config.seed) # CuDNN的卷积操作就是每次一样,实验可重复 torch.backends.cudnn.deterministic = True # sst2 数据准备 text_field = data.Field(tokenize='toktok', lower=True, include_lengths=True, fix_length=config.sequence_length) label_field = data.LabelField(dtype=torch.long) train_iterator, dev_iterator, test_iterator = load_sst2(config.data_path, text_field, label_field, config.batch_size, device, config.glove_word_file, config.cache_path) # 词向量准备 获取词向量按index顺序排序 pretrained_embeddings = text_field.vocab.vectors model_file = config.model_dir + 'model1.pt' # 模型准备 卷积核大小 filter_sizes = [int(val) for val in config.filter_sizes.split()] model = TextCNN(config.glove_word_dim, config.filter_num, filter_sizes, config.output_dim, config.dropout, pretrained_embeddings) optimizer = torch.optim.Adam(model.parameters()) criterion = nn.CrossEntropyLoss() model = model.to(device) criterion = criterion.to(device) if not config.do_train: train(config.epoch_num, model, train_iterator, dev_iterator, optimizer, criterion, ['0', '1'], model_file, config.log_dir,config.print_step, 'word') model.load_state_dict(torch.load(model_file)) test_loss, test_acc, test_report = evaluate(model, test_iterator, criterion, ['0', '1'], 'word') print('-----------Test----------') print('\\t Loss:{}|Acc:{}|Macro avg F1{}|Weighted avg F1{}'.format( test_loss, test_acc, test_report['macro avg']['f1-score'], test_report['weighted avg']['f1-score'] )) 太多啦，懒得再记录鸟~下面分析一下卷积的过程吧。 卷积的过程 代码中用了5个卷积核，卷积核的大小分别是[1,2,3,4,5]。一维卷积层的输入是词向量的拼接，本代码中使用的词向量是50维，每个样本有60个词，因此卷积的输入是50x60的矩阵XXX。一维卷积核的输入通道数就是XXX的宽50，输出通道数是卷积核的个数200，则卷积核[1,2,3,4,5]对应的输出分别为[200*x60,200x59,200x58,200x57,200x56]。再分别对行进行最大池化，得到的输出为5个[200x1]的向量，将这5个向量拼接起来成[1000x1]的向量，输入前馈神经网络[1000x2]，即可进行二分类。老实说，一维卷积的过程自己没太看懂。代码中将卷积核的个数作为一维卷积的输出通道数，embedding维度作为卷积的输入通道数。一直以来看到的都是二维卷积，一维卷积有点懵，下面画个草图理解下吧。","categories":[],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://mz2sj.github.io/tags/CNN/"},{"name":"文本分类","slug":"文本分类","permalink":"https://mz2sj.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"colab使用","slug":"colab使用","date":"2020-01-07T05:26:04.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/07/colab使用/","link":"","permalink":"https://mz2sj.github.io/2020/01/07/colab%E4%BD%BF%E7%94%A8/","excerpt":"","text":"最近学习pytorch的使用，自己的电脑没有显卡，一些实验做不了，想起之前本科老师安利的colab，感谢谷歌提供的免费资源。 访问谷歌 colab基于谷歌云盘，既然是谷歌的服务，难免就要翻墙了。自己用的shadowsocks，一年100块左右，大部分时间都挺稳定的。开启小飞机，你就可以看到外面的世界啦！ 云盘挂载 关于colab的基础使用这里就不介绍了，可以自行百度。在使用colab的过程中，经常需要挂载云盘文件，参考了许多人的方法后，介绍一个无坑版。首先，在colab左侧的代码提示部分选择挂载colab会自动生成云盘挂载代码，如下所示： from google.colab import drive drive.mount('/gdrive') 运行第一个单元格的代码，第二个是打开相关文件的代码忽略。运行后会给出一个超链接，点击进去验证得到验证码输入即可挂载成功，这时候我们挂载的位置是在/gdrive/gdrive/gdrive 目录下。那么如何切换到我们想要的目录呢？再次打开左侧的代码提示部分，进入文件栏，点击 ......... 进入上层目录，进入/gdrive/gdrive/gdrive目录，可以看到我们云盘下的目录文件，右击复制文件路径。 再通过魔法函数%cd 路径就%cd filepath 就可以切换到指定的路径，此时colab的工作路径就是你指定的路径，你就可以像在本地一样操作colab的文件。操作非当前文件夹下的文件，指定好文件的路径即可。 %cd /gdrive/My Drive/Colab Notebooks/测试用的 无限容量 colab默认的存储容量是15G，可能放几个大文件就占满了，升级的话又不支持支付宝，而且一年也要100多块。贫穷使我想起了歪门邪道。在淘宝上提供了colab扩容服务，20块就够了，其原理是将你加入一些学校的共享盘群组，你可以在上面无限制上传文件。可能会存在一些安全问题，但平时跑跑程序是完全没问题的。加入贡献盘后，我们可以看到自己的gdrive文件夹下多了一个Shared drives。我们可以在上面进行和原硬盘相同的操作，并且无线空间。厉害啦👍 纪念自己的第一篇博客 既然开了这个博客就要好好写哦，记录自己的学习、生活，努力成为更好的人啊！ 加油 冲","categories":[],"tags":[{"name":"colab Google Drive","slug":"colab-Google-Drive","permalink":"https://mz2sj.github.io/tags/colab-Google-Drive/"}]}]}
{"meta":{"title":"å¾®é£å’Œæš–","subtitle":"","description":"","author":"mz2sj","url":"https://mz2sj.github.io","root":"/"},"pages":[],"posts":[{"title":"Hello World","slug":"hello-world","date":"2023-01-08T06:30:22.504Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2023/01/08/hello-world/","link":"","permalink":"https://mz2sj.github.io/2023/01/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"å†³ç­–æ ‘","slug":"å†³ç­–æ ‘","date":"2021-06-08T12:57:19.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2021/06/08/å†³ç­–æ ‘/","link":"","permalink":"https://mz2sj.github.io/2021/06/08/%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"","text":"å†³ç­–æ ‘å­¦ä¹ é€šå¸¸åŒ…æ‹¬3ä¸ªæ­¥éª¤ï¼šç‰¹å¾é€‰æ‹©ã€å†³ç­–æ ‘çš„ç”Ÿæˆã€å†³ç­–æ ‘çš„å‰ªæã€‚ ID3 ID3é€šè¿‡ä¿¡æ¯å¢ç›Šæ¥é€‰æ‹©ç‰¹å¾å’Œåˆ†è£‚ç‚¹ï¼Œä¿¡æ¯å¢ç›Šç”±ä¿¡æ¯ç†µå’Œæ¡ä»¶ç†µè®¡ç®—å¾—å‡ºã€‚ ç†µçš„å…¬å¼è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š H(D)=âˆ’âˆ‘k=1Kâˆ£Ckâˆ£âˆ£Dâˆ£logâ¡2âˆ£Ckâˆ£âˆ£Dâˆ£\\begin{array}{l} H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log _{2} \\frac{ \\mid C_{k} \\mid}{|D|} \\\\\\end{array}H(D)=âˆ’âˆ‘k=1Kâ€‹âˆ£Dâˆ£âˆ£Ckâ€‹âˆ£â€‹log2â€‹âˆ£Dâˆ£âˆ£Ckâ€‹âˆ£â€‹â€‹ å½“æ ·æœ¬ä¸­å„ç±»åˆ«æ•°æ®æ¦‚ç‡ç›¸ç­‰æ—¶ï¼Œç†µæœ€å¤§ã€‚æ¢ç§æ–¹å¼ç†è§£ï¼Œæ‹¿å‡ºä¸€ä¸ªæ ·æœ¬ï¼Œå®ƒæ˜¯å„ä¸ªç±»åˆ«çš„æ¦‚ç‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ­¤æ—¶æœ€éš¾åˆ†è¾¨æ ·æœ¬çš„ç±»åˆ«ã€‚ç†µè¶Šå¤§æ ·æœ¬çš„ä¸ç¡®å®šæ€§è¶Šé«˜ã€‚ æ¡ä»¶ç†µè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š H(Dâˆ£A)=âˆ‘i=1nâˆ£Diâˆ£âˆ£Dâˆ£H(Di)=âˆ’âˆ‘i=1nâˆ£Diâˆ£âˆ£Dâˆ£âˆ£âˆ‘k=1kâˆ£Diâˆ£âˆ£âˆ£Diâˆ£logâ¡2âˆ£Dikâˆ£âˆ£Diâˆ£H(D \\mid A)=\\sum_{i=1}^{n} \\frac{\\mid D_{i} \\mid}{\\mid D \\mid} H\\left(D_{i}\\right)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D| \\mid} \\sum_{k=1}^{k} \\frac{\\left|D_{i \\mid}\\right|}{\\mid D_{i \\mid}} \\log _{2} \\frac{\\mid D_{i k} \\mid}{\\mid D_{i \\mid}} \\\\H(Dâˆ£A)=âˆ‘i=1nâ€‹âˆ£Dâˆ£âˆ£Diâ€‹âˆ£â€‹H(Diâ€‹)=âˆ’âˆ‘i=1nâ€‹âˆ£Dâˆ£âˆ£âˆ£Diâ€‹âˆ£â€‹âˆ‘k=1kâ€‹âˆ£Diâˆ£â€‹âˆ£Diâˆ£â€‹âˆ£â€‹log2â€‹âˆ£Diâˆ£â€‹âˆ£Dikâ€‹âˆ£â€‹ å…¶ä¸­iiiä»£è¡¨AAAç‰¹å¾æœ‰å‡ ä¸ªç±»åˆ«ï¼Œå³Aç‰¹å¾çš„åŸºæ•°ã€‚æ¡ä»¶ç†µæ˜¯åœ¨æŸç‰¹å¾å„ä¸ªç±»åˆ«å€¼å‡ºç°æ¦‚ç‡çš„åŸºç¡€ä¸Šè®¡ç®—ç†µã€‚ ç†µå‡å»æ¡ä»¶ç†µå°±æ˜¯æŸä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›Šï¼š $ g(D, A)=H(D)-H(D \\mid A)$ ID3ç®—æ³•é€šè¿‡ä¿¡æ¯å¢ç›Šæ¥é€‰æ‹©ç‰¹å¾ï¼šä¿¡æ¯å¢ç›Šå¤§çš„ç‰¹å¾ å…·æœ‰æ›´å¼ºçš„åˆ†ç±»èƒ½åŠ›ã€‚å¦‚æœä¸€ä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›Šä¸º0ï¼Œå³è¡¨ç¤ºè¯¥ç‰¹å¾æ²¡æœ‰ä»€ä¹ˆåˆ†ç±»èƒ½åŠ›ã€‚ ID3ç®—æ³•æ­¥éª¤ç®€åŒ–ç†è§£ï¼š 1.è®¡ç®—å„ä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›ŠAgA_gAgâ€‹,é€‰æ‹©æœ€å¤§çš„AgA_gAgâ€‹å€¼å¯¹åº”çš„ç‰¹å¾ã€‚ 2.å¦‚æœAgA_gAgâ€‹å°äºé˜ˆå€¼Ïµ\\epsilonÏµ,åˆ™è¯¥ç‰¹å¾åˆ†ç±»ç‚¹ä¸‹é¢æ‰€æœ‰æ ·æœ¬å®ä¾‹æ•°æœ€å¤šçš„ç±»åˆ«å°±æ˜¯è¯¥èŠ‚ç‚¹çš„ç±»æ ‡è®°ã€‚ 3.å¦åˆ™ï¼Œå¯¹äºAgA_gAgâ€‹ä¸­çš„å„ä¸ªç±»åˆ«å€¼ï¼Œåˆ†è£‚æˆå„ä¸ªå­èŠ‚ç‚¹ã€‚ 4.è¿­ä»£è¿›è¡Œä¸Šé¢3æ­¥ï¼Œç›´åˆ°ä¿¡æ¯å¢ç›Šå‡å°äºé˜ˆå€¼Ïµ\\epsilonÏµæˆ–è€…æ²¡æœ‰ç‰¹å¾å¯ä»¥é€‰æ‹©ä¸ºæ­¢ã€‚ ç¼ºç‚¹ï¼š 1.ID3æ²¡æœ‰å‰ªææ“ä½œï¼Œåªæœ‰è¾“çš„ç”Ÿæˆï¼Œæ‰€ä»¥å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ 2.ä¿¡æ¯å¢ç›Šå‡†åˆ™å¯¹å¯å–å€¼æ•°ç›®è¾ƒå¤šçš„å±æ€§æœ‰æ‰€åå¥½ã€‚ 3.æ²¡æœ‰è€ƒè™‘ç¼ºå¤±å€¼çš„å¤„ç†ã€‚ C4.5 C4.5ä¸ºäº†å…‹æœID3å¯¹é«˜åŸºæ•°ç‰¹å¾æœ‰æ‰€åå¥½çš„ç‰¹ç‚¹ï¼Œå¼•å…¥äº†ä¿¡æ¯å¢ç›Šæ¯”æ¥é€‰æ‹©ç‰¹å¾ï¼Œä¿¡æ¯å¢ç›Šæ¯”è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š gR(D,A)=g(D,A)HA(D) g_{R}(D, A)=\\frac{g(D, A)}{H_{A}(D)} gRâ€‹(D,A)=HAâ€‹(D)g(D,A)â€‹ HA(D)=âˆ’âˆ‘i=1nâˆ£Diâˆ£âˆ£Dâˆ£logâ¡2âˆ£Diâˆ£âˆ£Dâˆ£H_{A}(D)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} \\log _{2} \\frac{\\left|D_{i}\\right|}{|D|}HAâ€‹(D)=âˆ’âˆ‘i=1nâ€‹âˆ£Dâˆ£âˆ£Diâ€‹âˆ£â€‹log2â€‹âˆ£Dâˆ£âˆ£Diâ€‹âˆ£â€‹ ä¿¡æ¯å¢ç›Šæ¯”å…¶å®å°±æ˜¯æŸä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›Šä¸å°†æŸä¸ªç‰¹å¾ç±»åˆ«çš„ä¿¡æ¯ç†µçš„æ¯”ï¼Œä¿¡æ¯å¢ç›Šæ¯”å€¾å‘äºåŸºæ•°ä½çš„ç‰¹å¾ã€‚ C4.5çš„å†³ç­–æ ‘ç”Ÿæˆä¸ID3ç›¸åŒï¼Œå¼•å…¥äº†å‰ªæç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„å‰ªæå’Œåå‰ªæã€‚ é¢„å‰ªæ åœ¨èŠ‚ç‚¹åˆ’åˆ†å‰æ¥ç¡®å®šæ˜¯å¦ç»§ç»­å¢é•¿ï¼ŒåŠæ—©åœæ­¢å¢é•¿çš„ä¸»è¦æ–¹æ³•æœ‰ï¼š èŠ‚ç‚¹å†…æ•°æ®æ ·æœ¬ä½äºæŸä¸€é˜ˆå€¼ï¼› æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾éƒ½å·²åˆ†è£‚ï¼› èŠ‚ç‚¹åˆ’åˆ†å‰å‡†ç¡®ç‡æ¯”åˆ’åˆ†åå‡†ç¡®ç‡é«˜ã€‚ é¢„å‰ªæå¯èƒ½ä¼šå¸¦æ¥æ¬ æ‹Ÿåˆé—®é¢˜ã€‚ åå‰ªæ C4.5 é‡‡ç”¨çš„æ‚²è§‚å‰ªææ–¹æ³•ï¼Œç”¨é€’å½’çš„æ–¹å¼ä»ä½å¾€ä¸Šé’ˆå¯¹æ¯ä¸€ä¸ªéå¶å­èŠ‚ç‚¹ï¼Œè¯„ä¼°ç”¨ä¸€ä¸ªæœ€ä½³å¶å­èŠ‚ç‚¹å»ä»£æ›¿è¿™è¯¾å­æ ‘æ˜¯å¦æœ‰ç›Šã€‚å¦‚æœå‰ªæåä¸å‰ªæå‰ç›¸æ¯”å…¶é”™è¯¯ç‡æ˜¯ä¿æŒæˆ–è€…ä¸‹é™ï¼Œåˆ™è¿™æ£µå­æ ‘å°±å¯ä»¥è¢«æ›¿æ¢æ‰ã€‚ C4.5 é€šè¿‡è®­ç»ƒæ•°æ®é›†ä¸Šçš„é”™è¯¯åˆ†ç±»æ•°é‡æ¥ä¼°ç®—æœªçŸ¥æ ·æœ¬ä¸Šçš„é”™è¯¯ç‡ã€‚ ç¼ºç‚¹ï¼š å‰ªæç­–ç•¥å¯ä»¥å†ä¼˜åŒ–ï¼› C4.5 ç”¨çš„æ˜¯å¤šå‰æ ‘ï¼Œç”¨äºŒå‰æ ‘æ•ˆç‡æ›´é«˜ï¼› C4.5 åªèƒ½ç”¨äºåˆ†ç±»ï¼› C4.5 ä½¿ç”¨çš„ç†µæ¨¡å‹æ‹¥æœ‰å¤§é‡è€—æ—¶çš„å¯¹æ•°è¿ç®—ï¼Œè¿ç»­å€¼è¿˜æœ‰æ’åºè¿ç®—ï¼› C4.5 åœ¨æ„é€ æ ‘çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹æ•°å€¼å±æ€§å€¼éœ€è¦æŒ‰ç…§å…¶å¤§å°è¿›è¡Œæ’åºï¼Œä»ä¸­é€‰æ‹©ä¸€ä¸ªåˆ†å‰²ç‚¹ï¼Œæ‰€ä»¥åªé€‚åˆäºèƒ½å¤Ÿé©»ç•™äºå†…å­˜çš„æ•°æ®é›†ï¼Œå½“è®­ç»ƒé›†å¤§å¾—æ— æ³•åœ¨å†…å­˜å®¹çº³æ—¶ï¼Œç¨‹åºæ— æ³•è¿è¡Œ ç¼ºç‚¹è¿˜è¦å†è¿›è¡Œç†è§£ã€‚ CART CARTå›å½’æ ‘ å›å½’æ ‘è¡¨ç¤º ä¸€æ£µ CART å›å½’æ ‘å¯¹åº”ç€è¾“å…¥ç©ºé—´çš„ä¸€ä¸ªåˆ’åˆ†ï¼Œä»¥åŠåœ¨åˆ’åˆ†å•å…ƒä¸Šçš„è¾“å‡ºå€¼ã€‚ è®¾è¾“å‡º y ä¸ºè¿ç»­å˜é‡, è®­ç»ƒæ•°æ®é›† $ \\mathbb{D}=\\left{\\left(\\overrightarrow{\\mathbf{x}}{1}, \\tilde{y}{1}\\right),\\left(\\overrightarrow{\\mathbf{x}}{2}, \\tilde{y}{2}\\right), \\cdots,\\left(\\overrightarrow{\\mathbf{x}}{N}, \\tilde{y}{N}\\right)\\right} $ è®¾å·²ç»å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸º M ä¸ªå•å…ƒ $ R_{1}, R_{2}, \\cdots, R_{M} $, ä¸”åœ¨æ¯ä¸ªå•å…ƒ R_{m} ä¸Šæœ‰ä¸€ä¸ªå›ºå®šçš„è¾“å‡ºå€¼ cmc_{m}cmâ€‹ åˆ™ CART å›å½’æ ‘æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º: f(xâ†’)=âˆ‘m=1McmI(xâ†’âˆˆRm)f(\\overrightarrow{\\mathbf{x}})=\\sum_{m=1}^{M} c_{m} I\\left(\\overrightarrow{\\mathbf{x}} \\in R_{m}\\right)f(x)=âˆ‘m=1Mâ€‹cmâ€‹I(xâˆˆRmâ€‹) å…¶ä¸­$ I(\\cdot) $ ä¸ºç¤ºæ€§å‡½æ•°ã€‚ å›å½’æ ‘æŸå¤±å‡½æ•° å¦‚æœå·²çŸ¥è¾“å…¥ç©ºé—´çš„å•å…ƒåˆ’åˆ†ï¼ŒåŸºäºå¹³æ–¹è¯¯å·®æœ€å°çš„å‡†åˆ™ï¼Œåˆ™ CART å›å½’æ ‘åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„æŸå¤±å‡½æ•°ä¸º: âˆ‘m=1Mâˆ‘xâ†’iâˆˆRm(y~iâˆ’cm)2\\sum_{m=1}^{M} \\sum_{\\overrightarrow{\\mathbf{x}}_{i} \\in R_{m}}\\left(\\tilde{y}_{i}-c_{m}\\right)^{2}âˆ‘m=1Mâ€‹âˆ‘xiâ€‹âˆˆRmâ€‹â€‹(y~â€‹iâ€‹âˆ’cmâ€‹)2 æ ¹æ®æŸå¤±å‡½æ•°æœ€å°, åˆ™å¯ä»¥æ±‚è§£å‡ºæ¯ä¸ªå•å…ƒä¸Šçš„æœ€ä¼˜è¾“å‡ºå€¼$ \\hat{c}{m} $ ä¸º ğŸ˜’ R{m} $ä¸Šæ‰€æœ‰è¾“å…¥æ ·æœ¬ $\\overrightarrow{\\mathbf{x}}{i} $ å¯¹åº”çš„è¾“å‡º $ \\tilde{y}{i} $ çš„å¹³å‡å€¼ã€‚ å³: $\\hat{c}{m}=\\frac{1}{N{m}} \\sum_{\\overrightarrow{\\mathbf{x}}{i} \\in R{m}} \\tilde{y}{i} $, å…¶ä¸­ $N{m} $ è¡¨ç¤ºå•å…ƒ $R_{m} $ ä¸­çš„æ ·æœ¬æ•°é‡ã€‚ å¦‚ä½•é€‰æ‹©åˆ‡åˆ†ç‚¹è¿›è¡Œåˆ’åˆ† è®¾è¾“å…¥ä¸º nnn ç»´:$ \\overrightarrow{\\mathbf{x}}=\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)^{T}$ ã€‚ 1.é€‰æ‹©ç¬¬ jjj ç»´ $ x_{j} $ å’Œå®ƒçš„å–å€¼ sss ä½œä¸ºåˆ‡åˆ†å˜é‡å’Œåˆ‡åˆ†ç‚¹ã€‚å®šä¹‰ä¸¤ä¸ªåŒºåŸŸ: R1(j,s)={xâ†’âˆ£xjâ‰¤s}R2(j,s)={xâ†’âˆ£xj&gt;s}\\begin{array}{l} R_{1}(j, s)=\\left\\{\\overrightarrow{\\mathbf{x}} \\mid x_{j} \\leq s\\right\\} \\\\ R_{2}(j, s)=\\left\\{\\overrightarrow{\\mathbf{x}} \\mid x_{j}&gt;s\\right\\} \\end{array}R1â€‹(j,s)={xâˆ£xjâ€‹â‰¤s}R2â€‹(j,s)={xâˆ£xjâ€‹&gt;s}â€‹ 2.ç„¶åå¯»æ±‚æœ€ä¼˜åˆ‡åˆ†å˜é‡ jjj å’Œæœ€ä¼˜åˆ‡åˆ†ç‚¹ sss ã€‚å³æ±‚è§£: (jâˆ—,sâˆ—)=minâ¡j,s[minâ¡c1âˆ‘xâƒ—iâˆˆR1(j,s)(y~iâˆ’c1)2+minâ¡c2âˆ‘xâƒ—iâˆˆR2(j,s)(y~iâˆ’c2)2]\\left(j^{*}, s^{*}\\right)=\\min _{j, s}\\left[\\min _{c_{1}} \\sum_{\\vec{x}_{i} \\in R_{1}(j, s)}\\left(\\tilde{y}_{i}-c_{1}\\right)^{2}+\\min _{c_{2}} \\sum_{\\vec{x}_{i} \\in R_{2}(j, s)}\\left(\\tilde{y}_{i}-c_{2}\\right)^{2}\\right](jâˆ—,sâˆ—)=minj,sâ€‹[minc1â€‹â€‹âˆ‘xiâ€‹âˆˆR1â€‹(j,s)â€‹(y~â€‹iâ€‹âˆ’c1â€‹)2+minc2â€‹â€‹âˆ‘xiâ€‹âˆˆR2â€‹(j,s)â€‹(y~â€‹iâ€‹âˆ’c2â€‹)2] å…¶æ„ä¹‰ä¸ºï¼š é¦–å…ˆå‡è®¾å·²çŸ¥åˆ‡åˆ†å˜é‡ jjj , åˆ™éå†æœ€ä¼˜åˆ‡åˆ†ç‚¹ s1s_{1}s1â€‹ åˆ™åˆ°: c^1=1N1âˆ‘xâ†’iâˆˆR1(j,s)y~i,c^2=1N2âˆ‘xâƒ—iâˆˆR2(j,s)y~i\\hat{c}_{1}=\\frac{1}{N_{1}} \\sum_{\\overrightarrow{\\mathbf{x}}_{i} \\in R_{1}(j, s)} \\tilde{y}_{i}, \\quad \\hat{c}_{2}=\\frac{1}{N_{2}} \\sum_{\\vec{x}_{i} \\in R_{2}(j, s)} \\tilde{y}_{i}c^1â€‹=N1â€‹1â€‹âˆ‘xiâ€‹âˆˆR1â€‹(j,s)â€‹y~â€‹iâ€‹,c^2â€‹=N2â€‹1â€‹âˆ‘xiâ€‹âˆˆR2â€‹(j,s)â€‹y~â€‹iâ€‹ å…¶ä¸­ N1N_{1}N1â€‹ å’Œ $ N_{2} $ åˆ†åˆ«ä»£è¡¨åŒºåŸŸ R1R_{1}R1â€‹ å’Œ $ R_{2}$ ä¸­çš„æ ·æœ¬æ•°é‡ã€‚ ç„¶åéå†æ‰€æœ‰çš„ç‰¹å¾ç»´åº¦, å¯¹æ¯ä¸ªç»´åº¦æ‰¾åˆ°æœ€ä¼˜åˆ‡åˆ†ç‚¹ã€‚ä»è¿™äº› (åˆ‡åˆ†ç»´åº¦,æœ€ä¼˜åˆ‡åˆ†ç‚¹) ä¸­æ‰¾åˆ°ä½¿ä½†æŸå¤±å‡½æ•°æœ€å°çš„é‚£ä¸ªã€‚ 3.ä¾æ¬¡å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸï¼Œç„¶åé‡å¤å¯¹å­åŒºåŸŸåˆ’åˆ†ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ä¸ºæ­¢ã€‚è¿™æ ·çš„å›å½’æ ‘ç§°ä¸ºæœ€å°äºŒä¹˜å›å½’æ ‘ã€‚ CARTåˆ†ç±»æ ‘ç”Ÿæˆ cartåˆ†ç±»æ ‘é‡‡ç”¨åŸºå°¼æŒ‡æ•°é€‰æ‹©æœ€ä¼˜ç‰¹å¾ã€‚ Giniâ¡(p)=âˆ‘k=1Kpk(1âˆ’pk)=1âˆ’âˆ‘k=1Kpk2\\operatorname{Gini}(p)=\\sum_{k=1}^{K} p_{k}\\left(1-p_{k}\\right)=1-\\sum_{k=1}^{K} p_{k}^{2}Gini(p)=âˆ‘k=1Kâ€‹pkâ€‹(1âˆ’pkâ€‹)=1âˆ’âˆ‘k=1Kâ€‹pk2â€‹ å¯¹äºäºŒåˆ†ç±»é—®é¢˜,åŸºå°¼æŒ‡æ•°ä¸º Gini(p)=p(1âˆ’p)+(1âˆ’p)(1âˆ’(1âˆ’p))=2p(1âˆ’p)Gini(p)=p(1-p)+(1-p)(1-(1-p))=2p(1-p)Gini(p)=p(1âˆ’p)+(1âˆ’p)(1âˆ’(1âˆ’p))=2p(1âˆ’p) å¦‚æœæ ·æœ¬é›†åˆDæ ¹æ®ç‰¹å¾Açš„æŸä¸€å€¼aåˆ†ä¸ºD1D_1D1â€‹å’ŒD2D_2D2â€‹ä¸¤éƒ¨åˆ†ï¼Œåˆ™åœ¨ç‰¹å¾Açš„æ¡ä»¶ä¸‹ï¼ŒåŸºå°¼æŒ‡æ•°å®šä¹‰ä¸º Giniâ¡(Dâˆ£A)=âˆ£D1âˆ£âˆ£Dâˆ£Giniâ¡(D1)+âˆ£D2âˆ£âˆ£Dâˆ£Giniâ¡(D2)\\operatorname{Gini}(D \\mid A)=\\frac{\\left|D_{1}\\right|}{|D|} \\operatorname{Gini}\\left(D_{1}\\right)+\\frac{\\left|D_{2}\\right|}{|D|} \\operatorname{Gini}\\left(D_{2}\\right)Gini(Dâˆ£A)=âˆ£Dâˆ£âˆ£D1â€‹âˆ£â€‹Gini(D1â€‹)+âˆ£Dâˆ£âˆ£D2â€‹âˆ£â€‹Gini(D2â€‹) cartåˆ†ç±»æ ‘ç”Ÿæˆè¿‡ç¨‹ä¸å›å½’æ ‘ç±»ä¼¼ï¼Œ éå†æ‰€æœ‰å¯èƒ½çš„ç»´åº¦jjj å’Œè¯¥ç»´åº¦æ‰€æœ‰å¯èƒ½çš„å–å€¼ sssï¼Œå–ä½¿å¾—åŸºå°¼ç³»æ•°æœ€å°çš„é‚£ä¸ªç»´åº¦ jjjå’Œåˆ‡åˆ†ç‚¹sss ã€‚ åˆ’åˆ†åŒºåŸŸä¸­æ ·æœ¬æ¯”ä¾‹é«˜çš„ç±»åˆ«å³æ˜¯è¯¥æ ·æœ¬åŒºåŸŸå¯¹åº”çš„ç±»åˆ«ã€‚ cart åˆ†ç±»æ ‘å’Œcart å›å½’æ ‘é€šå¸¸çš„åœæ­¢æ¡ä»¶ä¸ºï¼š ç»“ç‚¹ä¸­æ ·æœ¬ä¸ªæ•°å°äºé¢„å®šå€¼ï¼Œè¿™è¡¨ç¤ºæ ‘å·²ç»å¤ªå¤æ‚ã€‚ æ ·æœ¬é›†çš„æŸå¤±å‡½æ•°æˆ–è€…åŸºå°¼æŒ‡æ•°å°äºé¢„å®šå€¼ï¼Œè¡¨ç¤ºç»“ç‚¹å·²ç»éå¸¸çº¯å‡€ã€‚ æ²¡æœ‰æ›´å¤šçš„ç‰¹å¾å¯ä¾›åˆ‡åˆ†ã€‚ CARTå‰ªæ CART æ ‘çš„å‰ªææ˜¯ä»å®Œå…¨ç”Ÿé•¿çš„CART æ ‘åº•ç«¯å‡å»ä¸€äº›å­æ ‘ï¼Œä½¿å¾—CART æ ‘å˜å°ï¼ˆå³æ¨¡å‹å˜ç®€å•ï¼‰ï¼Œä»è€Œä½¿å¾—å®ƒå¯¹æœªçŸ¥æ•°æ®æœ‰æ›´å¥½çš„é¢„æµ‹èƒ½åŠ›ã€‚CARTå‰ªæç®—æ³•åˆ†ä¸ºä¸¤æ­¥ï¼šé¦–å…ˆä»ç”Ÿæˆç®—æ³•çš„å†³ç­–æ ‘T0T_0T0â€‹ä¸ç®—å‰ªæï¼Œå½¢æˆä¸€ä¸ªå­æ ‘åºåˆ—T0,T1,...,Tn{T_0,T_1,...,T_n}T0â€‹,T1â€‹,...,Tnâ€‹,ç„¶åé€šè¿‡äº¤å‰éªŒè¯æ³•åœ¨ç‹¬ç«‹çš„éªŒè¯æ•°æ®é›†ä¸Šå¯¹å­æ ‘åºåˆ—è¿›è¡Œé¢„æµ‹ï¼Œä»ä¸­é€‰æ‹©æœ€ä¼˜å­æ ‘ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"ç®—æ³•","slug":"æ•°æ®åˆ†æ/ç®—æ³•","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"æœºå™¨å­¦ä¹ ","slug":"æœºå™¨å­¦ä¹ ","permalink":"https://mz2sj.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Regression","slug":"Regression","date":"2021-04-06T13:32:09.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2021/04/06/Regression/","link":"","permalink":"https://mz2sj.github.io/2021/04/06/Regression/","excerpt":"","text":"emâ€¦,æˆ‘åˆæ¥ç«‹flagäº†ï¼Œä¹‹å‰ä¹°äº†ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹è¿™æœ¬ä¹¦ï¼Œæœ¬ä»¥ä¸ºä¼šå‘å¥‹è¯»ä¹¦ï¼Œæ²¡æƒ³åˆ°ä¹Ÿæ˜¯åŠé€”è€ŒåºŸã€‚ç°åœ¨æƒ³æƒ³è¿˜æ˜¯å¾—å­¦ç‚¹ç®—æ³•ç±»çš„çŸ¥è¯†ï¼Œ æ±‚æ±‚ä½ äº†ï¼Œå­Ÿé•‡ï¼Œåšä¸ªäººå§ï¼Œèƒ½å†™å‡ºæ¥å˜›ï¼Ÿ æ±‚æ±‚ä½ ï¼ŒæŠ„åˆ«äººçš„ä¹ŸæŠ„å‡ºæ¥å§ï¼ çº¿æ€§å›å½’ æ•°æ®é›†ï¼š å‡è®¾æ•°æ®é›†ä¸º: D={(x1,y1),(x2,y2),â‹¯ ,(xN,yN)}\\mathcal{D}=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}D={(x1â€‹,y1â€‹),(x2â€‹,y2â€‹),â‹¯,(xNâ€‹,yNâ€‹)} åé¢æˆ‘ä»¬è®°: X=(x1,x2,â‹¯ ,xN)T,Y=(y1,y2,â‹¯ ,yN)TX=\\left(x_{1}, x_{2}, \\cdots, x_{N}\\right)^{T}, Y=\\left(y_{1}, y_{2}, \\cdots, y_{N}\\right)^{T}X=(x1â€‹,x2â€‹,â‹¯,xNâ€‹)T,Y=(y1â€‹,y2â€‹,â‹¯,yNâ€‹)T çº¿æ€§å›å½’å‡è®¾: f(w)=wTxf(w)=w^{T} xf(w)=wTx æœ€å°äºŒä¹˜æ³• å¯¹è¿™ä¸ªé—®é¢˜, é‡‡ç”¨äºŒèŒƒæ•°å®šä¹‰çš„å¹³æ–¹è¯¯å·®æ¥å®šä¹‰æŸå¤±å‡½æ•°: L(w)=âˆ‘i=1Nâˆ¥wTxiâˆ’yiâˆ¥22L(w)=\\sum_{i=1}^{N}\\left\\|w^{T} x_{i}-y_{i}\\right\\|_{2}^{2}L(w)=âˆ‘i=1Nâ€‹âˆ¥âˆ¥â€‹wTxiâ€‹âˆ’yiâ€‹âˆ¥âˆ¥â€‹22â€‹ å±•å¼€å¾—åˆ°ï¼š L(w)=(wTx1âˆ’y1,â‹¯ ,wTxNâˆ’yN)â‹…(wTx1âˆ’y1,â‹¯ ,wTxNâˆ’yN)T=(wTXTâˆ’YT)â‹…(Xwâˆ’Y)=wTXTXwâˆ’YTXwâˆ’wTXTY+YTY=wTXTXwâˆ’2wTXTY+YTY\\begin{aligned} L(w) &amp;=\\left(w^{T} x_{1}-y_{1}, \\cdots, w^{T} x_{N}-y_{N}\\right) \\cdot\\left(w^{T} x_{1}-y_{1}, \\cdots, w^{T} x_{N}-y_{N}\\right)^{T} \\\\ &amp;=\\left(w^{T} X^{T}-Y^{T}\\right) \\cdot(X w-Y)=w^{T} X^{T} X w-Y^{T} X w-w^{T} X^{T} Y+Y^{T} Y \\\\ &amp;=w^{T} X^{T} X w-2 w^{T} X^{T} Y+Y^{T} Y \\end{aligned}L(w)â€‹=(wTx1â€‹âˆ’y1â€‹,â‹¯,wTxNâ€‹âˆ’yNâ€‹)â‹…(wTx1â€‹âˆ’y1â€‹,â‹¯,wTxNâ€‹âˆ’yNâ€‹)T=(wTXTâˆ’YT)â‹…(Xwâˆ’Y)=wTXTXwâˆ’YTXwâˆ’wTXTY+YTY=wTXTXwâˆ’2wTXTY+YTYâ€‹ æœ€å°åŒ–è¿™ä¸ªå€¼çš„ $\\hat{w} $,è¿™é‡Œæ³¨æ„å¯¹ç”¨å˜é‡çš„æ±‚å¯¼æƒ…å†µï¼š w^=argminâ¡wL(w)âŸ¶âˆ‚âˆ‚wL(w)=0âŸ¶2XTXw^âˆ’2XTY=0âŸ¶w^=(XTX)âˆ’1XTY=X+Y\\begin{aligned} \\hat{w}=\\operatorname{argmin}_{w} L(w) &amp; \\longrightarrow \\frac{\\partial}{\\partial w} L(w)=0 \\\\ &amp; \\longrightarrow 2 X^{T} X \\hat{w}-2 X^{T} Y=0 \\\\ &amp; \\longrightarrow \\hat{w}=\\left(X^{T} X\\right)^{-1} X^{T} Y=X^{+} Y \\end{aligned}w^=argminwâ€‹L(w)â€‹âŸ¶âˆ‚wâˆ‚â€‹L(w)=0âŸ¶2XTXw^âˆ’2XTY=0âŸ¶w^=(XTX)âˆ’1XTY=X+Yâ€‹ è¿™ä¸ªå¼å­ä¸­ $\\left(X^{T} X\\right)^{-1} X^{T} $ åˆè¢«ç§°ä¸ºä¼ªé€†ã€‚å¯¹äºè¡Œæ»¡ç§©æˆ–è€…åˆ—æ»¡ç§©çš„ X , å¯ä»¥ç›´æ¥æ±‚è§£, ä½†æ˜¯å¯¹äºéæ»¡ç§©çš„æ ·æœ¬é›†åˆï¼Œéœ€è¦ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ (SVD) çš„æ–¹æ³•, å¯¹ X æ±‚å¥‡å¼‚å€¼åˆ†è§£, å¾—åˆ° X=UÎ£VTX=U \\Sigma V^{T}X=UÎ£VT W^=(XâŠ¤x)âˆ’1XâŠ¤Y=((UÎ£VâŠ¤)âŠ¤(UÎ£VâŠ¤))âˆ’1(UÎ£V)âŠ¤Y=(VÎ£âŠ¤UâŠ¤UÎ£VâŠ¤)âˆ’1VÎ£âŠ¤UâŠ¤Y=(VÎ£TâŠ¤Î£VâŠ¤)âŠ¤VÎµâŠ¤UâŠ¤Y=(VâŠ¤)âˆ’1Î£âˆ’1(Î£âŠ¤)âˆ’1Vâˆ’1VÎ£âŠ¤UâŠ¤Y=Vâˆ‘âˆ’1(Î£âŠ¤)âˆ’1Î£âŠ¤UâŠ¤Y=VÎ£âˆ’1UâŠ¤Y\\begin{aligned} \\hat{W}=\\left(X^{\\top} x\\right)^{-1} X^{\\top} Y &amp;=\\left(\\left(U \\Sigma V^{\\top}\\right)^{\\top}\\left(U \\Sigma V^{\\top}\\right)\\right)^{-1}\\left(U \\Sigma V\\right)^{\\top} Y \\\\ &amp;=\\left(V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top}\\right)^{-1} V \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=\\left(V_{\\Sigma} T^{\\top} \\Sigma V^{\\top}\\right)^{\\top} V \\varepsilon^{\\top} U^{\\top} Y \\\\ &amp;=\\left(V^{\\top}\\right)^{-1} \\Sigma^{-1}\\left(\\Sigma^{\\top}\\right)^{-1} V^{-1} V \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=V \\sum^{-1}\\left(\\Sigma^{\\top}\\right)^{-1} \\Sigma^{\\top} U^{\\top} Y \\\\ &amp;=V \\Sigma^{-1} U^{\\top} Y \\end{aligned}W^=(XâŠ¤x)âˆ’1XâŠ¤Yâ€‹=((UÎ£VâŠ¤)âŠ¤(UÎ£VâŠ¤))âˆ’1(UÎ£V)âŠ¤Y=(VÎ£âŠ¤UâŠ¤UÎ£VâŠ¤)âˆ’1VÎ£âŠ¤UâŠ¤Y=(VÎ£â€‹TâŠ¤Î£VâŠ¤)âŠ¤VÎµâŠ¤UâŠ¤Y=(VâŠ¤)âˆ’1Î£âˆ’1(Î£âŠ¤)âˆ’1Vâˆ’1VÎ£âŠ¤UâŠ¤Y=Vâˆ‘âˆ’1â€‹(Î£âŠ¤)âˆ’1Î£âŠ¤UâŠ¤Y=VÎ£âˆ’1UâŠ¤Yâ€‹ äºæ˜¯: X+=VÎ£âˆ’1UTX^{+}=V \\Sigma^{-1} U^{T}X+=VÎ£âˆ’1UT å‡ ä½•æƒ³è±¡æ³• æˆ‘ä»¬é€šè¿‡å¯¹æ ·æœ¬XXXçš„ç»„åˆï¼Œä¹Ÿå°±æ˜¯x1,x2,...,xnx_1,x_2,...,x_nx1â€‹,x2â€‹,...,xnâ€‹è·å¾—é¢„æµ‹å€¼, Y=(y1,y2,â‹¯ ,yN)TY=\\left(y_{1}, y_{2}, \\cdots, y_{N}\\right)^{T}Y=(y1â€‹,y2â€‹,â‹¯,yNâ€‹)Tæ˜¯nâˆ—1n*1nâˆ—1çš„å‘é‡ï¼Œåˆ™æˆ‘ä»¬å¸Œæœ›å­˜åœ¨ä¸€ä¸ªå‘é‡mâˆ—1m*1mâˆ—1çš„å‘é‡Î²\\betaÎ² ï¼Œä½¿å¾—XTÎ²X^{T}\\betaXTÎ²ä¸YYYä¹‹é—´çš„è·ç¦»è¶Šå°è¶Šå¥½ï¼Œé‚£ä¹ˆXTÎ²X^{T}\\betaXTÎ²ä¸YYYä¹‹é—´çš„å·®å°±ä¸XTX^{T}XTçš„åŸºæœ¬ç©ºé—´å‚ç›´ï¼Œå³ï¼š XTâ‹…(Yâˆ’XÎ²)=0âŸ¶Î²=(XTX)âˆ’1XTYX^{T} \\cdot(Y-X \\beta)=0 \\longrightarrow \\beta=\\left(X^{T} X\\right)^{-1} X^{T} YXTâ‹…(Yâˆ’XÎ²)=0âŸ¶Î²=(XTX)âˆ’1XTY å™ªå£°å…ˆéªŒé«˜æ–¯åˆ†å¸ƒçš„MLE å¯¹äºä¸€ç»´çš„æƒ…å†µï¼Œè®° y=wTx+Ïµy=w^{T} x+\\epsilony=wTx+Ïµ, Ïµâˆ¼N(0,Ïƒ2)\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)Ïµâˆ¼N(0,Ïƒ2) , é‚£ä¹ˆ $ y \\sim \\mathcal{N}\\left(w^{T} x, \\sigma^{2}\\right)$ ã€‚ä»£å…¥æå¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼š L(w)=logâ¡p(Yâˆ£X,w)=logâ¡âˆi=1Np(yiâˆ£xi,w)=âˆ‘i=1Nlogâ¡(12Ï€Ïƒeâˆ’(yiâˆ’uTxi)22Ïƒ2)argmaxâ¡wL(w)=argminâ¡wâˆ‘i=1N(yiâˆ’wTxi)2\\begin{aligned} L(w)=\\log p(Y \\mid X, w) &amp;=\\log \\prod_{i=1}^{N} p\\left(y_{i} \\mid x_{i}, w\\right) \\\\ &amp;=\\sum_{i=1}^{N} \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma}} e^{-\\frac{\\left(y_{i}-u^{T} x_{i}\\right)^{2}}{2 \\sigma^{2}}}\\right) \\\\ \\underset{w}{\\operatorname{argmax}} L(w) &amp;=\\underset{w}{\\operatorname{argmin}} \\sum_{i=1^{N}}\\left(y_{i}-w^{T} x_{i}\\right)^{2} \\end{aligned}L(w)=logp(Yâˆ£X,w)wargmaxâ€‹L(w)â€‹=logi=1âˆNâ€‹p(yiâ€‹âˆ£xiâ€‹,w)=i=1âˆ‘Nâ€‹log(2Ï€Ïƒâ€‹1â€‹eâˆ’2Ïƒ2(yiâ€‹âˆ’uTxiâ€‹)2â€‹)=wargminâ€‹i=1Nâˆ‘â€‹(yiâ€‹âˆ’wTxiâ€‹)2â€‹ è¿™ä¸ªè¡¨è¾¾å¼å’Œæœ€å°äºŒä¹˜ä¼°è®¡å¾—åˆ°çš„ç»“æœä¸€æ ·ã€‚ æƒé‡å…ˆéªŒä¸ºé«˜æ–¯åˆ†å¸ƒçš„MAP-æœ€å¤§åéªŒä¼°è®¡ å–å…ˆéªŒåˆ†å¸ƒ $w \\sim \\mathcal{N}\\left(0, \\sigma_{0}^{2}\\right) $, Ïµâˆ¼N(0,Ïƒ2)\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)Ïµâˆ¼N(0,Ïƒ2) ,yâˆ£x;wâˆ¼(wâŠ¤x,Ïƒ2)y \\mid x ; w \\sim\\left(w^{\\top} x, \\sigma^{2}\\right)yâˆ£x;wâˆ¼(wâŠ¤x,Ïƒ2)ã€‚äºæ˜¯ï¼š w^=argmaxâ¡wp(wâˆ£Y)=argmaxâ¡wp(Yâˆ£w)p(w)=argmaxâ¡wlogâ¡p(Yâˆ£w)p(w)=argmaxâ¡w(logâ¡p(Yâˆ£w)+logâ¡p(w))=argminâ¡w[(yâˆ’wTx)2+Ïƒ2Ïƒ02wTw]\\begin{aligned} \\hat{w}=\\operatorname{argmax}_{w} p(w \\mid Y) &amp;=\\underset{w}{\\operatorname{argmax}} p(Y \\mid w) p(w) \\\\ &amp;=\\underset{w}{\\operatorname{argmax}} \\log p(Y \\mid w) p(w) \\\\ &amp;=\\underset{w}{\\operatorname{argmax}}(\\log p(Y \\mid w)+\\log p(w)) \\\\ &amp;=\\underset{w}{\\operatorname{argmin}}\\left[\\left(y-w^{T} x\\right)^{2}+\\frac{\\sigma^{2}}{\\sigma_{0}^{2}} w^{T} w\\right] \\end{aligned}w^=argmaxwâ€‹p(wâˆ£Y)â€‹=wargmaxâ€‹p(Yâˆ£w)p(w)=wargmaxâ€‹logp(Yâˆ£w)p(w)=wargmaxâ€‹(logp(Yâˆ£w)+logp(w))=wargminâ€‹[(yâˆ’wTx)2+Ïƒ02â€‹Ïƒ2â€‹wTw]â€‹ è¿™é‡Œçœç•¥äº†XXX,$ p(Y)$ å’Œ www æ²¡æœ‰å…³ç³»: p(wâˆ£Y)=p(w,Y)p(Y)=p(Yâˆ£wâˆ£p(w)p(Y)p(w \\mid Y)=\\frac{p(w, Y)}{p(Y)}=\\frac{p(Y|w| p(w)}{p(Y)}p(wâˆ£Y)=p(Y)p(w,Y)â€‹=p(Y)p(Yâˆ£wâˆ£p(w)â€‹ $ p(Y)æ˜¯å·²æœ‰å…ˆéªŒæ•°æ®å’Œæ˜¯å·²æœ‰å…ˆéªŒæ•°æ®å’Œæ˜¯å·²æœ‰å…ˆéªŒæ•°æ®å’Œw$æ— å…³ã€‚ æ­£åˆ™åŒ– æ­£åˆ™åŒ–çš„ä¸¤ç§æ–¹å¼ï¼š L1:argminâ¡wL(w)+Î»âˆ¥wâˆ¥1,Î»&gt;0L2:argminâ¡wL(w)+Î»âˆ¥wâˆ¥22,Î»&gt;0\\begin{array}{l} L 1: \\underset{w}{\\operatorname{argmin}} L(w)+\\lambda\\|w\\|_{1}, \\lambda&gt;0 \\\\ L 2: \\operatorname{argmin}_{w} L(w)+\\lambda\\|w\\|_{2}^{2}, \\lambda&gt;0 \\end{array}L1:wargminâ€‹L(w)+Î»âˆ¥wâˆ¥1â€‹,Î»&gt;0L2:argminwâ€‹L(w)+Î»âˆ¥wâˆ¥22â€‹,Î»&gt;0â€‹ L1æ­£åˆ™åŒ– Lasso L1æ­£åˆ™åŒ–å¯ä»¥å¼•èµ·ç¨€ç–è§£ ä»æœ€å°åŒ–æŸå¤±çš„è§’åº¦çœ‹ï¼Œç”±äº L1\\mathrm{L} 1L1 é¡¹æ±‚å¯¼åœ¨Oé™„è¿‘çš„å·¦å³å¯¼æ•°éƒ½ä¸æ˜¯0, å› æ­¤æ›´å®¹æ˜“å–åˆ°0 è§£ã€‚ L2æ­£åˆ™åŒ– Ridge w^=argminâ¡wL(w)+Î»wTwâŸ¶âˆ‚âˆ‚wL(w)+2Î»w=0âŸ¶2XTXw^âˆ’2XTY+2Î»w^=0âŸ¶w^=(XTX+Î»I)âˆ’1XTY\\begin{aligned} \\hat{w}=\\operatorname{argmin}_{w} L(w)+\\lambda w^{T} w &amp; \\longrightarrow \\frac{\\partial}{\\partial w} L(w)+2 \\lambda w=0 \\\\ &amp; \\longrightarrow 2 X^{T} X \\hat{w}-2 X^{T} Y+2 \\lambda \\hat{w}=0 \\\\ &amp; \\longrightarrow \\hat{w}=\\left(X^{T} X+\\lambda \\mathbb{I}\\right)^{-1} X^{T} Y \\end{aligned}w^=argminwâ€‹L(w)+Î»wTwâ€‹âŸ¶âˆ‚wâˆ‚â€‹L(w)+2Î»w=0âŸ¶2XTXw^âˆ’2XTY+2Î»w^=0âŸ¶w^=(XTX+Î»I)âˆ’1XTYâ€‹ å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªæ­£åˆ™åŒ–å‚æ•°å’Œå‰é¢çš„ MAP ç»“æœä¸è°‹è€Œåˆã€‚åˆ©ç”¨2èŒƒæ•°è¿›è¡Œæ­£åˆ™åŒ–ä¸ä»…å¯ ä»¥æ˜¯æ¨¡å‹é€‰æ‹© www è¾ƒå°çš„å‚æ•°ï¼ŒåŒæ—¶ä¹Ÿé¿å… XTXX^{T}XXTXä¸å¯é€†çš„é—®é¢˜ã€‚L2æ­£åˆ™åŒ–å¯ä»¥é™ä½è¿‡æ‹Ÿåˆã€‚ æ­£åˆ™åŒ–çš„çŸ¥è¯†è¿˜éœ€è¦è¿›ä¸€æ­¥ç†è§£ã€‚","categories":[{"name":"ç®—æ³•","slug":"ç®—æ³•","permalink":"https://mz2sj.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"æ•°æ®åˆ†æ","slug":"ç®—æ³•/æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"ç®—æ³•","slug":"ç®—æ³•","permalink":"https://mz2sj.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"numpyçŸ¥è¯†ç‚¹","slug":"numpyçŸ¥è¯†ç‚¹","date":"2020-12-16T02:30:38.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/12/16/numpyçŸ¥è¯†ç‚¹/","link":"","permalink":"https://mz2sj.github.io/2020/12/16/numpy%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"è®°å½•ä¸€äº›å¸¸ç”¨å‡½æ•°å’ŒçŸ¥è¯†ç‚¹ axisç†è§£ numpyä¸­axisçš„ç†è§£ axis=0,æ²¿ç€ç«–ç›´æ–¹å‘æ“ä½œï¼Œæ“ä½œå¯¹è±¡æ˜¯æ°´å¹³çš„ä¸€è¡Œï¼Œ axis=1,æ²¿ç€æ°´å¹³æ–¹å‘æ“ä½œï¼Œæ“ä½œå¯¹è±¡æ˜¯ç«–ç›´çš„ä¸€åˆ—ã€‚ &gt;&gt;&gt;df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], \\ columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) &gt;&gt;&gt;df col1 col2 col3 col4 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 &gt;&gt;&gt; df.mean(axis=1) 0 1 1 2 2 3 å¦‚ä»£ç æ‰€ç¤ºï¼Œaxis=1è¡¨ç¤ºæ²¿æ°´å¹³æ–¹å‘æ“ä½œç›¸åŠ æ±‚å¹³å‡ï¼Œæ“ä½œå¯¹è±¡æ˜¯ç«–ç›´çš„ä¸€åˆ—ï¼Œä¹Ÿå°±æ˜¯ 1 1 1 1 2 + 2 + 2 + 2 3 3 3 3 åŒç†çœ‹ä¸€ä¸‹drop &gt;&gt;&gt; df.drop(\"col4\", axis=1) col1 col2 col3 0 1 1 1 1 2 2 2 2 3 3 3 dropçš„å¯¹è±¡æ˜¯ä¸€åˆ—ï¼Œä¹Ÿå°±æ˜¯dropä¸‹é¢è¿™äº›åˆ—ä¸­çš„æŸä¸€åˆ— 1 1 1 1 2 2 2 3 3 sumå’Œdropä¸åŒçš„çš„æ“ä½œå«ä¹‰å’Œaxisç»“åˆèµ·æ¥å¯ä»¥äº§ç”Ÿä¸åŒçš„æ•ˆæœã€‚ np.full() åˆ›é€ å…¨ä¸ºæŸä¸ªå€¼çš„æ•°ç»„ np.full((2,5),6,dtype=np.int) array([[6, 6, 6, 6, 6], [6, 6, 6, 6, 6]], dtype=uint32) åˆ›é€ å½¢çŠ¶ç›¸åŒï¼Œå€¼ä¸ºæŸä¸ªå€¼çš„æ•°ç»„ x=np.arange(4,dtype=np.int64) np.full_like(x,6) array([6, 6, 6, 6], dtype=int64) np.asscalar() x = np.array([30]) np.asscalar(x) np.arange() np.arange(2, 101, 2) array([ 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]) np.linspace() ç»™å®šä¸€æ®µèŒƒå›´åˆ’åˆ†æŒ‡å®šæ•°ç›® np.linspace(3., 10, 50) array([ 3. , 3.14285714, 3.28571429, 3.42857143, 3.57142857, 3.71428571, 3.85714286, 4. , 4.14285714, 4.28571429, 4.42857143, 4.57142857, 4.71428571, 4.85714286, 5. , 5.14285714, 5.28571429, 5.42857143, 5.57142857, 5.71428571, 5.85714286, 6. , 6.14285714, 6.28571429, 6.42857143, 6.57142857, 6.71428571, 6.85714286, 7. , 7.14285714, 7.28571429, 7.42857143, 7.57142857, 7.71428571, 7.85714286, 8. , 8.14285714, 8.28571429, 8.42857143, 8.57142857, 8.71428571, 8.85714286, 9. , 9.14285714, 9.28571429, 9.42857143, 9.57142857, 9.71428571, 9.85714286, 10. ]) np.logspace() åŒnp.linspace(),åªä¸è¿‡å–äº†log np.logspace(3,10,50,endpoint=False) array([ 1.00000000e+03, 1.38038426e+03, 1.90546072e+03, 2.63026799e+03, 3.63078055e+03, 5.01187234e+03, 6.91830971e+03, 9.54992586e+03, 1.31825674e+04, 1.81970086e+04, 2.51188643e+04, 3.46736850e+04, 4.78630092e+04, 6.60693448e+04, 9.12010839e+04, 1.25892541e+05, 1.73780083e+05, 2.39883292e+05, 3.31131121e+05, 4.57088190e+05, 6.30957344e+05, 8.70963590e+05, 1.20226443e+06, 1.65958691e+06, 2.29086765e+06, 3.16227766e+06, 4.36515832e+06, 6.02559586e+06, 8.31763771e+06, 1.14815362e+07, 1.58489319e+07, 2.18776162e+07, 3.01995172e+07, 4.16869383e+07, 5.75439937e+07, 7.94328235e+07, 1.09647820e+08, 1.51356125e+08, 2.08929613e+08, 2.88403150e+08, 3.98107171e+08, 5.49540874e+08, 7.58577575e+08, 1.04712855e+09, 1.44543977e+09, 1.99526231e+09, 2.75422870e+09, 3.80189396e+09, 5.24807460e+09, 7.24435960e+09]) np.diagonal() è¾“å‡ºå¯¹è§’çº¿ï¼ŒçŸ©é˜µçš„è¿¹ X = np.array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) np.diag(X) X.diagonal() array([ 0, 5, 10]) åˆ›é€ å¯¹è§’çº¿æŒ‡å®šå€¼ï¼Œå…¶ä½™ä¸º0çš„çŸ©é˜µ np.diagflat([1,2,3,4]) array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) np.tri() åˆ›é€ å¯¹è§’çº¿æŒ‡å®šæ–¹å‘ä¸Šä¸‹ä¸º1ï¼Œå…¶ä½™ä¸º0çš„çŸ©é˜µ np.tri(5,5,1) array([[1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) np.tri(5,5,0) array([[1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.]]) np.tri(5,5,-1) array([[0., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 1., 0., 0., 0.], [1., 1., 1., 0., 0.], [1., 1., 1., 1., 0.]]) #ä¸‹ä¸‰è§’ å¯¹è§’çº¿ä»¥ä¸‹ä¿ç•™ï¼Œä»¥ä¸Šä¸º0 np.tril(np.arange(1, 13).reshape(4, 3), -1) array([[ 0, 0, 0], [ 4, 0, 0], [ 7, 8, 0], [10, 11, 12]]) np.triu(np.arange(1, 13).reshape(4, 3), -1) array([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) æ‰“å¹³ x = np.array([[1, 2, 3], [4, 5, 6]]) out1=np.ravel(x,order='F') out2=x.flatten(order='F') np.swapaxes() x=np.zeros((3,4,5)) out1=np.swapaxes(x,1,0) out2=x.transpose([1,0,2]) np.expand_dims() x=np.zeros((3,4)) np.expand_dims(x,axis=1) np.squeeze(x) np.concatenate() x = np.array([[1, 2, 3], [4, 5, 6]]) y = np.array([[7, 8, 9], [10, 11, 12]]) out1 = np.concatenate((x, y), 1) #æ°´å¹³æ–¹å‘æ“ä½œï¼Œç«–ç›´ä¸ºæ“ä½œå¯¹è±¡ out2 = np.hstack((x, y)) #æ°´å¹³æ–¹å‘å †å  #æŒ‰åˆ—æ¥å †å  x = np.array((1,2,3)) y = np.array((4,5,6)) out1 = np.column_stack((x, y)) x = np.array([[1],[2],[3]]) y = np.array([[4],[5],[6]]) out = np.dstack((x, y)) [[[1 4]] [[2 5]] [[3 6]]] np.split() x = np.arange(1, 10) np.split(x, [4, 6]) [array([1, 2, 3, 4]), array([5, 6]), array([7, 8, 9])]","categories":[{"name":"numpy","slug":"numpy","permalink":"https://mz2sj.github.io/categories/numpy/"}],"tags":[{"name":"numpy","slug":"numpy","permalink":"https://mz2sj.github.io/tags/numpy/"}]},{"title":"ç¬¬äºŒå±Šç¿¼æ”¯ä»˜æ¯å¤§æ•°æ®å»ºæ¨¡å¤§èµ›å¤ç›˜","slug":"ç¬¬äºŒå±Šç¿¼æ”¯ä»˜æ¯å¤§æ•°æ®å»ºæ¨¡å¤§èµ›å¤ç›˜","date":"2020-10-03T07:51:44.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/10/03/ç¬¬äºŒå±Šç¿¼æ”¯ä»˜æ¯å¤§æ•°æ®å»ºæ¨¡å¤§èµ›å¤ç›˜/","link":"","permalink":"https://mz2sj.github.io/2020/10/03/%E7%AC%AC%E4%BA%8C%E5%B1%8A%E7%BF%BC%E6%94%AF%E4%BB%98%E6%9D%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E5%A4%A7%E8%B5%9B%E5%A4%8D%E7%9B%98/","excerpt":"","text":"æš‘å‡çš„æ—¶å€™å‚åŠ äº†ä¸€ä¸ªæ•°æ®ç«èµ›â€“ç¬¬äºŒå±Šç¿¼æ”¯ä»˜æ¯å¤§æ•°æ®å»ºæ¨¡å¤§èµ›-ä¿¡ç”¨é£é™©ç”¨æˆ·è¯†åˆ«ã€‚æœ€åå¤èµ›Aæ¦œæ’åœ¨ç¬¬15åï¼ŒBæ¦œ12åã€‚ç¬¬ä¸€æ¬¡æ­£å¼å‚åŠ æ¯”èµ›ï¼ŒåŸæœ¬åªæƒ³æ’åˆ°å…­ä¸ƒååå°±å¤Ÿäº†ï¼Œæ²¡æƒ³åˆ°æœ€åèƒ½è¿›åˆ°å‰äºŒåï¼Œæ„Ÿè°¢é˜Ÿå‹å’Œè‡ªå·±çš„ä»˜å‡ºã€‚è™½ç„¶æœ€åçš„ç»“æœè¿˜ä¸é”™ï¼Œä½†æ˜¯åšçš„è¿˜æ˜¯æ¨¡æ¨¡ç³Šç³Šçš„ï¼Œç°åœ¨å¯¹ä¸€äº›topæ–¹æ¡ˆå’Œè‡ªå·±å‚åŠ æ¯”èµ›çš„æƒ³æ³•åšä¸€äº›æ€»ç»“ã€‚ æ•°æ® åŸºç¡€ä¿¡æ¯ base_df å­—æ®µå å­—æ®µè¯´æ˜ï¼ˆæ•°æ®ç»è¿‡è„±æ•å¤„ç†ï¼‰ user æ ·æœ¬ç¼–å·ï¼Œe.g., Train_00000ã€Train_00001â€¦ sex æ€§åˆ«ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 age å¹´é¾„ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° provider è¿è¥å•†ç±»å‹ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category 1â€¦ level ç”¨æˆ·ç­‰çº§ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category 1â€¦ verified æ˜¯å¦å®åï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 using_time ä½¿ç”¨æ—¶é•¿ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° regist_type æ³¨å†Œç±»å‹ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category 1â€¦ card_a_cnt aç±»å‹å¡çš„æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° card_b_cnt bç±»å‹å¡çš„æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° card_c_cnt cç±»å‹å¡çš„æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° card_d_cnt dç±»å‹å¡çš„æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° op1_cnt æŸç±»å‹1æ“ä½œæ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° op2_cnt æŸç±»å‹2æ“ä½œæ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° service1_cnt æŸä¸šåŠ¡1äº§ç”Ÿæ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° service1_amt æŸä¸šåŠ¡1äº§ç”Ÿé‡‘é¢ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° service2_cnt æŸä¸šåŠ¡2äº§ç”Ÿæ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° agreement_total å¼€é€šåè®®æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° agreement1 æ˜¯å¦å¼€é€šåè®®1ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 agreement2 æ˜¯å¦å¼€é€šåè®®2ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 agreement3 æ˜¯å¦å¼€é€šåè®®3ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 agreement4 æ˜¯å¦å¼€é€šåè®®4ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 acc_count è´¦å·æ•°é‡ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° login_cnt_period1 æŸæ®µæ—¶æœŸ1çš„ç™»å½•æ¬¡æ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° login_cnt_period2 æŸæ®µæ—¶æœŸ2çš„ç™»å½•æ¬¡æ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° ip_cnt æŸæ®µæ—¶æœŸç™»å½•ipä¸ªæ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° login_cnt_avg æŸæ®µæ—¶æœŸç™»å½•æ¬¡æ•°å‡å€¼ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° login_days_cnt æŸæ®µæ—¶æœŸç™»å½•å¤©æ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° province çœä»½ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  city åŸå¸‚ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  balance ä½™é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 balance_avg è¿‘æŸæ®µæ—¶æœŸä½™é¢å‡å€¼ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 balance1 ç±»å‹1ä½™é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 balance1_avg è¿‘æŸæ®µæ—¶æœŸç±»å‹1ä½™é¢å‡å€¼ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 balance2 ç±»å‹2ä½™é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 balance2_avg è¿‘æŸæ®µæ—¶æœŸç±»å‹2ä½™é¢å‡å€¼ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 service3 æ˜¯å¦æœåŠ¡3ç”¨æˆ·ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1 service3_level æœåŠ¡3ç­‰çº§ï¼Œç¼–ç åå–å€¼ä¸ºï¼šcategory 0ã€category1â€¦ product1_amount äº§å“1é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product2_amount äº§å“2é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product3_amount äº§å“3é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product4_amount äº§å“4é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product5_amount äº§å“5é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product6_amount äº§å“6é‡‘é¢ç­‰çº§ï¼Œå¤„ç†æˆä¿ç•™å¤§å°å…³ç³»çš„ç±»åˆ«ç¼–ç ï¼šlevel 1ã€level2â€¦ ä¾‹å¦‚ï¼šlevel 2 &gt; level 1 product7_cnt äº§å“7ç”³è¯·æ¬¡æ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° product7_fail_cnt äº§å“7ç”³è¯·å¤±è´¥æ¬¡æ•°ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° æ“ä½œä¿¡æ¯ op_df å­—æ®µå å­—æ®µè¯´æ˜ï¼ˆæ•°æ®ç»è¿‡è„±æ•å¤„ç†ï¼‰ user æ ·æœ¬ç¼–å·ï¼Œe.g., Train_00000ã€Train_00001â€¦ op_type æ“ä½œç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  op_mode æ“ä½œæ¨¡å¼ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  op_device æ“ä½œè®¾å¤‡ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  ip è®¾å¤‡ipç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  net_type ç½‘ç»œç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  channel æ¸ é“ç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  ip_3 è®¾å¤‡ipå‰ä¸‰ä½ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  tm_diff è·ç¦»æŸèµ·å§‹æ—¶é—´ç‚¹çš„æ—¶é—´é—´éš”ï¼Œå¤„ç†æˆå¦‚ä¸‹æ ¼å¼ã€‚ä¾‹å¦‚ï¼š 9 days 09:02:45.000000000ï¼Œè¡¨ç¤ºè·ç¦»æŸèµ·å§‹æ—¶é—´ç‚¹9å¤©9å°æ—¶2åˆ†é’Ÿ45ç§’ äº¤æ˜“ä¿¡æ¯ trans_df å­—æ®µå å­—æ®µè¯´æ˜ï¼ˆæ•°æ®ç»è¿‡è„±æ•å¤„ç†ï¼‰ user æ ·æœ¬ç¼–å·ï¼Œe.g., Train_00000ã€Train_00001â€¦ platform å¹³å°ç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  tunnel_in æ¥æºç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  tunnel_out å»å‘ç±»å‹ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  amount äº¤æ˜“é‡‘é¢ï¼Œå¤„ç†åä»…ä¿ç•™å¤§å°å…³ç³»ï¼Œä¸ºæŸä¸€åŒºé—´çš„æ•´æ•° type1 äº¤æ˜“ç±»å‹1ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  type2 äº¤æ˜“ç±»å‹2ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  ip è®¾å¤‡ipç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  ip_3 è®¾å¤‡ipå‰ä¸‰ä½ç¼–ç ï¼Œå¤„ç†æˆç±»åˆ«ç¼–ç  tm_diff è·ç¦»æŸèµ·å§‹æ—¶é—´ç‚¹çš„æ—¶é—´é—´éš”ï¼Œå¤„ç†æˆå¦‚ä¸‹æ ¼å¼ã€‚ä¾‹å¦‚ï¼š 9 days 09:02:45.000000000ï¼Œè¡¨ç¤ºè·ç¦»æŸèµ·å§‹æ—¶é—´ç‚¹9å¤©9å°æ—¶2åˆ†é’Ÿ45ç§’ è¯„ä»·æ–¹å¼é‡‡ç”¨roc_auc_score top1æ–¹æ¡ˆ 1.ç¬¬ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„ç‚¹å°±æ˜¯ä½œè€…æ ¹æ®useré”®å°†trans_dfå’Œop_dfä¸­mergeè¿›äº†base_dfçš„label,è¿™æ ·å°±èƒ½å¯¹trans_dfå’Œop_dfä½œtarget_encodingå•¦~ def gen_trans_op_label(trans, op, train_label): trans_ = trans.copy() op_ = op.copy() user_label_dict = dict(zip(train_label.user.values, train_label.label.values)) trans_['label'] = trans_['user'].map(user_label_dict) op_['label'] = op_['user'].map(user_label_dict) return trans_, op_ è®©æˆ‘ä»¬å­¦ä¹ ä¸€ä¸‹pandasä¸­çš„mapæ“ä½œã€‚ å…¶æ¬¡å°±æ˜¯å°†trans_dfå’Œop_dfæ‹¼æ¥æˆä¸€å¼ è¡¨trans_op_dfï¼Œè¿™æ ·ä¸€å¼ è¡¨å°±åŒæ—¶åŒ…å«æ“ä½œå’Œäº¤æ˜“ä¿¡æ¯,ä¹Ÿè®¸ä¼šå¸¦æ¥æ›´å¤šçš„ä¿¡æ¯å“¦ 2.æ„å»ºsessionï¼Œè¿™ä¸ªæ˜¯æ ¹æ®æ“ä½œæˆ–è€…äº¤æ˜“çš„æ—¶é—´å·®æ¥ç¡®å®šä¸€ä¸ªsessionï¼Œå¹¶ä¸æ˜¯å¾ˆæ‡‚ã€‚å½“ç›¸é‚»ä¸¤ä¸ªæ“ä½œæˆ–è€…äº¤æ˜“æ—¶é—´å·®è¶…è¿‡600æ—¶ï¼Œèµ‹å€¼ä¸º1ä½œcumsumã€‚ def gen_session_fea(file): tmp = file.groupby('user')['timestamp'].shift() file['delta_time'] = file['timestamp'] - tmp file['session'] = np.where(file['delta_time'] &gt; 600, 1, 0) file['session'] = file.groupby(['user'])['session'].transform('cumsum') del file['delta_time'] return file æ€ä¹ˆè§£é‡Šå‘¢ï¼Ÿå½“ç›¸é‚»çš„æ“ä½œæˆ–äº¤æ˜“åœ¨ä¸€å®šçš„æ—¶é—´èŒƒå›´å†…å®Œæˆæ—¶ï¼Œå°±è®¤ä¸ºæ˜¯ä¸€ä¸ªsessionï¼Œå½“å¾ˆå¤šæ“ä½œåœ¨ç›¸é‚»çš„æ—¶é—´å†…å®Œæˆï¼Œæ˜¯å¦å¯ä»¥çœ‹æˆä¸€ç§è¡Œä¸ºæ¨¡å¼ï¼Œç”¨cumsumæ¥ç´¯ç§¯ï¼Ÿå¦å¤–ä¸€ä¸ªç‚¹å°±æ˜¯pandasä¸­çš„groupbyåä½œshiftçš„æ“ä½œï¼Œå¯ä»¥å¯¹ä¸€ä¸ªç»„å†…çš„æŸä¸ªå˜é‡è¿›è¡Œåç§»æ“ä½œã€‚ 3.ä¸€äº›é’ˆå¯¹op_dfå’Œtrans_dfçš„äº¤å‰ç‰¹å¾ def gen_fea_op_df(op_df): op_df['op_pattern'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str) op_df['op_type_mode'] = op_df['op_type'].map(str) + '_' + op_df['op_mode'].map(str) op_df['op_type_device'] = op_df['op_type'].map(str) + '_' + op_df['op_device'].map(str) op_df['op_mode_device'] = op_df['op_mode'].map(str) + '_' + op_df['op_device'].map(str) op_df['ip_net_type'] = op_df['ip'].map(str) + '_' + op_df['net_type'].map(str) op_df['ip3_net_type'] = op_df['ip_3'].map(str) + '_' + op_df['net_type'].map(str) op_df['net_type_channel'] = op_df['net_type'].map(str) + '_' + op_df['channel'].map(str) # op_df['time_diff'] = op_df['timestamp'].diff(-1) op_df.rename(columns={'ip' : 'op_ip', 'ip_3': 'op_ip_3',}, inplace=True) return op_df å¯ä»¥çœ‹å‡ºæ¥çš„ä¸€ç‚¹æ˜¯ï¼Œå¯¹äºäº¤å‰ç‰¹è¯Šï¼Œä½œè€…ä¹Ÿæ˜¯å°†ç›¸å…³çš„ä¸€äº›ç‰¹å¾è¿›è¡Œç»„åˆã€‚è‡ªå·±å½“æ—¶åªåšäº†ä¸€åˆ—åˆ—åˆ«æ•°é‡è¾ƒå°‘çš„ç‰¹å¾çš„äº¤å‰ï¼Œåœ¨è¿™é‡Œå¯ä»¥çœ‹åˆ°ä½œè€…å¯¹ipç­‰å¾ˆå¤šç±»åˆ«çš„ç‰¹å¾ä¹Ÿè¿›è¡Œäº†äº¤å‰ã€‚ def gen_fea_trans_df(trans_df): trans_df['tunnel_io'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['tunnel_out'].astype(str) trans_df['type'] = trans_df['type1'].astype(str) + '_' +trans_df['type2'].astype(str) trans_df['tunnel_io_type'] = trans_df['tunnel_io'].astype(str) + '_' + trans_df['type'].astype(str) trans_df['platform_tunnel_io_type'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io_type'] trans_df['platform_tunnel_io'] = trans_df['platform'].astype(str) + '_' + trans_df['tunnel_io'] trans_df['platform_type'] = trans_df['platform'].astype(str) + '_' + trans_df['type'] trans_df['platform_amount'] = trans_df['platform'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_io_amount'] = trans_df['type'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type1_amount'] = trans_df['type1'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['type2_amount'] = trans_df['type2'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_in_amount'] = trans_df['tunnel_in'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['tunnel_out_amount'] = trans_df['tunnel_out'].astype(str) + '_' + trans_df['amount'].astype(str) trans_df['amount_diff'] = trans_df['amount'].astype(int).diff(-1) trans_df['time_diff'] = trans_df['timestamp'].diff(-1) trans_df['amount_per_time'] = trans_df['amount_diff'] / np.where(trans_df['time_diff'] == 0, 0.01, trans_df['time_diff']) trans_df = gen_session_fea(trans_df) æ¯”è¾ƒæœ‰çœ‹ç‚¹çš„æ˜¯å¯¹amountåšäº†diffæ“ä½œï¼Œè®¡ç®—ç›¸é‚»ä¸¤æ¬¡äº¤æ˜“çš„å·®é¢ï¼Œå¹¶è®¡ç®—äº†ç›¸é‚»ä¸¤æ¬¡äº¤æ˜“çš„æ—¶é—´ï¼Œä¸¤è€…ç›¸é™¤å¾—åˆ°äº¤æ˜“é¢ä¸æ—¶é—´å·®çš„æ¯”å€¼ã€‚ 3.base_dfäº¤å‰ç‰¹å¾ã€‚å¯¹äºbase_dfé‡Œçš„æ•°å€¼ç‰¹å¾ä½œåŠ å‡ä¹˜é™¤æ“ä½œã€‚ def int_cols_cross(df, cols): \"\"\"[summary] å¯¹baseæ•°æ®çš„int64ç‰¹å¾è¿›è¡Œmin-maxå½’ä¸€åŒ–åè¿›è¡ŒåŠ å‡ä¹˜é™¤äº¤äº’ Parameters ---------- df : [DataFrame] [è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆå¹¶çš„æ•°æ®] cols : [list] [äº¤äº’ç‰¹å¾] Returns ------- [DataFrame, list] [æ•´æ•°ç‰¹å¾äº¤äº’åçš„data, åŠäº¤äº’ç‰¹å¾åç§°] \"\"\" cross_feature = [] df = df.copy() for i, col in tqdm(enumerate(cols), desc='extract cross feature for base'): for j in range(i + 1, len(cols)): df[col + '_' + 'div_' + cols[j]] = min_max_unif(df[col]) / min_max_unif(df[cols[j]]) df[col + '_' + 'sub_' + cols[j]] = min_max_unif(df[col]) - min_max_unif(df[cols[j]]) df[col + '_' + 'mul_' + cols[j]] = min_max_unif(df[col]) * min_max_unif(df[cols[j]]) df[col + '_' + 'sum_' + cols[j]] = min_max_unif(df[col]) + min_max_unif(df[cols[j]]) cross_feature.append(col + '_' + 'div_' + cols[j]) cross_feature.append(col + '_' + 'sub_' + cols[j]) cross_feature.append(col + '_' + 'mul_' + cols[j]) cross_feature.append(col + '_' + 'sum_' + cols[j]) return df, cross_feature è¿™ä¸ªæ„Ÿè§‰å¾ˆå¼ºå‘€ï¼ŒåŠ å‡ä¹˜é™¤éƒ½ç”¨åˆ°æ•°å€¼ç‰¹å¾ä¸Šã€‚ 3.countè®¡æ•°ç‰¹å¾ def gen_cnt_feature(df, feature): cnt_features = [] for fea in feature: df[fea + '_count'] = df.groupby([fea])['user'].transform('count') cnt_features.append(fea + '_count') return df ä½†æ˜¯ä¸ºä»€ä¹ˆåªå¯¹cnt_feature = ['city', 'province', 'balance', 'ip_cnt', 'using_time', ]è¿™äº›ç‰¹å¾ä½œè®¡æ•°ç‰¹å¾å°±ä¸çŸ¥é“æ˜¯ä¸ºä»€ä¹ˆäº†ï¼Œéš¾é“ä¹Ÿæ˜¯ä¸€ä¸ªä¸ªè¯•å‡ºæ¥çš„å˜›ï¼Ÿ 4.trans_dfçš„amountç»Ÿè®¡ç‰¹å¾ def gen_user_amount_features(df): group_df = df.groupby(['user'])['amount'].agg({ 'user_amount_mean': 'mean', # 'user_amount_std': 'std', 'user_amount_max': 'max', 'user_amount_min': 'min', 'user_amount_sum': 'sum', 'user_amount_med': 'median', 'user_amount_cnt': 'count', # 'user_amount_q1': lambda x: x.quantile(0.25), # 'user_amount_q3': lambda x: x.quantile(0.75), #'user_amount_qsub': lambda x: x.quantile(0.75) - x.quantile(0.25) #'user_amount_skew': 'skew', }).reset_index() return group_df è¿™ä¸ªæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œè‚¯å®šå¾—åš 5.nuniqueç‰¹å¾ def gen_user_nunique_features(df, value, prefix): group_df = df.groupby(['user'])[value].agg({ 'user_{}_{}_nuniq'.format(prefix, value): 'nunique' }).reset_index() return group_df è®¡ç®—ä¸€ä¸ªç”¨æˆ·åœ¨æŸä¸ªå­—æ®µä¸Šæœ‰å‡ ä¸ªuniqueå€¼. trans_df:['days_diff', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', ] 6.ä¸€ä¸ªå€¼æœ‰å‡ ä¸ªç”¨æˆ·çš„nuniqueç»Ÿè®¡ç»“æœ é¦–å…ˆè®¡ç®—ä¸€ä¸ªå€¼æœ‰å‡ ä¸ªç”¨æˆ· def file_cols_user_nunique(file, feature_lst, prefix): col_nuniq_fea_lst = [] for col in tqdm(feature_lst): col_nuniq = file.groupby(col)['user'].nunique() col_nuniq_dic = dict(zip(col_nuniq.index, col_nuniq.values)) file[prefix + '_' + col + '_user_nuniq'] = file[col].map(col_nuniq_dic) col_nuniq_fea_lst.append(prefix + '_' + col + '_user_nuniq') return file, col_nuniq_fea_lst åœ¨å¯¹ä¸Šé¢çš„è®¡ç®—å€¼ä½œç»Ÿè®¡è®¡ç®— def gen_stastic_col_user_nunique(file, feat, prefix): group_df = file.groupby('user')[feat].agg({ prefix + feat + '_mean': 'mean', prefix + feat + '_std': 'std', prefix + feat + '_max': 'max', prefix + feat + '_min': 'min', prefix + feat + '_sum': 'sum', prefix + feat + '_med': 'median', #prefix + feat + '_q1' : lambda x: x.quantile(0.25), #prefix + feat + '_q3' : lambda x: x.quantile(0.75), #prefix + feat + 'q_sub': lambda x: x.quantile(0.75) - x.quantile(0.25), #prefix + feat + '_skew': 'skew', }) return group_df è¿™ä¸ªåœ¨trans_dfã€op_dfã€trans_op_dféƒ½å¯ä»¥åšã€‚ 7.åˆ©ç”¨pivot_tableè®¡ç®—åœ¨å„ä¸ªå€¼ç±»åˆ«ä¸Šäº¤æ˜“çš„èšåˆ def gen_user_group_amount_features(df, value): group_df = df.pivot_table(index='user', columns=value, values='amount', dropna=False, aggfunc=['count', 'sum', 'mean', 'max', 'min', 'median', ]).fillna(0) group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns] group_df.reset_index(inplace=True) return group_df è¿™ç§ç‰¹å¾æ„é€ æ–¹å¼åœ¨ç±»åˆ«ä¸æ˜¯ç‰¹åˆ«å¤šçš„æ—¶å€™æ¯”è¾ƒé€‚ç”¨ã€‚ 8.è®¡ç®—ä¸åŒæ—¶é—´çª—å£ä¸‹ç”¨æˆ·äº¤æ˜“é¢ç»Ÿè®¡ä¿¡æ¯ def gen_user_window_amount_features(df, window): group_df = df[df['days_diff']&gt;window].groupby('user')['amount'].agg({ 'user_amount_mean_{}d'.format(window): 'mean', 'user_amount_std_{}d'.format(window): 'std', 'user_amount_max_{}d'.format(window): 'max', 'user_amount_min_{}d'.format(window): 'min', 'user_amount_sum_{}d'.format(window): 'sum', 'user_amount_med_{}d'.format(window): 'median', 'user_amount_cnt_{}d'.format(window): 'count', # 'user_amount_q1_{}d'.format(window): lambda x: x.quantile(0.25), # 'user_amount_q3_{}d'.format(window): lambda x: x.quantile(0.75), # 'user_amount_qsub_{}d'.format(window): lambda x: x.quantile(0.75) - x.quantile(0.25), # 'user_amount_skew_{}d'.format(window): 'skew', # 'user_amount_q4_{}d'.format(window): lambda x: x.quantile(0.8), # 'user_amount_q5_{}d'.format(window): lambda x: x.quantile(0.3), # 'user_amount_q6_{}d'.format(window): lambda x: x.quantile(0.7), }).reset_index() return group_df 9.ç©ºå€¼ç‰¹å¾ def gen_user_null_features(df, value, prefix): df['is_null'] = 0 df.loc[df[value].isnull(), 'is_null'] = 1 group_df = df.groupby(['user'])['is_null'].agg({'user_{}_{}_null_cnt'.format(prefix, value): 'sum', 'user_{}_{}_null_ratio'.format(prefix, value): 'mean'}).reset_index() return group_df 10.ç»Ÿè®¡ä¸€äº›ç‰¹æ®Šå€¼ä¸Šæ—¶é—´çš„èšåˆç‰¹å¾ def gen_file_type_days_diff(df, file, type, time_feat): plot_feats = [] #file_type_unique = file[type].value_counts().index.tolist() file_type_unique = [] if type == 'type1': file_type_unique = ['45a1168437c708ff', 'f67d4b5a05a1352a', ] elif type == 'type2': file_type_unique = ['11a213398ee0c623',] elif type == 'channel': file_type_unique = ['b2e7fa260df4998d', '116a2503b987ea81', '8adb3dcfea9dcf5e'] elif type == 'tunnel_io': file_type_unique = ['b2e7fa260df4998d_6ee790756007e69a',] elif type == 'type': file_type_unique = ['f67d4b5a05a1352a_nan', '19d44f1a51919482_11a213398ee0c623', '45a1168437c708ff_11a213398ee0c623', '674e8d5860bc033d_11a213398ee0c623', '0a3cf8dac7dca9d1_b5a8be737a50b171'] for tp in file_type_unique: assert file_type_unique != [] group_df = file[file[type] == tp].groupby(['user'])[time_feat].agg( {'user_{}_{}_min_{}'.format(type, tp, time_feat): 'min', 'user_{}_{}_mean_{}'.format(type, tp, time_feat): 'mean', 'user_{}_{}_max_{}'.format(type, tp, time_feat): 'max', 'user_{}_{}_std_{}'.format(type, tp, time_feat): 'std', 'user_{}_{}_median_{}'.format(type, tp, time_feat): 'median', 'user_{}_{}_sum_{}'.format(type, tp, time_feat): 'sum', # 'user_{}_{}_q1_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.25), # 'user_{}_{}_q3_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75), # 'user_{}_{}_q_sub_{}'.format(type, tp, time_feat): lambda x: x.quantile(0.75) - x.quantile(0.25), # 'user_{}_{}_skew_{}'.format(type, tp, time_feat): 'skew', }).reset_index() df = df.merge(group_df, on=['user'], how='left') stastic = ['min', 'max', 'max', 'std', 'median', 'sum',] for stast in stastic: plot_feats.append('user_{}_{}_{}_{}'.format(type, tp, stast, time_feat)) return df, plot_feats è¿™ä¸ªç‰¹æ®Šå€¼çš„é€‰æ‹©ä½œè€…ä¹Ÿæ²¡æœ‰æåˆ°ï¼Œåº”è¯¥æ˜¯è¦æ ¹æ®ä¸€äº›ç”»å›¾åˆ†æå¾—åˆ°ã€‚ 11.doc2vecç‰¹å¾ def d2v_feat(df, feat, length, num): print('Start training Doc2Vec models.......') df[feat] = df[feat].astype(str) group_df = df.groupby(['user'])[feat].agg(list).reset_index() documents = [TaggedDocument(doc, [i]) for i, doc in zip(group_df['user'].values, group_df[feat])] model = Doc2Vec(documents, vector_size=length, window=10, min_count=1, workers=1, seed=2020, epochs=20, hs=1, ) if not os.path.exists('./d2v_models/'): os.makedirs('./d2v_models/') model.save('../d2v_models/d2v_testb_{}.model'.format(num)) # model = Doc2Vec.load('./d2v_models/d2v_testb_{}.model'.format(num)) doc_df = group_df['user'].apply(lambda x: ','.join([str(i) for i in model[x]])).str.split(',', expand=True).apply(pd.to_numeric) doc_df.columns = ['{}_d2v_{}'.format(feat, i) for i in range(length)] return pd.concat([group_df[['user']], doc_df], axis=1) emâ€¦,çœ‹æ‡‚ä»£ç å°±è¡Œå•¦ã€‚ç”¨åœ¨trans_dfçš„amountä¸Šã€‚è‡ªå·±ä¹Ÿåšäº†è¿™ä¸ªç‰¹å¾ï¼Œè¿˜ç”¨åœ¨äº†å…¶ä»–çš„å­—æ®µåºåˆ—ä¸Šï¼Œæ„Ÿè§‰ä¹Ÿä¸èƒ½æ— è„‘å¯¹æ‰€æœ‰å­—æ®µéƒ½ç”¨ï¼Œè¿˜æ˜¯è¦æ ¹æ®å®éªŒç»“æœæœ‰é€‰æ‹©çš„ç”¨ã€‚ 12.word2vecç‰¹å¾ def w2v_feat(df, feat, length, num): \"\"\" :param df: è¿›è¡Œword2vecç¼–ç çš„æ•°æ® :param feat: è¿›è¡Œç¼–ç çš„ç‰¹å¾ :param length: embeddingå‘é‡é•¿åº¦ :return: \"\"\" global w2v_fea_lst w2v_fea_lst = [] print('Start training Word2Vec models.....') df[feat] = df[feat].astype(str) group_df = df.groupby(['user'])[feat].agg(list).reset_index() model = Word2Vec(group_df[feat].values, size=length, window=10, min_count=1, sg=1, hs=1, workers=1, iter=20, seed=2020,) # if feat == 'amount': # model = Word2Vec.load('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) # elif feat == 'channel': # model = Word2Vec.load('../w2v_models/w2v_channel_16.model') # elif feat == 'trans_op_ip' or feat == 'trans_op_ip_3': # model = Word2Vec.load('../w2v_models/w2v_trans_op_2.model') # else: # if not os.path.exists('../w2v_models/'): # os.makedirs('../w2v_models/') # model = Word2Vec.load('./w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) model.save('../w2v_models/w2v_testb_{}_{}.model'.format(feat, num)) group_df[feat] = group_df[feat].apply(lambda x: pd.DataFrame([model[c] for c in x])) for m in tqdm(range(length), desc='extract w2v {} statistic feature'.format(feat)): group_df['{}_w2v_{}_mean'.format(feat,m)] = group_df[feat].apply(lambda x: x[m].mean()) # group_df['{}_w2v_{}_median'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].median()) # group_df['{}_w2v_{}_max'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].max()) # group_df['{}_w2v_{}_min'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].min()) # group_df['{}_w2v_{}_sum'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].sum()) # group_df['{}_w2v_{}_std'.format(feat, m)] = group_df[feat].apply(lambda x: x[m].std()) w2v_fea_lst.append('{}_w2v_{}_mean'.format(feat,m)) del group_df[feat] return group_df ç”¨åœ¨trans_dfçš„amountä¸Š 13.tf_idfç‰¹å¾ def gen_user_tfidf_features(df, value,): print('Start tfdif encoding for {}........'.format(value)) df[value] = df[value].astype(str) df[value].fillna('-1', inplace=True) group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index() group_df.columns = ['user', 'list'] group_df['list'] = group_df['list'].apply(lambda x: ','.join(x)) enc_vec = TfidfVectorizer() tfidf_vec = enc_vec.fit_transform(group_df['list']) svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020) vec_svd = svd_enc.fit_transform(tfidf_vec) vec_svd = pd.DataFrame(vec_svd) vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)] group_df = pd.concat([group_df, vec_svd], axis=1) del group_df['list'] return group_df 14.countvecç‰¹å¾ def gen_user_countvec_features(df, value,): print('Start countvec encoding for {}........'.format(value)) df[value] = df[value].astype(str) df[value].fillna('-1', inplace=True) group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index() group_df.columns = ['user', 'list'] group_df['list'] = group_df['list'].apply(lambda x: ','.join(x)) enc_vec = CountVectorizer() tfidf_vec = enc_vec.fit_transform(group_df['list']) svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020) vec_svd = svd_enc.fit_transform(tfidf_vec) vec_svd = pd.DataFrame(vec_svd) vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)] group_df = pd.concat([group_df, vec_svd], axis=1) del group_df['list'] return group_df 15.è®¡ç®—å„ä¸ªç”¨æˆ·åœ¨op_dfå’Œtrans_dfä¸Šåœ¨ä¸€äº›å­—æ®µä¸Šçš„ç»Ÿè®¡ç‰¹å¾ def gen_user_group_trans_op_features(df, columns, value): group_df = df.pivot_table(index='user', columns=columns, values=value, dropna=False, aggfunc=['count', 'sum', 'mean', 'max', 'min', 'median', ]).fillna(0) group_df.columns = ['user_{}_{}_{}_{}'.format(columns, f[1], value, f[0]) for f in group_df.columns] group_df['op_trans_ratio'] = group_df['user_property_trans_{}_count'.format(value)] / group_df[ 'user_property_op_{}_count'.format(value)] group_df.reset_index(inplace=True) return group_df 16.è·å–å„ä¸ªç”¨æˆ·åœ¨ç¬¬ä¸€æ¬¡å’Œæœ€åä¸€æ¬¡æ“ä½œæˆ–äº¤æ˜“æ—¶çš„ç‰¹å¾ def user_trans_behavior_feature(df, trans_op): print(\"Starting extract user's trans behavior......\") # è·å–ç¬¬ä¸€æ¬¡å’Œæœ€åä¸€æ¬¡è¡Œä¸º group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[-1]).to_dict() df['last_beahvior'] = df['user'].map(group_dic) group_dic = trans_op.groupby('user').apply(lambda x: x['property'].values[0]).to_dict() df['first_beahvior'] = df['user'].map(group_dic) # æ˜¯å¦æœ‰è¿‡äº¤æ˜“è¡Œä¸º group_dic = trans_op.groupby('user').apply(lambda x: judge_has_trans(x)).to_dict() df['has_trans'] = df['user'].map(group_dic) # æœ€åä¸€æ¬¡äº¤æ˜“days_diff group_dic = trans_op.groupby('user').apply(lambda x: last_trans_time(x)).to_dict() df['last_days_diff_trans'] = df['user'].map(group_dic) # ç¬¬ä¸€æ¬¡äº¤æ˜“days_diff group_dic = trans_op.groupby('user').apply(lambda x: first_trans_time(x)).to_dict() df['first_days_diff_trans'] = df['user'].map(group_dic) # æœ€åä¸€æ¬¡äº¤æ˜“hour group_dic = trans_op.groupby('user').apply(lambda x: last_trans_hour(x)).to_dict() df['last_hour_trans'] = df['user'].map(group_dic) # ç¬¬ä¸€æ¬¡äº¤æ˜“hour group_dic = trans_op.groupby('user').apply(lambda x: first_trans_hour(x)).to_dict() df['first_hour_trans'] = df['user'].map(group_dic) # æœ€åä¸€æ¬¡äº¤æ˜“week group_dic = trans_op.groupby('user').apply(lambda x: last_trans_week(x)).to_dict() df['last_week_trans'] = df['user'].map(group_dic) # ç¬¬ä¸€æ¬¡äº¤æ˜“week group_dic = trans_op.groupby('user').apply(lambda x: first_trans_week(x)).to_dict() df['first_week_trans'] = df['user'].map(group_dic) # æœ€åä¸€æ¬¡äº¤æ˜“timestamp group_dic = trans_op.groupby('user').apply(lambda x: last_trans_timestamp(x)).to_dict() df['last_time_trans'] = df['user'].map(group_dic) # ç¬¬ä¸€æ¬¡äº¤æ˜“timestamp group_dic = trans_op.groupby('user').apply(lambda x: first_trans_timestamp(x)).to_dict() df['first_time_trans'] = df['user'].map(group_dic) # å¹³å‡äº¤æ˜“æ¬¡æ•° group_dic = trans_op.groupby('user').apply(lambda x: gen_trans_count(x)).to_dict() df['trans_count'] = df['user'].map(group_dic) # æ“ä½œæ¬¡æ•° group_dic = trans_op.groupby('user').apply(lambda x: gen_op_count(x)).to_dict() df['op_count'] = df['user'].map(group_dic) return df 17.åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æœ‰äº¤æ˜“ data['has_trans'] = pd.factorize(data['has_trans'])[0] äºŒå€¼ç‰¹å¾ç”¨ä¸€åˆ—01å°±å¯ä»¥äº† 18.ç±»åˆ«ç¼–ç  def kfold_stats_feature(train, test, feats, k, seed): folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed) # è¿™é‡Œæœ€å¥½å’Œåé¢æ¨¡å‹çš„KæŠ˜äº¤å‰éªŒè¯ä¿æŒä¸€è‡´ train['fold'] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): train.loc[val_idx, 'fold'] = fold_ kfold_features = [] for feat in tqdm(feats, desc='Target encoding for base feature'): nums_columns = ['label'] for f in nums_columns: colname = feat + '_' + f + '_kfold_mean' kfold_features.append(colname) train[colname] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): tmp_trn = train.iloc[trn_idx] order_label = tmp_trn.groupby([feat])[f].mean() tmp = train.loc[train.fold == fold_, [feat]] train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label) # fillna global_mean = tmp_trn[f].mean() train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean) train[colname] = train[colname].astype(float) for f in nums_columns: colname = feat + '_' + f + '_kfold_mean' test[colname] = None order_label = train.groupby([feat])[f].mean() test[colname] = test[feat].map(order_label) # fillna global_mean = train[f].mean() test[colname] = test[colname].fillna(global_mean) test[colname] = test[colname].astype(float) del train['fold'] return train, test å¯¹ä¸€äº›å€¼ç±»åˆ«æ¯”è¾ƒå¤šçš„å˜é‡ä½œtarget encoding 19.å¯¹åºåˆ—å˜é‡ä½œtarget_encoding def target_encoding(file, train, test, feats, k, prefix, not_adp=True, agg_lst=['mean'], seed=2020): folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed) # è¿™é‡Œæœ€å¥½å’Œåé¢æ¨¡å‹çš„KæŠ˜äº¤å‰éªŒè¯ä¿æŒä¸€è‡´ train['fold'] = None for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): train.loc[val_idx, 'fold'] = fold_ tt_file = train[['user', 'label']].merge(file, on='user', how='left') te_features = [] for feat in tqdm(feats, desc='Target encoding for {} feature '.format(prefix)): col_name = feat + '_te' # te_features.append(col_name) for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])): tmp_users = train.iloc[trn_idx]['user'].values tmp_file = file[file.user.isin(tmp_users)] tmp_file = tmp_file.merge(train[['user', 'label']], on='user', how='left') if not_adp: match = tmp_file.groupby(feat)['label'].mean() else: match = tmp_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index() match['eu_mean'] = match['eu_sum'] / match['eu_count'] match = match['eu_mean'].groupby(match[feat]).mean() tmp_users = train.iloc[val_idx]['user'].values tmp_file = file[file.user.isin(tmp_users)] tmp_file[col_name] = tmp_file[feat].map(match) for agg_ in agg_lst: tmp = tmp_file.groupby('user')[col_name].agg(agg_) train.loc[train.fold == fold_, col_name + '_' + agg_] = train.loc[train.fold == fold_, 'user'].map(tmp) if not_adp: match = tt_file.groupby(feat)['label'].mean() else: match = tt_file.groupby([feat, 'user'])['label'].agg(eu_sum='sum', eu_count='count').reset_index() match['eu_mean'] = match['eu_sum'] / match['eu_count'] match = match['eu_mean'].groupby(match[feat]).mean() tmp_file = file[file.user.isin(test['user'].values)] tmp_file[col_name] = tmp_file[feat].map(match) for agg_ in agg_lst: tmp = tmp_file.groupby('user')[col_name].agg(agg_) test[col_name + '_' + agg_] = test['user'].map(tmp) del train['fold'] gc.collect() # print(train[te_features]) return train, test é¦–å…ˆè¦æŠŠåºåˆ—mergeä¸Šlabelå†è¿›è¡Œç¼–ç èšåˆåè¾“å‡ºå•è¡Œçš„ç¼–ç ç»“æœ 20.æœ‰åºå˜é‡ç¼–ç  def order_encode(df, col): \"\"\" :param df: Dataframe :param col: feature :description: å¯¹æœ‰åºç±»åˆ«å˜é‡é¡ºåºç¼–ç  :return: \"\"\" df.loc[df[col].notnull(), col] = df.loc[df[col].notnull(), col].apply(lambda x: str(x).split(' ')[1]).astype(int) df[col] = df[col].fillna(-1).astype(int) return df å¯¹äºä¸€äº›æœ‰åºçš„å˜é‡è¦è¿˜åŸä¸ºå¯¹åº”æ•°å€¼ä»¥æ•è·å¯¹åº”çš„å¤§å°å…³ç³»ã€‚ 21.æ— åºå˜é‡ç¼–ç  def label_encode(df, order_cols): # LabelEncoder cat_cols = [f for f in df.select_dtypes('object').columns if f not in ['user'] + order_cols] for col in cat_cols: le = LabelEncoder() df[col].fillna('-1', inplace=True) df[col] = le.fit_transform(df[col]) #cat_cols.append(col) return df 22.é£é™©å€¼è¾ƒé«˜çš„çœä»½äºŒå€¼åŒ–ç¼–ç  def province_binary(df, ): \"\"\"[summary] å¯¹é£é™©ç‡æ’åæœ€é«˜çš„äº”ä¸ªçœä»½è¿›è¡ŒäºŒå€¼åŒ–åŠç»„åˆç¼–ç  Parameters ---------- df : [dataæ•°æ®] [train or test] Returns ------- [DataFrame] [ç»è¿‡çœä»½äºŒå€¼åŒ–ç¼–ç çš„è®­ç»ƒé›†/æµ‹è¯•é›†] \"\"\" #çœä»½äºŒå€¼åŒ–ç¼–ç  df['is_21_province'] = df.apply(lambda x: 1 if x.province == 21 else 0, axis=1) df['is_26_province'] = df.apply(lambda x: 1 if x.province == 26 else 0, axis=1) df['is_30_province'] = df.apply(lambda x: 1 if x.province == 30 else 0, axis=1) df['is_20_province'] = df.apply(lambda x: 1 if x.province == 20 else 0, axis=1) df['is_16_province'] = df.apply(lambda x: 1 if x.province == 16 else 0, axis=1) df['binary_province'] = df['is_21_province'].map(str) + df['is_26_province'].map(str) + \\ df['is_30_province'].map(str) + df['is_20_province'].map(str) + df['is_16_province'].map(str) le = LabelEncoder() df['binary_province'].fillna('-1', inplace=True) df['binary_province'] = le.fit_transform(df['binary_province']) return df 23.äº¤æ˜“æ“ä½œæ¯”ä¾‹ train_data['trans_ratio'] = train_data['trans_count'] / train_data['op_count'] test_data['trans_ratio'] = test_data['trans_count'] / test_data['op_count'] åˆ°è¿™é‡Œï¼Œä½œè€…ç”¨åˆ°çš„å„ç§ç¼–ç æ–¹å¼å°±ä»‹ç»ç»“æŸäº†ã€‚å…¶å®ï¼Œè‡ªå·±å·²ç»ç”¨åˆ°äº†å¤§å¤šæ•°ç¼–ç æ–¹å¼ï¼Œä¸å¤šè‡ªå·±ç”¨çš„æ¯”è¾ƒæ— è„‘ï¼Œå¯¹å¯ä»¥ç”¨çš„å˜é‡è‡ªå·±éƒ½ç”¨äº†ã€‚è™½çƒ­ä¹Ÿæœ‰ä¸€äº›ç‰¹å¾é€‰æ‹©çš„æ–¹æ³•ï¼Œä½†å¥½åƒå¹¶ä¸æ˜¯é‚£ä¹ˆå®ç”¨ï¼Œé—®äº†ä¸€äº›å¤§ä½¬ï¼Œæ¨èçš„åšæ³•æ˜¯åšä¸€ç»„æˆ–ä¸€ä¸ªç‰¹å¾å°±å®éªŒä¸€ä¸‹ï¼Œç»“æœæå‡äº†å°±ä¿ç•™ã€‚å¯¹äºä¸€äº›ç±»åˆ«å˜é‡ï¼Œlgbä¸ç”¨onehotå˜é‡ã€‚ top2 top2çš„å¾ˆå¤šç‰¹å¾ç¼–ç æ–¹å¼å’Œtop1ç›¸è¿‘ï¼Œè¿™é‡Œä¸»è¦ä»‹ç»å…¶ç‰¹æ®Šçš„ç‰¹å¾å¤„ç†æ–¹å¼ã€‚","categories":[{"name":"æ•°æ®ç«èµ›","slug":"æ•°æ®ç«èµ›","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"}],"tags":[{"name":"æ•°æ®ç«èµ›","slug":"æ•°æ®ç«èµ›","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B/"}]},{"title":"01-ä¸šåŠ¡çŸ¥è¯†-å¦‚ä½•æ¢³ç†ä¸šåŠ¡é€»è¾‘","slug":"01-ä¸šåŠ¡çŸ¥è¯†-å¦‚ä½•æ¢³ç†ä¸šåŠ¡é€»è¾‘","date":"2020-09-08T00:39:24.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/09/08/01-ä¸šåŠ¡çŸ¥è¯†-å¦‚ä½•æ¢³ç†ä¸šåŠ¡é€»è¾‘/","link":"","permalink":"https://mz2sj.github.io/2020/09/08/01-%E4%B8%9A%E5%8A%A1%E7%9F%A5%E8%AF%86-%E5%A6%82%E4%BD%95%E6%A2%B3%E7%90%86%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91/","excerpt":"","text":"å› ä¸ºä¸æ‡‚ä¸šåŠ¡ï¼Œæ‰€ä»¥æ‰¾äº†é—¨è¯¾æ¥çœ‹çœ‹ï¼Œè®°å½•ä¸€ä¸‹è‡ªå·±çš„å†ç¨‹å§ï¼ æ‰€è°“ä¸ƒæ­¥æˆè¯—æ³•ï¼Œä¸€ä¸€é“æ¥~ æˆ‘ä»¬æŒ£ä»€ä¹ˆé’±ï¼ˆç»è¥æ¨¡å¼ï¼‰è€æ¿å†³å®š è¿™ä¸ªå¥½ç†è§£ä¸€ç‚¹ï¼Œä¼ä¸šå¯èƒ½åŒæ ·æ˜¯åšä¸€ä»¶äº‹ï¼Œä½†æ˜¯ä»–çš„ç›®æ ‡å¯èƒ½æ˜¯ä¸ä¸€æ ·çš„ã€‚ æ¯”è¾ƒç»å…¸çš„æ˜¯ä¸Šé¢è¿™ä¸ªä¾‹å­ï¼Œæ¯”å¦‚è§†é¢‘ç½‘ç«™ï¼ŒçœŸæ­£çš„ä½¿ç”¨è€…ä½¿ç”¨æˆ·ï¼Œç”¨æˆ·åªéœ€è¦çœ‹å¹¿å‘Šï¼Œè€Œå¹¿å‘Šä¸»å´æ˜¯çœŸæ­£ä»˜é’±çš„äººï¼Œè§†é¢‘ç½‘ç«™è¥æ”¶çš„å¤§å¤´ä¹Ÿæ˜¯å¹¿å‘Šè´¹ç”¨ã€‚ç”¨æˆ·æ˜¯ğŸï¼Œå¹¿å‘Šä¸»æ˜¯ğŸ•ã€‚ å†æ¯”å¦‚ä¿¡ç”¨å¡ä¸šåŠ¡çš„ä¾‹å­ï¼Œä¿¡ç”¨å¡ä¸­ä¹±æ”¶è´¹çš„æ”¶å…¥å æ¯”åªæœ‰2%ï¼ŒçœŸæ­£å æ”¶å…¥çš„å¤§å¤´æ˜¯ä¿¡è´·ã€‚ å¦‚ä½•å¼„æ¸…ä¼ä¸šçš„ç»è¥æ¨¡å¼å‘¢ï¼Ÿ 1.ä¸ä¸šå†…äººå£«å¤šäº¤æµã€‚ 2.å¤šçœ‹æ–°é—»ã€è´¢æŠ¥ã€è¡Œä¸šæ–‡ç«  3.ç«™åœ¨ä¼ä¸šç»è¥è€…çš„è§’åº¦æ€è€ƒé—®é¢˜ 4.å…³æ³¨å…¬å¸å†…éƒ¨é”€å”®æ¸ é“è€ƒæ ¸æŒ‡æ ‡ï¼Œå¹´æŠ¥ æˆ‘ä»¬æŒ£è°çš„é’±ï¼ˆç›®æ ‡å®¢æˆ·ï¼‰å¸‚åœºéƒ¨/äº§å“ è¿™ä¸ªéƒ¨åˆ†æ˜¯æ‰¾ğŸå’ŒğŸ•çš„é—®é¢˜ï¼Œä»¥ä¿¡ç”¨å¡é—®é¢˜ä¸ºä¾‹ï¼Œå¦‚ä½•æ‰¾åˆ°é‚£äº›ä¿¡è´·éœ€æ±‚æ—ºç››çš„ç¾¤ä½“å‘¢ï¼Ÿ ç›®æ ‡ç”¨æˆ·ç¾¤ä¾æ¬¡æ˜¯ï¼šä¼ä¸šç»è¥è€…ã€å°è€æ¿å•†æˆ·ã€é«˜çº§ç™½é¢†ã€å°ç™½é¢†ã€å±Œä¸ã€‚ é”å®šæ ¸å¿ƒç”¨æˆ·çš„åŸåˆ™ï¼š å¯¹ç”¨æˆ·è€Œè¨€ï¼šè¿™ä¸ªä¸šåŠ¡æ˜¯æˆ‘çš„åˆšæ€§éœ€æ±‚ï¼Œéç”¨ä¸å¯ å¯¹ä¼ä¸šè€Œè¨€ï¼šè¿™ä¸ªå®¢æˆ·æœ‰å¼ºå¤§çš„ä»˜é’±èƒ½åŠ›ã€‚ä¼ä¸šæ˜¯æœ‰é’±å°±è¦ä¸Šã€‚ ä¸ºä»€ä¹ˆå®¢æˆ·è¦ç”¨æˆ‘ä»¬çš„äº§å“ï¼ˆäº§å“å±æ€§ï¼‰å¸‚åœºéƒ¨/äº§å“ å¤§éƒ¨åˆ†äº§å“å¹¶éç”Ÿæ´»å¿…éœ€å“ï¼Œè¦åŸ¹å…»ç”¨æˆ·çš„ä½¿ç”¨ä¹ æƒ¯ã€‚æ¯”å¦‚ä¿¡ç”¨å¡ä¼šç»™ä½ ä¸€å®šé¢åº¦ï¼Œè®©ä½ å…ˆå…è´¹ç”¨çˆ½å‡ å¤©ã€åšä¸€äº›ä¼˜æƒ æ´»åŠ¨ï¼Œè®©ä½ æ„Ÿè§‰å¾ˆæ–¹ä¾¿ã€‚ ç¬¬ä¸€æ—¶é—´æ•è·ç”¨æˆ·çš„éœ€æ±‚ï¼Œç„¶åæ¨é”€è‡ªå·±çš„äº§å“ã€‚ å¦‚ä½•äº†è§£äº§å“å‘¢ï¼Ÿ 1.äº²èº«ä½“éªŒ 2.ç«™åœ¨å®¢æˆ·è§’åº¦ï¼Œæˆ‘è¿™ä¸ªäº§å“æ»¡è¶³äº†å®¢æˆ·å“ªäº›éœ€æ±‚ 3.ç»“åˆäº§å“ä½¿ç”¨åœºæ™¯ï¼Œåˆ†æäº§å“åŠŸèƒ½è®¾ç½®åŸå›  4.äº†è§£é…å¥—çš„è¿è¥æ¨å¹¿æ´»åŠ¨ï¼Œå…¨é¢è®¤è¯†äº§å“ å¦‚ä½•æ‰¾åˆ°å®¢æˆ· ï¼ˆé”€å”®æ¸ é“ï¼‰æ¸ é“/æ¨å¹¿ å¸¸è§„æ¸ é“æœ‰ï¼šé”€å”®é˜Ÿä¼ä¸šåŠ¡å‘˜ã€ç”µè¯é”€å”®ï¼ˆè¿™ä¸ªæœ€çƒ¦äº†ï¼‰ã€åˆ†è¡Œç½‘ç‚¹ã€ç½‘ç«™ã€‚ ç‰¹æ®Šæ¸ é“ï¼švipå‘— ä¼ ç»Ÿè¡Œä¸šï¼Œé—¨åº—ã€å–åœºå¾ˆé‡è¦ã€‚ å¦‚ä½•å¼„æ¸…é”€å”®æ¸ é“å‘¢ï¼Ÿ 1.æ”¶é›†äº§å“é”€å”®åœºæ‰€ï¼Œäº†è§£å¯èƒ½çš„é”€å”®æ¸ é“ã€‚ 2.åˆ†æ¸…æ¶ˆè´¹æ¨¡å¼æ˜¯B2Cè¿˜æ˜¯B2B2C 3.äº†è§£å„æ¸ é“æ¶ˆè´¹é‡ï¼Œé”€å”®é¢å æ¯” 4.äº†è§£é…åˆé”€å”®åšçš„æ¨å¹¿ã€å¹¿å‘Š ä¸ºä»€ä¹ˆå®¢æˆ·ä¸ç”¨åˆ«äººçš„äº§å“ï¼ˆå¸‚åœºç­–ç•¥ï¼‰å¸‚åœºéƒ¨/è¿è¥ è¿™æ˜¯ä¸ªç«äº‰ç­–ç•¥çš„é—®é¢˜ï¼Œæ˜¯å’ŒåŒè¡Œç«äº‰çš„é—®é¢˜ã€‚åˆ«äººç”¨æˆ‘çš„ä¸ç”¨ä½ çš„ï¼Œè‚¯å®šæ˜¯å› ä¸ºæˆ‘çš„å¥½ï¼Œå“ªé‡Œå¥½å‘¢ï¼š 1.äº§å“æ ¸å¿ƒåŠŸèƒ½å¼º 2.äº§å“é™„åŠ å€¼é«˜ 3.æä¿ƒé”€ã€æå®£ä¼ ã€ç²‰ä¸ç»è¥ ä¸»è¦ç«äº‰æ–¹å¼æœ‰ï¼š 1.çˆ†æ¬¾äº§å“ 2.äº§å“ç»„åˆ 3.ä¿ƒé”€æ´»åŠ¨ 4.å“ç‰Œå®£ä¼  5.ä¼šå‘˜ç­–ç•¥ å¼„æ¸…ç«äº‰ç­–ç•¥çš„æ–¹å¼ï¼š 1.æ”¶é›†åˆ«äººçš„è¥é”€æ´»åŠ¨ä¿¡æ¯ã€å“ç‰Œå¹¿å‘Šã€‚ 2.æ•´ç†ä¿¡æ¯ï¼Œå½’ç±»ï¼Œå¹¿å‘Šå±äºå“ªäº›ç±»åˆ«ï¼Œä¿ƒé”€æ´»åŠ¨å¯¹è±¡è§„åˆ™æ˜¯ä»€ä¹ˆã€‚ è°æ¥å¹²ï¼ˆéƒ¨é—¨åˆ†å·¥ï¼‰ çœ‹ä¸€çœ¼ä¼ ç»Ÿä¼ä¸šçš„æ¶æ„ å†çœ‹ä¸€çœ¼äº’è”ç½‘ä¼ä¸š å¹²äº†æŒ£äº†å¤šå°‘ï¼ˆè¥æ”¶æƒ…å†µï¼‰ é”€å”®æ”¶å…¥æ˜¯å¤§éƒ¨åˆ†å…¬å¸å”¯ä¸€åˆ©æ¶¦æ¥æºï¼Œæ‰€ä»¥ä¼ ç»Ÿä¼ä¸šæ¸ é“ååˆ†é‡è¦ã€‚ æˆæœ¬æ¥æºï¼š 1.æ¨å¹¿æˆæœ¬ 2.äº§å“é™ˆæœ¬ 3.è¿ä½œæˆæœ¬ï¼šå…¬å¸éƒ¨é—¨è¿ä½œ 4.æ¸ é“æˆæœ¬ï¼šå•ä½äº§å“åœ¨æ¸ é“é”€å”®çš„æˆæœ¬ï¼Œæ¨å¹¿æˆ–é”€å”® 5.æ–°å®¢æˆ·è·å–æˆæœ¬ äº†è§£è¥æ”¶å°±è¦å¤šçœ‹è´¢æŠ¥ï¼Œæ‰¾åˆ°å…¬å¸å„éƒ¨é—¨çš„æ—¥æœˆå­£æŠ¥ï¼Œä»ä¸­å‘ç°ç”¨æˆ·æ•°ã€ä»˜è´¹æƒ…å†µã€æ´»åŠ¨æŠ•å…¥ç­‰ä¿¡æ¯ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"ä¸šåŠ¡","slug":"ä¸šåŠ¡","permalink":"https://mz2sj.github.io/tags/%E4%B8%9A%E5%8A%A1/"},{"name":"è¯¾ç¨‹","slug":"è¯¾ç¨‹","permalink":"https://mz2sj.github.io/tags/%E8%AF%BE%E7%A8%8B/"}]},{"title":"01-sqlåˆ·é¢˜ç‰›å®¢","slug":"01-sqlåˆ·é¢˜ç‰›å®¢","date":"2020-08-25T15:22:16.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/08/25/01-sqlåˆ·é¢˜ç‰›å®¢/","link":"","permalink":"https://mz2sj.github.io/2020/08/25/01-sql%E5%88%B7%E9%A2%98%E7%89%9B%E5%AE%A2/","excerpt":"","text":"ä»Šå¤©æ˜¯åˆ·é¢˜è®°å½•çš„ç¬¬ä¸€å¤©ï¼ŒåŠ æ²¹å•¦ï¼Œå°å­Ÿå†²å†²å†²ï¼ï¼ï¼ æŸ¥æ‰¾æœ€æ™šå…¥èŒå‘˜å·¥çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä¸ºäº†å‡è½»å…¥é—¨éš¾åº¦ï¼Œç›®å‰æ‰€æœ‰çš„æ•°æ®é‡Œå‘˜å·¥å…¥èŒçš„æ—¥æœŸéƒ½ä¸æ˜¯åŒä¸€å¤©(sqliteé‡Œé¢çš„æ³¨é‡Šä¸º--,mysqlä¸ºcomment) CREATE TABLE employees ( `emp_no int(11) NOT NULL, -- 'å‘˜å·¥ç¼–å·'` `birth_date date NOT NULL,` `first_name varchar(14) NOT NULL,` `last_name varchar(16) NOT NULL,` `gender char(1) NOT NULL,` `hire_date date NOT NULLPRIMARY KEY (emp_no)); è‡ªå·±çš„å†™æ³•ï¼Œè€ƒè™‘çš„ä¸æ˜¯é‚£ä¹ˆå…¨é¢ï¼Œåªèƒ½æŸ¥å‡ºä¸€æ¡æ•°æ® SELECT * FROM employees ORDER BY hire_date DESC LIMIt 1; ä½†æ˜¯å‡å¦‚æœ€åä¸€å¤©æœ‰å¤šäººå…¥èŒï¼Œè¿™ç§å†™æ³•å°±holdä¸ä½äº†ã€‚ SELECT * FROM employees WHERE hire_date=(SELECT MAX(hire_date) FROM employees); æŸ¥æ‰¾å…¥èŒå‘˜å·¥æ—¶é—´æ’åå€’æ•°ç¬¬ä¸‰çš„å‘˜å·¥æ‰€æœ‰ä¿¡æ¯ï¼Œä¸ºäº†å‡è½»å…¥é—¨éš¾åº¦ï¼Œç›®å‰æ‰€æœ‰çš„æ•°æ®é‡Œå‘˜å·¥å…¥èŒçš„æ—¥æœŸéƒ½ä¸æ˜¯åŒä¸€å¤© CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è¿™é¢˜ä¸»è¦çš„è€ƒç‚¹æ˜¯æŸ¥æ‰¾å€’æ•°ç¬¬ä¸‰ï¼Œè¿™ç§å…³äºé¡ºåºçš„é—®é¢˜å¯ä»¥ç”¨ LIMIT(A,B)æ¥å¤„ç†ï¼Œè¡¨ç¤ºä»ç¬¬Aè¡Œå–ç¬¬Bä¸ªæ•°æ®ï¼ŒLIMIT(2,1)æ„æ€å°±æ˜¯ä»ç¬¬2ä¸ªæ•°æ®å–ç¬¬1ä¸ªæ•°æ®ï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸‰åã€‚ SELECT * FROM employees WHERE emp_no IN (SELECT emp_no FROM employees ORDER BY hire_date DESC LIMIT 2,1); æŸ¥æ‰¾å„ä¸ªéƒ¨é—¨å½“å‰(dept_manager.to_date='9999-01-01')é¢†å¯¼å½“å‰(salaries.to_date='9999-01-01')è–ªæ°´è¯¦æƒ…ä»¥åŠå…¶å¯¹åº”éƒ¨é—¨ç¼–å·dept_no(æ³¨:è¯·ä»¥salariesè¡¨ä¸ºä¸»è¡¨è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å‡ºç»“æœä»¥salaries.emp_noå‡åºæ’åºï¼Œå¹¶ä¸”è¯·æ³¨æ„è¾“å‡ºç»“æœé‡Œé¢dept_noåˆ—æ˜¯æœ€åä¸€åˆ—) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, -- 'å‘˜å·¥ç¼–å·', `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, -- 'éƒ¨é—¨ç¼–å·' `emp_no` int(11) NOT NULL, -- 'å‘˜å·¥ç¼–å·' `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); å¸¸è§„çš„è¿æ¥æŸ¥è¯¢ï¼Œæ³¨æ„è¾“å‡ºå°±å¯ä»¥äº† SELECT s.emp_no,s.salary,s.from_date,s.to_date,dm.dept_no FROM salaries s INNER JOIN dept_manager dm ON s.emp_no=dm.emp_no WHERE dm.to_date='9999-01-01' AND s.to_date='9999-01-01' ORDER BY s.emp_no ASC; æŸ¥æ‰¾æ‰€æœ‰å·²ç»åˆ†é…éƒ¨é—¨çš„å‘˜å·¥çš„last_nameå’Œfirst_nameä»¥åŠdept_no(è¯·æ³¨æ„è¾“å‡ºæè¿°é‡Œå„ä¸ªåˆ—çš„å‰åé¡ºåº) CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); ä¸¤ä¸ªè¡¨åªæœ‰ä¸€ä¸ªå…±åŒåˆ—ï¼Œç›´æ¥ä½¿ç”¨å†…è¿æ¥ã€‚æ­¤å¤–ï¼Œå› ä¸ºdept_no å·²ç»æŒ‡å®šï¼®ï¼¯ï¼´ ï¼®ï¼µï¼¬ï¼¬äº†æ‰€ä»¥é¢˜ç›®æ‰€æœ‰çš„å‘˜å·¥å¿…ç„¶éƒ½åˆ†é…äº†éƒ¨é—¨ã€‚ SELECT e.last_name,e.first_name,de.dept_no FROM employees e INNER JOIN dept_emp de ON e.emp_no=de.emp_no WHERE NOT (de.dept_no ISNULL); ï¼•. æŸ¥æ‰¾æ‰€æœ‰å‘˜å·¥çš„last_nameå’Œfirst_nameä»¥åŠå¯¹åº”éƒ¨é—¨ç¼–å·dept_noï¼Œä¹ŸåŒ…æ‹¬æš‚æ—¶æ²¡æœ‰åˆ†é…å…·ä½“éƒ¨é—¨çš„å‘˜å·¥(è¯·æ³¨æ„è¾“å‡ºæè¿°é‡Œå„ä¸ªåˆ—çš„å‰åé¡ºåº) CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è¿™é¢˜è¯´äº†åŒ…æ‹¬æš‚æ—¶æ²¡æœ‰åˆ†é…å…·ä½“éƒ¨é—¨çš„å‘˜å·¥,æ‰€æœ‰åªè¦æ˜¯å‘˜å·¥å°±å¾—æŸ¥å‡ºæ¥,æ‰€ä»¥å¯¹employeesä½¿ç”¨å·¦è¿æ¥å•¦,ä½¿ç”¨å†…è¿æ¥çš„è¯,æœ‰çš„å‘˜å·¥å¯èƒ½æ²¡æœ‰åˆ†é…éƒ¨é—¨,é‚£ä¹ˆéƒ¨é—¨è¡¨å°±ä¸ä¼šæœ‰ä»–,é‚£å†å†…è¿æ¥å°±æ‰¾ä¸åˆ°ä»–çš„æ•°æ®äº†. SELECT e.last_name,e.first_name,de.dept_no FROM employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no; æŸ¥æ‰¾æ‰€æœ‰å‘˜å·¥å…¥èŒæ—¶å€™çš„è–ªæ°´æƒ…å†µï¼Œç»™å‡ºemp_noä»¥åŠsalaryï¼Œ å¹¶æŒ‰ç…§emp_noè¿›è¡Œé€†åº(è¯·æ³¨æ„ï¼Œä¸€ä¸ªå‘˜å·¥å¯èƒ½æœ‰å¤šæ¬¡æ¶¨è–ªçš„æƒ…å†µ) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å‡ å¤©ä¸å†™æ‰‹æ„Ÿåˆç”Ÿç–äº†å‘¢ï¼é‡ç‚¹åœ¨äºåˆšå…¥èŒï¼Œå…¥èŒæ—¶é—´å°±æ˜¯hire_dateå‘€ SELECT e.emp_no,s.salary FROM employees e INNER JOIN salaries s ON e.emp_no=s.emp_no AND s.from_date=e.hire_date ORDER BY e.emp_no DESC; ä¹Ÿå¯ä»¥ç”¨èšåˆå‡½æ•°ï¼ŒHAVINGç”¨æ¥è¿›è¡Œè¿‡æ»¤ SELECT emp_no,salary FROM salaries GROUP BY emp_no HAVING from_date=MIN(from_date) ORDER BY emp_no DESC; æŸ¥æ‰¾è–ªæ°´å˜åŠ¨è¶…è¿‡15æ¬¡çš„å‘˜å·¥å·emp_noä»¥åŠå…¶å¯¹åº”çš„å˜åŠ¨æ¬¡æ•°t CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); è¿™é¢˜è€ƒå¯Ÿçš„è¿˜æ˜¯èšåˆå‡½æ•°ï¼Œæ³¨æ„é¢˜ç›®è¦æ±‚å˜åŠ¨æ¬¡æ•°t SELECT emp_no,COUNT(*) AS t FROM salaries GROUP BY emp_no HAVING t&gt;15; æ‰¾å‡ºæ‰€æœ‰å‘˜å·¥å½“å‰(to_date='9999-01-01')å…·ä½“çš„è–ªæ°´salaryæƒ…å†µï¼Œå¯¹äºç›¸åŒçš„è–ªæ°´åªæ˜¾ç¤ºä¸€æ¬¡,å¹¶æŒ‰ç…§é€†åºæ˜¾ç¤º CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å•çº¯çš„æŸ¥salaryè¡¨é‡Œæœ‰å¤šå°‘ä¸ªsalaryï¼Œç›¸åŒè–ªæ°´åªæ˜¾ç¤ºä¸€æ¬¡ï¼Œç”¨DISTINCTå¯ä»¥è§£å†³ SELECT DISTINCT(salary) FROM salaries WHERE to_date='9999-01-01' ORDER BY salary DESC; çœ‹è®¨è®ºåŒºè¯´DISTINCTæ¶ˆè€—èµ„æºè¾ƒå¤§ï¼Œç”¨GROUP BY è§£å†³æ•°æ®é‡å¤é—®é¢˜,æ³¨æ„GROUP BY è¦æ”¾åœ¨WHERE å­å¥åé¢ SELECT salary FROM salaries WHERE to_date='9999-01-01' GROUP BY salary ORDER BY salary DESC; è·å–æ‰€æœ‰éƒ¨é—¨å½“å‰(dept_manager.to_date='9999-01-01')managerçš„å½“å‰(salaries.to_date='9999-01-01')è–ªæ°´æƒ…å†µï¼Œç»™å‡ºdept_no, emp_noä»¥åŠsalary(è¯·æ³¨æ„ï¼ŒåŒä¸€ä¸ªäººå¯èƒ½æœ‰å¤šæ¡è–ªæ°´æƒ…å†µè®°å½•) CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); æ³¨æ„æ—¶é—´ï¼Œå…¶ä»–æ²¡ä»€ä¹ˆ SELECT dm.dept_no,s.emp_no,s.salary FROM dept_manager dm INNER JOIN salaries s ON dm.emp_no=s.emp_no WHERE dm.to_date='9999-01-01' AND s.to_date='9999-01-01'; è·å–æ‰€æœ‰émanagerçš„å‘˜å·¥emp_no CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è¿™ä¸ªé¢˜çš„ç‰¹ç‚¹æ˜¯è§£æ³•å¤šï¼Œå…ˆæ¥ç®€å•çš„ SELECT e.emp_no FROM employees e WHERE e.emp_no NOT IN (SELECT dm.emp_no FROM dept_manager dm); ä¹Ÿå¯ä»¥åˆ©ç”¨è¿æ¥æŸ¥è¯¢è·å–dept_noä¸ºç©ºçš„å‘˜å·¥ SELECT e.emp_no FROM employees e LEFT JOIN dept_manager dm ON e.emp_no=dm.emp_no WHERE dm.dept_no IS NULL; è¿˜æœ‰é›†åˆè¿ç®—çš„è§£æ³• EXCEPT é›†åˆå·®è¿ç®—|UNION é›†åˆå¹¶è¿ç®—|INTERSECT é›†åˆäº¤è¿ç®— SELECT emp_no FROM employees EXCEPT SELECT emp_no FROM dept_manager; è·å–æ‰€æœ‰å‘˜å·¥å½“å‰çš„(dept_manager.to_date='9999-01-01')managerï¼Œå¦‚æœå‘˜å·¥æ˜¯managerçš„è¯ä¸æ˜¾ç¤º(ä¹Ÿå°±æ˜¯å¦‚æœå½“å‰çš„manageræ˜¯è‡ªå·±çš„è¯ç»“æœä¸æ˜¾ç¤º)ã€‚è¾“å‡ºç»“æœç¬¬ä¸€åˆ—ç»™å‡ºå½“å‰å‘˜å·¥çš„emp_no,ç¬¬äºŒåˆ—ç»™å‡ºå…¶managerå¯¹åº”çš„emp_noã€‚ CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, -- 'æ‰€æœ‰çš„å‘˜å·¥ç¼–å·' `dept_no` char(4) NOT NULL, -- 'éƒ¨é—¨ç¼–å·' `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, -- 'éƒ¨é—¨ç¼–å·' `emp_no` int(11) NOT NULL, -- 'ç»ç†ç¼–å·' `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); å…³äºæ—¶é—´é™åˆ¶çš„å‘ï¼Œè¿˜æœ‰ä¸€ç‚¹å°±æ˜¯è¯¥å…³è”çš„é”®æ˜¯dept_no,æ˜¯æ ¹æ®dept_noæ¥æ‰¾ç›¸åŒéƒ¨é—¨çš„,å†æœ‰ä¸€ç‚¹å°±æ˜¯å­¦åˆ°äº†&lt;&gt;ä¸ç­‰äºã€‚NOT INçš„å†™æ³•ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œä½†æ˜¯è¦æ˜¯æŸ¥è¯¢çš„ç»“æœï¼Œæ¯”è¾ƒä¸¤ä¸ªå•ç‹¬å˜é‡ç›¸ç­‰ç”¨ä¸ç­‰äºå‘€ã€‚ SELECT de.emp_no AS emp_no,dm.emp_no AS manager_no FROM dept_emp de INNER JOIN dept_manager dm ON de.dept_no=dm.dept_no WHERE de.emp_no &lt;&gt; dm.emp_no AND dm.to_date='9999-01-01' è·å–æ‰€æœ‰éƒ¨é—¨ä¸­å½“å‰(dept_emp.to_date = '9999-01-01')å‘˜å·¥å½“å‰(salaries.to_date='9999-01-01')è–ªæ°´æœ€é«˜çš„ç›¸å…³ä¿¡æ¯ï¼Œç»™å‡ºdept_no, emp_noä»¥åŠå…¶å¯¹åº”çš„salaryï¼ŒæŒ‰ç…§éƒ¨é—¨å‡åºæ’åˆ—ã€‚ CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); ç”¨group_byåè€Œé”™äº†ï¼Œçœ‹ä¸‹åˆ«äººçš„è§£é‡Š ä½¿ç”¨group byå­å¥æ—¶ï¼Œselectå­å¥ä¸­åªèƒ½æœ‰èšåˆé”®ã€èšåˆå‡½æ•°ã€å¸¸æ•°ï¼Œå°±æ˜¯é™¤äº†å¸¸æ•°å’Œèšåˆå…¶ä»–é”®çš„å‡½æ•°å¤–ï¼Œä¸èƒ½æœ‰å…¶ä»–é”®ï¼Œæœ‰å…¶ä»–é”®ä¸€å®šè¦ä½¿ç”¨èšåˆã€‚emp_noå¹¶ä¸ç¬¦åˆè¿™ä¸ªè¦æ±‚ã€‚ ä»titlesè¡¨è·å–æŒ‰ç…§titleè¿›è¡Œåˆ†ç»„ï¼Œæ¯ç»„ä¸ªæ•°å¤§äºç­‰äº2ï¼Œç»™å‡ºtitleä»¥åŠå¯¹åº”çš„æ•°ç›®tã€‚ CREATE TABLE IF NOT EXISTS \"titles\" ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); ç®€å•çš„GROUP BY~ SELECT title,COUNT(*) AS t FROM titles GROUP BY title; æ³¨æ„å¯¹äºé‡å¤çš„emp_noè¿›è¡Œå¿½ç•¥(å³emp_noé‡å¤çš„titleä¸è®¡ç®—ï¼Œtitleå¯¹åº”çš„æ•°ç›®tä¸å¢åŠ )ã€‚ CREATE TABLE IF NOT EXISTS `titles` ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); é‡å¤çš„emp_noå¯ä»¥ç”¨DISTINCTåŒºåˆ†ï¼Œè®°ä½ä¸€ä¸ªç‚¹ï¼Œå»é‡ç”¨DISTINCT SELECT title,COUNT(DISTINCT emp_no) AS t FROM titles GROUP BY title HAVING t&gt;=2; æŸ¥æ‰¾employeesè¡¨æ‰€æœ‰emp_noä¸ºå¥‡æ•°ï¼Œä¸”last_nameä¸ä¸ºMary(æ³¨æ„å¤§å°å†™)çš„å‘˜å·¥ä¿¡æ¯ï¼Œå¹¶æŒ‰ç…§hire_dateé€†åºæ’åˆ—(é¢˜ç›®ä¸èƒ½ä½¿ç”¨modå‡½æ•°) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è¢«è¿™ä¸ªä¸èƒ½ä½¿ç”¨modå‡½æ•°ç»™å”¬ä½äº†ï¼Œå’±è¿˜å¯ä»¥ä½¿ç”¨%å˜› SELECT * FROM employees WHERE last_name!='Mary' AND emp_no%2==1 ORDER BY hire_date DESC; ç»Ÿè®¡å‡ºå½“å‰(titles.to_date='9999-01-01')å„ä¸ªtitleç±»å‹å¯¹åº”çš„å‘˜å·¥å½“å‰(salaries.to_date='9999-01-01')è–ªæ°´å¯¹åº”çš„å¹³å‡å·¥èµ„ã€‚ç»“æœç»™å‡ºtitleä»¥åŠå¹³å‡å·¥èµ„avgã€‚ CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); CREATE TABLE IF NOT EXISTS \"titles\" ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); GROUPY BY è¦æ”¾åœ¨WHEREä¹‹åï¼ŒWHEREå­å¥ä¸­ä¸èƒ½ä½¿ç”¨GROUPY BY SELECT t.title,AVG(s.salary) AS avg FROM salaries s INNER JOIN titles t ON s.emp_no=t.emp_no WHERE t.to_date='9999-01-01' AND s.to_date='9999-01-01' GROUP BY t.title; è·å–å½“å‰ï¼ˆto_date='9999-01-01'ï¼‰è–ªæ°´ç¬¬äºŒå¤šçš„å‘˜å·¥çš„emp_noä»¥åŠå…¶å¯¹åº”çš„è–ªæ°´salary CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å…ˆä¸Šä¸€ä¸ªé”™è¯¯åšæ³• SELECT emp_no,salary FROM salaries WHERE to_date='9999-01-01' ORDER by salary DESC LIMIT 1,1 å‡å¦‚æœ€å¤§å·¥èµ„åˆä¸¤ä¸ªæ˜¯ç›¸åŒçš„ï¼Œé‚£ä¹ˆè¿™æ ·æ‹¿åˆ°çš„æ•°æ®å°±ä¸æ˜¯ç¬¬äºŒå¤§çš„äº†ã€‚ä»¥åç¢°åˆ°è¿™ç§ç‰µæ‰¯å¯¼é¡ºåºçš„é¢˜ï¼Œè¦è€ƒè™‘èƒ½å¦æŠŠæ•°æ®æŸ¥å…¨æŸ¥å‡†ï¼Œç”¨GROUP BY æŸ¥åˆ°å€¼æ’åé¡ºåºçš„æ•°æ® SELECT emp_no,salary FROM salaries WHERE salary =(SELECT salary FROM salaries GROUP BY salary ORDER BY salary DESC LIMIT 1,1) AND to_date='9999-01-01'; æŸ¥æ‰¾å½“å‰è–ªæ°´(to_date='9999-01-01')æ’åç¬¬äºŒå¤šçš„å‘˜å·¥ç¼–å·emp_noã€è–ªæ°´salaryã€last_nameä»¥åŠfirst_nameï¼Œä½ å¯ä»¥ä¸ä½¿ç”¨order byå®Œæˆå— CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); æ’åºé¢˜ä¸è®©ç”¨ORDER BY,æ±‚ç¬¬äºŒå¤§salary,å°äºæœ€å¤§å€¼ä¸­çš„æœ€å¤§å€¼å°±æ˜¯ç¬¬äºŒå¤§å•¦. SELECT e.emp_no,MAX(s.salary),e.last_name,e.first_name FROM employees e INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.salary&lt;(SELECT MAX(salary) FROM salaries) AND s.to_date='9999-01-01'; å½“ç„¶è¿™ç§åªèƒ½æ±‚è§£ç¬¬äºŒé«˜ï¼Œä¸‹é¢è¿™ä¸ªæœ‰ç‚¹éš¾ï¼Œè¡¨å†…æ¡ä»¶è‡ªè¿æ¥ select e.emp_no,s.salary,e.last_name,e.first_name from employees e join salaries s on e.emp_no=s.emp_no and s.to_date='9999-01-01' and s.salary = ( select s1.salary from salaries s1 join salaries s2 on s1.salary&lt;=s2.salary and s1.to_date='9999-01-01' and s2.to_date='9999-01-01' group by s1.salary having count(distinct s2.salary)=2 ) æŸ¥æ‰¾æ‰€æœ‰å‘˜å·¥çš„last_nameå’Œfirst_nameä»¥åŠå¯¹åº”çš„dept_nameï¼Œä¹ŸåŒ…æ‹¬æš‚æ—¶æ²¡æœ‰åˆ†é…éƒ¨é—¨çš„å‘˜å·¥ CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è€ƒå¯Ÿå¤–è”ç»“ SELECT e.last_name,e.first_name,d.dept_name FROM (employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no) LEFT JOIN departments d ON d.dept_no=de.dept_no; ä¹Ÿå¯ä»¥è¿™æ ·å†™ SELECT e.last_name,e.first_name,d.dept_name FROM (employees e LEFT JOIN dept_emp de ON e.emp_no=de.emp_no) AS t LEFT JOIN departments d ON d.dept_no=t.dept_no; æŸ¥æ‰¾å‘˜å·¥ç¼–å·emp_noä¸º10001å…¶è‡ªå…¥èŒä»¥æ¥çš„è–ªæ°´salaryæ¶¨å¹…(æ€»å…±æ¶¨äº†å¤šå°‘)growth(å¯èƒ½æœ‰å¤šæ¬¡æ¶¨è–ªï¼Œæ²¡æœ‰é™è–ª) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); æ˜¯è®¡ç®—æ¶¨å¹…å‘€ï¼Œæœ€ç®€å•çš„ SELECT MAX(salary)-MIN(salary) AS growth FROM salaries WHERE emp_no=10001; ä¿é™©èµ·è§è¿˜æ˜¯æŒ‰ç…§æ—¥æœŸæ¥é€‰åˆå§‹å·¥èµ„å’Œè¿‘æœŸå·¥èµ„ SELECT (SELECT salary FROM salaries WHERE emp_no=10001 ORDER BY from_date DESC LIMIT 0,1) - (SELECT salary FROM salaries WHERE emp_no=10001 ORDER BY from_date ASC LIMIT 0,1) AS growth; æŸ¥æ‰¾æ‰€æœ‰å‘˜å·¥è‡ªå…¥èŒä»¥æ¥çš„è–ªæ°´æ¶¨å¹…æƒ…å†µï¼Œç»™å‡ºå‘˜å·¥ç¼–å·emp_noä»¥åŠå…¶å¯¹åº”çš„è–ªæ°´æ¶¨å¹…growthï¼Œå¹¶æŒ‰ç…§growthè¿›è¡Œå‡åº ï¼ˆæ³¨:å¯èƒ½æœ‰employeesè¡¨å’Œsalariesè¡¨é‡Œå­˜åœ¨è®°å½•çš„å‘˜å·¥ï¼Œæœ‰å¯¹åº”çš„å‘˜å·¥ç¼–å·å’Œæ¶¨è–ªè®°å½•ï¼Œä½†æ˜¯å·²ç»ç¦»èŒäº†ï¼Œç¦»èŒçš„å‘˜å·¥salariesè¡¨çš„æœ€æ–°çš„to_date!='9999-01-01'ï¼Œè¿™æ ·çš„æ•°æ®ä¸æ˜¾ç¤ºåœ¨æŸ¥æ‰¾ç»“æœé‡Œé¢ï¼‰ CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, -- 'å…¥èŒæ—¶é—´' PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, -- 'ä¸€æ¡è–ªæ°´è®°å½•å¼€å§‹æ—¶é—´' `to_date` date NOT NULL, -- 'ä¸€æ¡è–ªæ°´è®°å½•ç»“æŸæ—¶é—´' PRIMARY KEY (`emp_no`,`from_date`)); è¿™é¢˜çš„é‡ç‚¹æ˜¯æ‰¾åˆ°ç°åœ¨çš„å·¥èµ„å¯¹åº”çš„æ—¶é—´å’Œåˆåˆ°å…¬å¸å¯¹åº”çš„æ—¶é—´ï¼Œç„¶åè¿åˆƒè€Œè§£ã€‚æ³¨æ„hire_dateæ˜¯å‘˜å·¥è¢«é›‡ä½£çš„æ—¶é—´ã€‚ SELECT e.emp_no,a.salary-b.salary AS growth FROM employees e INNER JOIN salaries a ON e.emp_no=a.emp_no INNER JOIN salaries b ON e.hire_date=b.from_date WHERE a.to_date='9999-01-01' ORDER BY growth ASC; ç»Ÿè®¡å„ä¸ªéƒ¨é—¨çš„å·¥èµ„è®°å½•æ•°ï¼Œç»™å‡ºéƒ¨é—¨ç¼–ç dept_noã€éƒ¨é—¨åç§°dept_nameä»¥åŠéƒ¨é—¨åœ¨salariesè¡¨é‡Œé¢æœ‰å¤šå°‘æ¡è®°å½•sum CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); ç»Ÿè®¡å„ä¸ªéƒ¨é—¨ï¼Œç»Ÿè®¡å„ä¸ªéƒ¨é—¨ï¼Œæ˜æ˜¾æ˜¯æŒ‰æŸç±»è®¡ç®—è‚¯å®šè¦GROUP BY å•Šï¼Œä½ è¿™è„‘å­ SELECT d.dept_no,d.dept_name,COUNT(t.salary) AS sum FROM (salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no) AS t INNER JOIN departments d ON t.dept_no=d.dept_no GROUP BY d.dept_no; 23.âŒ å¯¹æ‰€æœ‰å‘˜å·¥çš„å½“å‰(to_date='9999-01-01')è–ªæ°´æŒ‰ç…§salaryè¿›è¡ŒæŒ‰ç…§1-Nçš„æ’åï¼Œç›¸åŒsalaryå¹¶åˆ—ä¸”æŒ‰ç…§emp_noå‡åºæ’åˆ— CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); è¿™é¢˜çš„éš¾ç‚¹æ˜¯å¦‚ä½•åœ¨ä¸åŒrank()å‡½æ•°çš„æƒ…å†µä¸‹è¿›è¡Œæ’åï¼Œæ’åºå…¶å®å¯ä»¥è½¬åŒ–ä¸ºå½“å‰å€¼å°äºå…¶ä»–å€¼çš„ä¸ªæ•°ï¼Œåˆå› ä¸ºsalaryæœ‰å¹¶åˆ—è¡Œä¸ºï¼Œæ‰€ä»¥è¿›è¡Œè®¡æ•°æ—¶è¦ä½¿ç”¨distinctï¼Œè‡³äºæœ‰çš„äººæåˆ°ä½¿ç”¨groupbyè¿™ç‚¹è¿˜æ²¡å¤ªçœ‹æ‡‚ã€‚ SELECT s1.emp_no,s1.salary , (SELECT COUNT(DISTINCT s2.salary) FROM salaries s2 WHERE s2.salary&gt;=s1.salary AND s2.to_date='9999-01-01') AS rank FROM salaries s1 WHERE s1.to_date='9999-01-01' ORDER BY rank,s1.emp_no ASC; èµ·å§‹sqlä¸­æœ‰å…³æ’åºçš„å‡½æ•°ä¹Ÿå¯ä»¥è§£å†³ä¸Šé¢çš„é—®é¢˜ select emp_no,salary, dense_rank() over (order by salary desc) as rank from salaries where to_date='9999-01-01' order by rank asc,emp_no asc; ä¸‹é¢ä»‹ç»å‡ ä¸ªå‡½æ•°çš„åŒºåˆ«: æ¯”å¦‚å¯¹åˆ†æ•°è¿›è¡Œæ’åï¼š90ã€85ã€85ã€70 ROW_NUMBER:1,2,3,4 RANK:1,2,2,4 DENSE_RANK:1ã€2ã€2ã€3 NTILEå‡½æ•°æ˜¯å°†æœ‰åºåˆ†åŒºä¸­çš„è¡Œåˆ†å‘åˆ°æŒ‡å®šæ•°ç›®çš„ç»„ä¸­ï¼Œå„ä¸ªç»„æœ‰ç¼–å·ï¼Œç¼–å·ä»1å¼€å§‹ï¼Œå°±åƒæˆ‘ä»¬è¯´çš„â€™åˆ†åŒºâ€™ä¸€æ · ï¼Œåˆ†ä¸ºå‡ ä¸ªåŒºï¼Œä¸€ä¸ªåŒºä¼šæœ‰å¤šå°‘ä¸ªã€‚ è·å–æ‰€æœ‰émanagerå‘˜å·¥å½“å‰çš„è–ªæ°´æƒ…å†µï¼Œç»™å‡ºdept_noã€emp_noä»¥åŠsalary ï¼Œå½“å‰è¡¨ç¤ºto_date='9999-01-01' CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); è¿™ä¸ªæ²¡æœ‰å¾ˆéš¾ï¼Œä½†æ˜¯å‰é¢salaryå°‘å†™äº†ä¸€ä¸ªå­—æ¯å°±è€½è¯¯äº†å¾ˆé•¿æ—¶é—´ã€‚è¿™é‡Œä¸èƒ½INNNER JOIN dept_managerï¼Œå¦åˆ™å°±åªç”Ÿä¸‹æ¥managerï¼Œå†™ä»£ç è¿˜æ˜¯è¦æœ‰é€»è¾‘ï¼Œä¸èƒ½çå†™ã€‚ SELECT de.dept_no,s.emp_no,s.salary FROM salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no AND s.to_date='9999-01-01' WHERE de.emp_no NOT IN (SELECT emp_no FROM dept_manager) AND de.to_date='9999-01-01'; 25.âŒ è·å–å‘˜å·¥å…¶å½“å‰çš„è–ªæ°´æ¯”å…¶managerå½“å‰è–ªæ°´è¿˜é«˜çš„ç›¸å…³ä¿¡æ¯ï¼Œå½“å‰è¡¨ç¤ºto_date='9999-01-01', ç»“æœç¬¬ä¸€åˆ—ç»™å‡ºå‘˜å·¥çš„emp_noï¼Œ ç¬¬äºŒåˆ—ç»™å‡ºå…¶managerçš„manager_noï¼Œ ç¬¬ä¸‰åˆ—ç»™å‡ºè¯¥å‘˜å·¥å½“å‰çš„è–ªæ°´emp_salary, ç¬¬å››åˆ—ç»™è¯¥å‘˜å·¥å¯¹åº”çš„managerå½“å‰çš„è–ªæ°´manager_salary CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `dept_manager` ( `dept_no` char(4) NOT NULL, `emp_no` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); çœ‹åˆ°è¿™ç§é¢˜ç›®è¦å­¦ä¼šåˆ†è§£ï¼Œdept_empå’Œsalariesèƒ½ç»„æˆå‘˜å·¥è–ªæ°´è¡¨ï¼Œdept_managerå’Œsalaiesèƒ½ç»„æˆç»ç†è–ªæ°´è¡¨ï¼Œä¸¤è€…éƒ½å…±æœ‰ä¸€ä¸ªdept_noï¼Œé‚£ä¹ˆåªè¦ä¸¤è€…dept_noç›¸åŒï¼Œå†æ¯”è¾ƒä¸¤è¡¨çš„è–ªæ°´å¤§å°å°±okäº†ã€‚ SELECT t1.emp_no,t2.emp_no AS manager_no,t1.salary AS emp_salary,t2.salary AS manager_salary FROM (SELECT s.emp_no,s.salary,de.dept_no FROM salaries s INNER JOIN dept_emp de ON s.emp_no=de.emp_no AND s.to_date='9999-01-01' AND de.to_date='9999-01-01') t1 INNER JOIN (SELECT s.emp_no,s.salary,dm.dept_no FROM salaries s INNER JOIN dept_manager dm ON s.emp_no=dm.emp_no AND s.to_date='9999-01-01' AND dm.to_date='9999-01-01') t2 ON t1.dept_no=t2.dept_no WHERE t1.salary&gt;t2.salary; è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç‚¹å°±æ˜¯JOINåçš„ONçš„è¡¨è¾¾å¼ä¸ä»…å±€é™äºç­‰äºè¿˜å¯ä»¥ä½¿ç”¨å¤§äºç­‰äºï¼Œå’Œwhereè¯­å¥èµ·åˆ°çš„ä½œç”¨ç›¸ä¼¼ã€‚æˆ–è€…ç”¨ä¸‹é¢è¿™ç§å†™æ³•ï¼Œå°†å¤æ‚æŸ¥è¯¢åˆ†ä¸ºä¸¤ä¸ªç®€å•æŸ¥è¯¢ï¼Œå†ç»„åˆã€‚salaryè¡¨å¯ä»¥å‡ºç°ä¸¤æ¬¡ï¼Œç”¨whereå­—å¥èµ·åˆ°ç±»ä¼¼joinçš„ä½œç”¨ã€‚ SELECT s1.emp_no,s2.emp_no AS manager_no,s1.salary AS emp_salary,s2.salary AS manager_salary FROM dept_emp de,dept_manager dm,salaries s1,salaries s2 WHERE de.emp_no=s1.emp_no AND dm.emp_no=s2.emp_no AND de.dept_no=dm.dept_no AND s1.salary&gt;s2.salary AND de.to_date='9999-01-01' AND dm.to_date='9999-01-01' AND s1.to_date='9999-01-01' AND s2.to_date='9999-01-01' æ±‡æ€»å„ä¸ªéƒ¨é—¨å½“å‰å‘˜å·¥çš„titleç±»å‹çš„åˆ†é…æ•°ç›®ï¼Œå³ç»“æœç»™å‡ºéƒ¨é—¨ç¼–å·dept_noã€dept_nameã€å…¶éƒ¨é—¨ä¸‹æ‰€æœ‰çš„å½“å‰(dept_emp.to_date = '9999-01-01')å‘˜å·¥çš„å½“å‰(titles.to_date = '9999-01-01')titleä»¥åŠè¯¥ç±»å‹titleå¯¹åº”çš„æ•°ç›®countï¼Œç»“æœæŒ‰ç…§dept_noå‡åºæ’åº (æ³¨ï¼šå› ä¸ºå‘˜å·¥å¯èƒ½æœ‰ç¦»èŒï¼Œæ‰€æœ‰dept_empé‡Œé¢to_dateä¸ä¸º'9999-01-01'å°±å·²ç»ç¦»èŒäº†ï¼Œä¸è®¡å…¥ç»Ÿè®¡ï¼Œè€Œä¸”å‘˜å·¥å¯èƒ½æœ‰æ™‹å‡ï¼Œæ‰€ä»¥å¦‚æœtitles.to_date ä¸ä¸º '9999-01-01'ï¼Œé‚£ä¹ˆè¿™ä¸ªå¯èƒ½æ˜¯å‘˜å·¥ä¹‹å‰çš„èŒä½ä¿¡æ¯ï¼Œä¹Ÿä¸è®¡å…¥ç»Ÿè®¡) CREATE TABLE `departments` ( `dept_no` char(4) NOT NULL, `dept_name` varchar(40) NOT NULL, PRIMARY KEY (`dept_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE IF NOT EXISTS `titles` ( `emp_no` int(11) NOT NULL, `title` varchar(50) NOT NULL, `from_date` date NOT NULL, `to_date` date DEFAULT NULL); è¾“å…¥æè¿°: è‡ªå·±ç«Ÿç„¶å†™å‡ºæ¥äº† SELECT de.dept_no,d.dept_name,t.title,COUNT(de.emp_no) as count FROM dept_emp de INNER JOIN titles t ON de.emp_no=t.emp_no INNER JOIN departments d ON de.dept_no=d.dept_no WHERE de.to_date='9999-01-01' AND t.to_date='9999-01-01' GROUP BY de.dept_no,t.title; è¿™é“é¢˜ä¹Ÿä¸éš¾ï¼Œæ— è„‘è¿æ¥ååŠ ä¸Šæ¡ä»¶å†groupbyå°±okäº†ï¼Œå€’æ˜¯groupbyçš„ç”¨æ³•è‡ªå·±åˆå¿«å¿˜å…‰äº†å‘¢ï¼ âŒ ç»™å‡ºæ¯ä¸ªå‘˜å·¥æ¯å¹´è–ªæ°´æ¶¨å¹…è¶…è¿‡5000çš„å‘˜å·¥ç¼–å·emp_noã€è–ªæ°´å˜æ›´å¼€å§‹æ—¥æœŸfrom_dateä»¥åŠè–ªæ°´æ¶¨å¹…å€¼salary_growthï¼Œå¹¶æŒ‰ç…§salary_growthé€†åºæ’åˆ—ã€‚ æç¤ºï¼šåœ¨sqliteä¸­è·å–datetimeæ—¶é—´å¯¹åº”çš„å¹´ä»½å‡½æ•°ä¸ºstrftime('%Y', to_date) (æ•°æ®ä¿è¯æ¯ä¸ªå‘˜å·¥çš„æ¯æ¡è–ªæ°´è®°å½•to_date-from_date=1å¹´ï¼Œè€Œä¸”åŒä¸€å‘˜å·¥çš„ä¸‹ä¸€æ¡è–ªæ°´è®°å½•from_data=ä¸Šä¸€æ¡è–ªæ°´è®°å½•çš„to_data) CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å¦‚ï¼šæ’å…¥ INSERT INTO salaries VALUES(10001,52117,'1986-06-26','1987-06-26'); INSERT INTO salaries VALUES(10001,62102,'1987-06-26','1988-06-25'); INSERT INTO salaries VALUES(10002,72527,'1996-08-03','1997-08-03'); INSERT INTO salaries VALUES(10002,72527,'1997-08-03','1998-08-03'); INSERT INTO salaries VALUES(10002,72527,'1998-08-03','1999-08-03'); INSERT INTO salaries VALUES(10003,43616,'1996-12-02','1997-12-02'); INSERT INTO salaries VALUES(10003,43466,'1997-12-02','1998-12-02'); å…ˆæ”¾ä¸Šè‡ªå·±çš„å†™æ³•ï¼Œå¯¹äºåŒä¸€ä¸ªäººç›¸é‚»ä¸¤æ¡è®°å½•çš„æ¯”è¾ƒæˆ‘ä»¬å¯ä»¥ç›´æ¥è®©åŒæ ·çš„è¡¨æŒ‰æŸç§æ¡ä»¶å†…è¿æ¥å°±ok SELECT s1.emp_no,s2.from_date,(s2.salary-s1.salary) AS salary_growth FROM salaries s1 INNER JOIN salaries s2 ON s1.emp_no=s2.emp_no AND s1.to_date=s2.from_date WHERE salary_growth&gt;5000 ORDER BY salary_growth DESC; è¿™é“é¢˜è¯„è®ºä¹Ÿç»™æˆ‘çœ‹æ™•äº†ï¼Œä¸»è¦æ˜¯å­˜åœ¨å¤šæ¬¡æ¶¨è–ªçš„é—®é¢˜ï¼Œæ¯”å¦‚è¯´åœ¨ä¸€å¹´å†…å¤šæ¬¡æ¶¨è–ªçš„é—®é¢˜ã€‚ç»™æˆ‘æ•´æ™•äº†ã€‚ SELECT s2.emp_no, s2.from_date, (s2.salary - s1.salary) AS salary_growth FROM salaries AS s1, salaries AS s2 WHERE s1.emp_no = s2.emp_no AND salary_growth &gt; 5000 AND (strftime(\"%Y\",s2.to_date) - strftime(\"%Y\",s1.to_date) = 1 OR strftime(\"%Y\",s2.from_date) - strftime(\"%Y\",s1.from_date) = 1 ) ORDER BY salary_growth DESC å†™ä¸Šå»å°±ç®—çœ‹ä¸€çœ‹strftimeçš„ç”¨æ³•å§ã€‚ 28.âŒ CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); æŸ¥æ‰¾æè¿°ä¿¡æ¯(film.description)ä¸­åŒ…å«robotçš„ç”µå½±å¯¹åº”çš„åˆ†ç±»åç§°(category.name)ä»¥åŠç”µå½±æ•°ç›®(count(film.film_id))ï¼Œè€Œä¸”è¿˜éœ€è¦è¯¥åˆ†ç±»åŒ…å«ç”µå½±æ€»æ•°é‡(count(film_category.category_id))&gt;=5éƒ¨ ç¬¬äºŒä¸ªæ¡ä»¶æœ‰ç‚¹æ— è¯­ï¼Œä¸æ˜¯å¤ªç†è§£,æ­£ç¡®ç­”æ¡ˆçš„ç†è§£æ˜¯è¿™ä¸¤ä¸ªæ¡ä»¶æ˜¯å¹¶åˆ—å…³ç³»ã€‚ select name,count(name) from film,film_category,category where film.description like '%robot%' and film.film_id= film_category.film_id and film_category.category_id= category.category_id and category.category_id in (select category_id from film_category group by category_id having count(film_id)&gt;=5) è‡ªå·±çš„æƒ³æ³•æ˜¯å…ˆæ»¡è¶³ç¬¬ä¸€ä¸ªæ¡ä»¶åï¼Œå†åˆ†ç»„ï¼Œä¸è¿‡é€šä¸è¿‡æµ‹è¯• SELECT c.name,COUNT(f.film_id) AS film_num FROM film f INNER JOIN film_category fc ON f.film_id=fc.film_id INNER JOIN category c ON c.category_id=fc.category_id WHERE f.description like '%robot%' GROUP BY c.category_id HAVING film_num&gt;=5; CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); ä½¿ç”¨joinæŸ¥è¯¢æ–¹å¼æ‰¾å‡ºæ²¡æœ‰åˆ†ç±»çš„ç”µå½±idä»¥åŠåç§° çœ‹æ¸…é¢˜ç›®ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºNULLï¼Œç”¨IS SELECT f.film_id,f.title FROM film f LEFT JOIN film_category fc ON f.film_id=fc.film_id WHERE fc.category_id IS NULL; CREATE TABLE IF NOT EXISTS film ( film_id smallint(5) NOT NULL DEFAULT '0', title varchar(255) NOT NULL, description text, PRIMARY KEY (film_id)); CREATE TABLE category ( category_id tinyint(3) NOT NULL , name varchar(25) NOT NULL, `last_update` timestamp, PRIMARY KEY ( category_id )); CREATE TABLE film_category ( film_id smallint(5) NOT NULL, category_id tinyint(3) NOT NULL, `last_update` timestamp); ä½ èƒ½ä½¿ç”¨å­æŸ¥è¯¢çš„æ–¹å¼æ‰¾å‡ºå±äºActionåˆ†ç±»çš„æ‰€æœ‰ç”µå½±å¯¹åº”çš„title,descriptionå— æ³¨æ„actionçš„æ¡ä»¶ SELECT f.title,f.description FROM film f INNER JOIN film_category fc ON f.film_id=fc.film_id INNER JOIN category c ON fc.category_id=c.category_id WHERE c.name='Action'; é¢˜ç›®è¿˜æåˆ°è¦ç”¨å­æŸ¥è¯¢çš„æ–¹å¼ SELECT title,description FROM film where film_id IN (SELECT film_id from film_category WHERE category_id in (SELECT category_id from category WHERE name='Action')) å°†employeesè¡¨çš„æ‰€æœ‰å‘˜å·¥çš„last_nameå’Œfirst_nameæ‹¼æ¥èµ·æ¥ä½œä¸ºNameï¼Œä¸­é—´ä»¥ä¸€ä¸ªç©ºæ ¼åŒºåˆ† (æ³¨ï¼šsqllite,å­—ç¬¦ä¸²æ‹¼æ¥ä¸º || ç¬¦å·ï¼Œä¸æ”¯æŒconcatå‡½æ•°ï¼Œmysqlæ”¯æŒconcatå‡½æ•°) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT last_name || ' ' || first_name FROM employees; å†çœ‹çœ‹CONCATç”¨æ³• CONCATæ–¹æ³•ï¼š select CONCAT(CONCAT(last_name,\" \"),first_name) as name from employees æˆ–è€… select CONCAT(last_name,\" \"ï¼Œfirst_name) as name from employees åˆ›å»ºä¸€ä¸ªactorè¡¨ï¼ŒåŒ…å«å¦‚ä¸‹åˆ—ä¿¡æ¯ åˆ—è¡¨ ç±»å‹ æ˜¯å¦ä¸ºNULL å«ä¹‰ actor_id smallint(5) not null ä¸»é”®id first_name varchar(45) not null åå­— last_name varchar(45) not null å§“æ° last_update date not null æ—¥æœŸ create table if not exists actor( actor_id smallint(5) not null , first_name varchar(45) not null, last_name varchar(45) not null, last_update timestamp not null default (datetime('now','localtime')), primary key(actor_id) ) é¢˜ç›®å·²ç»å…ˆæ‰§è¡Œäº†å¦‚ä¸‹è¯­å¥: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL) è¯·ä½ å¯¹äºè¡¨actoræ‰¹é‡æ’å…¥å¦‚ä¸‹æ•°æ®(ä¸èƒ½æœ‰2æ¡insertè¯­å¥å“¦!) actor_id first_name last_name last_update 1 PENELOPE GUINESS 2006-02-15 12:34:33 2 NICK WAHLBERG 2006-02-15 12:34:33 INSERT INTO actor VALUES (1,'PENELOPE','GUINESS','2006-02-15 12:34:33'), (2,'NICK','WAHLBERG','2006-02-15 12:34:33') é¢˜ç›®å·²ç»å…ˆæ‰§è¡Œäº†å¦‚ä¸‹è¯­å¥: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL); insert into actor values ('3', 'WD', 'GUINESS', '2006-02-15 12:34:33'); å¯¹äºè¡¨actoræ’å…¥å¦‚ä¸‹æ•°æ®,å¦‚æœæ•°æ®å·²ç»å­˜åœ¨ï¼Œè¯·å¿½ç•¥(ä¸æ”¯æŒä½¿ç”¨replaceæ“ä½œ) actor_id first_name last_name last_update â€˜3â€™ â€˜EDâ€™ â€˜CHASEâ€™ â€˜2006-02-15 12:34:33â€™ é¢˜ç›®å·²ç»å…ˆæ‰§è¡Œäº†å¦‚ä¸‹è¯­å¥: drop table if exists actor; CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update DATETIME NOT NULL); insert into actor values ('3', 'WD', 'GUINESS', '2006-02-15 12:34:33'); å¯¹äºè¡¨actoræ’å…¥å¦‚ä¸‹æ•°æ®,å¦‚æœæ•°æ®å·²ç»å­˜åœ¨ï¼Œè¯·å¿½ç•¥(ä¸æ”¯æŒä½¿ç”¨replaceæ“ä½œ) actor_id first_name last_name last_update â€˜3â€™ â€˜EDâ€™ â€˜CHASEâ€™ â€˜2006-02-15 12:34:33â€™ sqlite insert or ignore into actor values(3,'ED','CHASE','2006-02-15 12:34:33'); mysql insert IGNORE into actor values(3,'ED','CHASE','2006-02-15 12:34:33'); å¯¹äºå¦‚ä¸‹è¡¨actorï¼Œå…¶å¯¹åº”çš„æ•°æ®ä¸º: actor_id first_name last_name last_update 1 PENELOPE GUINESS 2006-02-15 12:34:33 2 NICK WAHLBERG 2006-02-15 12:34:33 è¯·ä½ åˆ›å»ºä¸€ä¸ªactor_nameè¡¨ï¼Œå¹¶ä¸”å°†actorè¡¨ä¸­çš„æ‰€æœ‰first_nameä»¥åŠlast_nameå¯¼å…¥è¯¥è¡¨. actor_nameè¡¨ç»“æ„å¦‚ä¸‹ï¼š åˆ—è¡¨ ç±»å‹ æ˜¯å¦ä¸ºNULL å«ä¹‰ first_name varchar(45) not null åå­— last_name varchar(45) not null å§“æ° ä¸€ç§å†™æ³•æ˜¯å…ˆå»ºè¡¨ï¼Œç„¶åæ’å…¥ CREATE TABLE actor_name( first_name varchar(45) not null, last_name varchar(45) not null); INSERT INTO actor_name SELECT first_name,last_name FROM actor; å¦ä¸€ç§æ˜¯ç”¨æŸ¥è¯¢åˆ°çš„æ•°æ®å»ºè¡¨ sqlite CREATE TABLE actor_name AS SELECT first_name,last_name FROM actor mysql CREATE TABLE actor_name SELECT first_name,last_name FROM actor; é’ˆå¯¹å¦‚ä¸‹è¡¨actorç»“æ„åˆ›å»ºç´¢å¼•ï¼š (æ³¨:åœ¨ SQLite ä¸­,é™¤äº†é‡å‘½åè¡¨å’Œåœ¨å·²æœ‰çš„è¡¨ä¸­æ·»åŠ åˆ—,ALTER TABLE å‘½ä»¤ä¸æ”¯æŒå…¶ä»–æ“ä½œï¼Œ mysqlæ”¯æŒALTER TABLEåˆ›å»ºç´¢å¼•) CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); å¯¹first_nameåˆ›å»ºå”¯ä¸€ç´¢å¼•uniq_idx_firstnameï¼Œå¯¹last_nameåˆ›å»ºæ™®é€šç´¢å¼•idx_lastname CREATE UNIQUE INDEX uniq_idx_firstname ON actor(first_name); CREATE INDEX idx_lastname ON actor(last_name); mysql ALTER TABLE actor ADD UNIQUE INDEX uniq_idx_firstname(first_name); ALTER TABLE actor ADD INDEX idx_lastname(last_name); é’ˆå¯¹actorè¡¨åˆ›å»ºè§†å›¾actor_name_viewï¼ŒåªåŒ…å«first_nameä»¥åŠlast_nameä¸¤åˆ—ï¼Œå¹¶å¯¹è¿™ä¸¤åˆ—é‡æ–°å‘½åï¼Œfirst_nameä¸ºfirst_name_vï¼Œlast_nameä¿®æ”¹ä¸ºlast_name_vï¼š CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); CREATE VIEW actor_name_view AS SELECT first_name first_name_v,last_name last_name_v FROM actor; CREATE VIEW actor_name_view (fist_name_v, last_name_v) AS SELECT first_name, last_name FROM actor é’ˆå¯¹salariesè¡¨emp_noå­—æ®µåˆ›å»ºç´¢å¼•idx_emp_noï¼ŒæŸ¥è¯¢emp_noä¸º10005, ä½¿ç”¨å¼ºåˆ¶ç´¢å¼•ã€‚ CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); create index idx_emp_no on salaries(emp_no); sqlite SELECT * FROM salaries INDEXED BY idx_emp_no WHERE emp_no=10005; mysql SELECT * FROM salaries FROCE INDEX BY idx_emp_no WHERE emp_no=10005; å­˜åœ¨actorè¡¨ï¼ŒåŒ…å«å¦‚ä¸‹åˆ—ä¿¡æ¯ï¼š CREATE TABLE actor ( actor_id smallint(5) NOT NULL PRIMARY KEY, first_name varchar(45) NOT NULL, last_name varchar(45) NOT NULL, last_update datetime NOT NULL); ç°åœ¨åœ¨last_updateåé¢æ–°å¢åŠ ä¸€åˆ—åå­—ä¸ºcreate_date, ç±»å‹ä¸ºdatetime, NOT NULLï¼Œé»˜è®¤å€¼ä¸ºâ€™2020-10-01 00:00:00â€™ ALTER TABLE actor ADD COLUMN create_date datetime not null default '2020-10-01 00:00:00' AFTER last_update; æ„é€ ä¸€ä¸ªè§¦å‘å™¨audit_logï¼Œåœ¨å‘employees_testè¡¨ä¸­æ’å…¥ä¸€æ¡æ•°æ®çš„æ—¶å€™ï¼Œè§¦å‘æ’å…¥ç›¸å…³çš„æ•°æ®åˆ°auditä¸­ã€‚ CREATE TABLE employees_test( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL ) CREATE TABLE audit( EMP_no INT NOT NULL, NAME TEXT NOT NULL ); beginå’Œendä¹‹é—´çš„è¯­å¥è¦ç”¨åˆ†å·ç»“æŸ CREATE TRIGGER audit_log AFTER INSERT ON employees_test begin INSERT INTO audit(EMP_no,NAME) VALUES (new.ID,new.NAME); end; åˆ é™¤emp_noé‡å¤çš„è®°å½•ï¼Œåªä¿ç•™æœ€å°çš„idå¯¹åº”çš„è®°å½•ã€‚ CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); å¯¹groupbyåˆäº§ç”Ÿäº†å¤§å¤§çš„ç–‘æƒ‘ï¼Œä¸æ˜¯è¯´selectåé¢åªèƒ½æ¥groupbyçš„ç›¸å…³å­—æ®µå— DELETE FROM titles_test WHERE id NOT IN ( SELECT MIN(id) FROM titles_test GROUP BY emp_no) ä¸¤ä¸¤æ¯”è¾ƒæ‰¾å‡ºæœ€å¤§å€¼æˆ–è€…æœ€å°å€¼ delete from titles_test where id in( select a.id from titles_test a,titles_test b where a.emp_no=b.emp_no and a.id&gt;b.id) å°†æ‰€æœ‰to_dateä¸º9999-01-01çš„å…¨éƒ¨æ›´æ–°ä¸ºNULL,ä¸” from_dateæ›´æ–°ä¸º2001-01-01ã€‚ CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); æ›´æ–°åçš„å€¼: titles_test è¡¨çš„å€¼ï¼š id emp_no title from_date to_date 1 10001 Senior Engineer 2001-01-01 NULL 2 10002 Staff 2001-01-01 NULL 3 10003 Senior Engineer 2001-01-01 NULL 4 10004 Senior Engineer 2001-01-01 NULL 5 10001 Senior Engineer 2001-01-01 NULL 6 10002 Staff 2001-01-01 NULL 7 10003 Senior Engineer 2001-01-01 NULL UPDATEè¯­å¥æ›´æ–°å¤šåˆ—ç”¨é€—å·éš”å¼€å“¦~ä¸æ˜¯ç”¨AND UPDATE titles_test SET to_date=NULL,from_date='2001-01-01' WHERE to_date='9999-01-01'; å°†id=5ä»¥åŠemp_no=10001çš„è¡Œæ•°æ®æ›¿æ¢æˆid=5ä»¥åŠemp_no=10005,å…¶ä»–æ•°æ®ä¿æŒä¸å˜ï¼Œä½¿ç”¨replaceå®ç°ï¼Œç›´æ¥ä½¿ç”¨updateä¼šæŠ¥é”™äº†ã€‚ CREATE TABLE titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); markä¸€ä¸‹replaceçš„ç”¨æ³• UPDATE titles_test SET emp_no=replace(emp_no,10001,10005) WHERE id=5; å…¨å­—æ®µæ›´æ–°æ›¿æ¢,ä¼šæ’å…¥ä¸€æ¡æ–°è®°å½•ã€‚å¹¶ä¸”è¦å°†æ‰€æœ‰å­—æ®µçš„å€¼å†™å‡ºï¼Œå¦åˆ™å°†ç½®ä¸ºç©ºã€‚ REPLACE INTO titles_test VALUES (5, 10005, 'Senior Engineer', '1986-06-26', '9999-01-01') å°†titles_testè¡¨åä¿®æ”¹ä¸ºtitles_2017ã€‚ CREATE TABLE IF NOT EXISTS titles_test ( id int(11) not null primary key, emp_no int(11) NOT NULL, title varchar(50) NOT NULL, from_date date NOT NULL, to_date date DEFAULT NULL); insert into titles_test values ('1', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('2', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('3', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('4', '10004', 'Senior Engineer', '1995-12-03', '9999-01-01'), ('5', '10001', 'Senior Engineer', '1986-06-26', '9999-01-01'), ('6', '10002', 'Staff', '1996-08-03', '9999-01-01'), ('7', '10003', 'Senior Engineer', '1995-12-03', '9999-01-01'); sqlite ALTER TABLE titles_test RENAME TO titles_2017; mysql alter table titles_test rename titles_2017 åœ¨auditè¡¨ä¸Šåˆ›å»ºå¤–é”®çº¦æŸï¼Œå…¶emp_noå¯¹åº”employees_testè¡¨çš„ä¸»é”®idã€‚ (ä»¥ä¸‹2ä¸ªè¡¨å·²ç»åˆ›å»ºäº†) CREATE TABLE employees_test( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL ); CREATE TABLE audit( EMP_no INT NOT NULL, create_date datetime NOT NULL ); å…ˆçœ‹mysqlçš„ç”¨æ³• ALTER TABLE audit ADD FOREIGN KEY (emp_no) REFERENCES employees_test (id); å†çœ‹sqlite,æ³¨æ„å¤–é”®æ ¼å¼ DROP TABLE audit; CREATE TABLE audit( emp_no INT NOT NULL, create_date datetime NOT NULL, FOREIGN KEY(emp_no) REFERENCES employees_test(id)) è¯·ä½ å†™å‡ºæ›´æ–°è¯­å¥ï¼Œå°†æ‰€æœ‰è·å–å¥–é‡‘çš„å‘˜å·¥å½“å‰çš„(salaries.to_date='9999-01-01')è–ªæ°´å¢åŠ 10%ã€‚(emp_bonusé‡Œé¢çš„emp_noéƒ½æ˜¯å½“å‰è·å¥–çš„æ‰€æœ‰å‘˜å·¥) create table emp_bonus( emp_no int not null, btype smallint not null); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å¦‚ï¼š INSERT INTO emp_bonus VALUES (10001,1); INSERT INTO salaries VALUES(10001,85097,'2001-06-22','2002-06-22'); INSERT INTO salaries VALUES(10001,88958,'2002-06-22','9999-01-01'); UPDATE salaries SET salary=salary*1.1 WHERE emp_no IN (SELECT emp_no FROM emp_bonus) AND to_date='9999-01-01'; å°†employeesè¡¨ä¸­çš„æ‰€æœ‰å‘˜å·¥çš„last_nameå’Œfirst_nameé€šè¿‡(')è¿æ¥èµ·æ¥ã€‚(sqliteä¸æ”¯æŒconcatï¼Œè¯·ç”¨||å®ç°ï¼Œmysqlæ”¯æŒconcat) CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); è¾“å‡ºæ ¼å¼: SELECT last_name || \"'\" || first_name FROM employees; SELECT CONCAT(last_name, '''', first_name) as name FROM employees; æŸ¥æ‰¾å­—ç¬¦ä¸²â€™10,A,Bâ€™ ä¸­é€—å·â€™,'å‡ºç°çš„æ¬¡æ•°cntã€‚ æŠ€å·§é¢˜å“ˆï¼Œå­¦ä¹ äº†LENGTH å’ŒREPLACEçš„ç”¨æ³• SELECT LENGTH('10,A,B' )-LENGTH(REPLACE('10,A,B',',','')) AS cnt è·å–Employeesä¸­çš„first_nameï¼ŒæŸ¥è¯¢æŒ‰ç…§first_nameæœ€åä¸¤ä¸ªå­—æ¯ï¼ŒæŒ‰ç…§å‡åºè¿›è¡Œæ’åˆ— CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT first_name FROM (SELECT first_name,SUBSTR(first_name,-2) AS first_name_2 FROM employees) ORDER BY first_name_2 ASC; SELECT first_name FROM employees ORDER BY substr(first_name,-2,2) asc; SELECT first_name from employees order by right(first_name, 2); æŒ‰ç…§dept_noè¿›è¡Œæ±‡æ€»ï¼Œå±äºåŒä¸€ä¸ªéƒ¨é—¨çš„emp_noæŒ‰ç…§é€—å·è¿›è¡Œè¿æ¥ï¼Œç»“æœç»™å‡ºdept_noä»¥åŠè¿æ¥å‡ºçš„ç»“æœemployees CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); è¾“å‡ºæ ¼å¼: è®°å½•ä¸€ä¸‹GROUP_CONCATçš„ç”¨æ³• sqlite SELECT dept_no,GROUP_CONCAT(emp_no) AS employees FROM dept_emp GROUP BY dept_no; mysql SELECT dept_no, group_concat(DISTINCT emp_no ORDER BY emp_no ASC SEPARATOR ',') AS employees FROM dept_emp GROUP BY dept_no; æŸ¥æ‰¾æ’é™¤æœ€å¤§ã€æœ€å°salaryä¹‹åçš„å½“å‰(to_date = '9999-01-01' )å‘˜å·¥çš„å¹³å‡å·¥èµ„avg_salaryã€‚ CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); å…³äºæ—¶é—´çš„é™åˆ¶ SELECT AVG(salary) AS avg_salary FROM salaries WHERE to_date = '9999-01-01' AND salary NOT IN (SELECT MAX(salary) FROM salaries WHERE to_date = '9999-01-01') AND salary NOT IN (SELECT MIN(salary) FROM salaries WHERE to_date = '9999-01-01') SELECT AVG(salary) FROM salaries WHERE salary&lt;&gt;(SELECT MAX(salary) FROM salaries WHERE to_date='9999-01-01') AND salary&lt;&gt;(SELECT MIN(salary) FROM salaries WHERE to_date='9999-01-01') AND to_date='9999-01-01' åˆ†é¡µæŸ¥è¯¢employeesè¡¨ï¼Œæ¯5è¡Œä¸€é¡µï¼Œè¿”å›ç¬¬2é¡µçš„æ•°æ® CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); LIMIT åçš„æ•°å­—ä»£è¡¨è¿”å›å‡ æ¡è®°å½•ï¼ŒOFFSET åçš„æ•°å­—ä»£è¡¨ä»ç¬¬å‡ æ¡è®°å½•å¼€å§‹è¿”å› SELECT * FROM employees LIMIT 5 OFFSET 5; åœ¨ LIMIT X,Y ä¸­ï¼ŒYä»£è¡¨è¿”å›å‡ æ¡è®°å½•ï¼ŒXä»£è¡¨ä»ç¬¬å‡ æ¡è®°å½•å¼€å§‹è¿”å›ï¼ˆç¬¬ä¸€æ¡è®°å½•åºå·ä¸º0ï¼‰ï¼Œåˆ‡å‹¿è®°åã€‚ è·å–æ‰€æœ‰å‘˜å·¥çš„emp_noã€éƒ¨é—¨ç¼–å·dept_noä»¥åŠå¯¹åº”çš„bonusç±»å‹btypeå’Œreceivedï¼Œæ²¡æœ‰åˆ†é…å¥–é‡‘çš„å‘˜å·¥ä¸æ˜¾ç¤ºå¯¹åº”çš„bonusç±»å‹btypeå’Œreceived CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); CREATE TABLE `emp_bonus`( emp_no int(11) NOT NULL, received datetime NOT NULL, btype smallint(5) NOT NULL); CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); SELECT e.emp_no,de.dept_no,eb.btype,eb.received FROM employees e INNER JOIN dept_emp de ON e.emp_no=de.emp_no LEFT JOIN emp_bonus eb ON e.emp_no=eb.emp_no ä½¿ç”¨å«æœ‰å…³é”®å­—existsæŸ¥æ‰¾æœªåˆ†é…å…·ä½“éƒ¨é—¨çš„å‘˜å·¥çš„æ‰€æœ‰ä¿¡æ¯ã€‚ CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); è¿™ä¸ªEXISTSå½“ä½œæ¡ä»¶åˆ¤æ–­å¾—ç†è§£ç†è§£ INæ˜¯å…ˆæ‰§è¡Œå­æŸ¥è¯¢ï¼Œå¾—åˆ°ä¸€ä¸ªç»“æœé›†ï¼Œå°†ç»“æœé›†ä»£å…¥å¤–å±‚è°“è¯æ¡ä»¶æ‰§è¡Œä¸»æŸ¥è¯¢ï¼Œå­æŸ¥è¯¢åªéœ€è¦æ‰§è¡Œä¸€æ¬¡ EXISTSæ˜¯å…ˆä»ä¸»æŸ¥è¯¢ä¸­å–å¾—ä¸€æ¡æ•°æ®ï¼Œå†ä»£å…¥åˆ°å­æŸ¥è¯¢ä¸­ï¼Œæ‰§è¡Œä¸€æ¬¡å­æŸ¥è¯¢ï¼Œåˆ¤æ–­å­æŸ¥è¯¢æ˜¯å¦èƒ½è¿”å›ç»“æœï¼Œä¸»æŸ¥è¯¢æœ‰å¤šå°‘æ¡æ•°æ®ï¼Œå­æŸ¥è¯¢å°±è¦æ‰§è¡Œå¤šå°‘æ¬¡,EXISTSä¸­è¦æ·»åŠ åˆ¤æ–­æ¡ä»¶ï¼Ÿï¼Ÿï¼Ÿ SELECT * FROM employees WHERE NOT EXISTS (SELECT emp_no FROM dept_emp WHERE employees.emp_no=dept_emp.emp_no) è·å–æœ‰å¥–é‡‘çš„å‘˜å·¥ç›¸å…³ä¿¡æ¯ã€‚ CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); CREATE TABLE `dept_emp` ( `emp_no` int(11) NOT NULL, `dept_no` char(4) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`dept_no`)); create table emp_bonus( emp_no int not null, received datetime not null, btype smallint not null); CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); ç»™å‡ºemp_noã€first_nameã€last_nameã€å¥–é‡‘ç±»å‹btypeã€å¯¹åº”çš„å½“å‰è–ªæ°´æƒ…å†µsalaryä»¥åŠå¥–é‡‘é‡‘é¢bonusã€‚ bonusç±»å‹btypeä¸º1å…¶å¥–é‡‘ä¸ºè–ªæ°´salaryçš„10%ï¼Œbtypeä¸º2å…¶å¥–é‡‘ä¸ºè–ªæ°´çš„20%ï¼Œå…¶ä»–ç±»å‹å‡ä¸ºè–ªæ°´çš„30%ã€‚ å½“å‰è–ªæ°´è¡¨ç¤ºto_date='9999-01-01' SELECT e.emp_no,e.first_name,e.last_name,eb.btype,s.salary,s.salary*eb.btype*0.1 as bonus FROM employees e INNER JOIN emp_bonus eb ON e.emp_no=eb.emp_no INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.to_date='9999-01-01' çœ‹çœ‹CASE WHEN THEN ENDçš„ç”¨æ³• SELECT e.emp_no,e.first_name,e.last_name,eb.btype,s.salary, (CASE eb.btype WHEN 1 THEN s.salary*0.1 WHEN 2 THEN s.salary*0.2 WHEN 3 THEN s.salary*0.3 END) AS bonus FROM employees e INNER JOIN emp_bonus eb ON e.emp_no=eb.emp_no INNER JOIN salaries s ON e.emp_no=s.emp_no WHERE s.to_date='9999-01-01' æŒ‰ç…§salaryçš„ç´¯è®¡å’Œrunning_totalï¼Œå…¶ä¸­running_totalä¸ºå‰Nä¸ªå½“å‰( to_date = '9999-01-01')å‘˜å·¥çš„salaryç´¯è®¡å’Œï¼Œå…¶ä»–ä»¥æ­¤ç±»æ¨ã€‚ å…·ä½“ç»“æœå¦‚ä¸‹Demoå±•ç¤ºã€‚ã€‚ CREATE TABLE `salaries` ( `emp_no` int(11) NOT NULL, `salary` int(11) NOT NULL, `from_date` date NOT NULL, `to_date` date NOT NULL, PRIMARY KEY (`emp_no`,`from_date`)); è¾“å‡ºæ ¼å¼: è®©æˆ‘ä»¬æ¥å­¦ä¹ ä¸€ç§æ–°çš„æ±‚å’Œæ–¹æ³•ï¼Œä¸¤è¡¨ç›¸åŒæŒ‰æŸåˆ—æ‹å¥½åºåï¼Œæ¯”è¾ƒå¤§å°å³å¯æ±‚å’Œã€‚ SELECT emp_no,salary, (select sum(s1.salary) FROM salaries s1 WHERE s1.emp_no&lt;=s2.emp_no AND s1.to_date='9999-01-01') AS running_total FROM salaries s2 WHERE to_date='9999-01-01' ORDER BY s2.emp_no SELECT emp_no,salary, SUM(salary) OVER(ORDER BY emp_no) AS running_total FROM salaries WHERE to_date= '9999-01-01'; 58.âŒ å¯¹äºemployeesè¡¨ä¸­ï¼Œè¾“å‡ºfirst_nameæ’å(æŒ‰first_nameå‡åºæ’åº)ä¸ºå¥‡æ•°çš„first_name CREATE TABLE `employees` ( `emp_no` int(11) NOT NULL, `birth_date` date NOT NULL, `first_name` varchar(14) NOT NULL, `last_name` varchar(16) NOT NULL, `gender` char(1) NOT NULL, `hire_date` date NOT NULL, PRIMARY KEY (`emp_no`)); å¯¹äºè¿™ä¸ªCOUNTçš„ç”¨æ³•è¿˜æ˜¯å­˜ç–‘(èšåˆæˆ–æ˜¯æ»¡è¶³æŸç§æ¡ä»¶ä¹‹åï¼Ÿï¼Ÿï¼Ÿ)ï¼Œå¯ä»¥ç”¨äºè®¡æ•°ï¼Œä¸¤ä¸¤æ¯”è¾ƒä¹‹é—´çš„è®¡æ•°ï¼Œç”¨äºæ’åº SELECT first_name FROM employees e1 WHERE (SELECT COUNT(*) FROM employees e2 WHERE e1.first_name&gt;=e2.first_name)%2=1; ç”¨row_number() over è¿›è¡Œæ’åº select tt.first_name from (select first_name, row_number() over (order by first_name) as row_idx from employees) as tt where tt.row_idx%2=1; SELECT e.first_name FROM employees e JOIN ( SELECT first_name , ROW_NUMBER() OVER(ORDER BY first_name ASC) AS r_num FROM employees ) AS t ON e.first_name = t.first_name WHERE t.r_num % 2 = 1; åœ¨ç‰›å®¢åˆ·é¢˜çš„å°ä¼™ä¼´ä»¬éƒ½æœ‰ç€ç‰›å®¢ç§¯åˆ†ï¼Œç§¯åˆ†(grade)è¡¨ç®€åŒ–å¯ä»¥å¦‚ä¸‹: SELECT number FROM grade GROUP BY number HAVING COUNT(id)&gt;=3 è¿™ä¸ªæ€è·¯ä¹Ÿå¾ˆç‰›é€¼:è¦ä¸‰æ¬¡ä»¥ä¸Šçš„ç§¯åˆ†ï¼Œé‚£ä¹ˆè‚¯å®šè¦æŸ¥æ‰¾3ä¸ªidä¸åŒä½†æ˜¯ç§¯åˆ†ç›¸åŒçš„æƒ…å†µ SELECT DISTINCT g1.number AS times FROM grade g1, grade g2, grade g3 WHERE g1.id != g2.id AND g2.id != g3.id AND g1.id !=g3.id AND g1.number = g2.number AND g2.number = g3.number 60.âŒ åœ¨ç‰›å®¢åˆ·é¢˜æœ‰ä¸€ä¸ªé€šè¿‡é¢˜ç›®ä¸ªæ•°çš„(passing_number)è¡¨ï¼Œidæ˜¯ä¸»é”®ï¼Œç®€åŒ–å¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º1çš„ç”¨æˆ·é€šè¿‡äº†4ä¸ªé¢˜ç›®; â€¦ ç¬¬6è¡Œè¡¨ç¤ºidä¸º6çš„ç”¨æˆ·é€šè¿‡äº†4ä¸ªé¢˜ç›®; è¯·ä½ æ ¹æ®ä¸Šè¡¨ï¼Œè¾“å‡ºé€šè¿‡çš„é¢˜ç›®çš„æ’åï¼Œé€šè¿‡é¢˜ç›®ä¸ªæ•°ç›¸åŒçš„ï¼Œæ’åç›¸åŒï¼Œæ­¤æ—¶æŒ‰ç…§idå‡åºæ’åˆ—ï¼Œæ•°æ®å¦‚ä¸‹: SELECT id,number,DENSE_RANK() OVER (ORDER BY number DESC) AS t_rank FROM passing_number ORDER BY t_rank,id ASC; è¿™ç§åšæ³•ä¹Ÿå¾—ç†è§£ç†è§£ select a.id,a.number, (select count(distinct b.number) from passing_number b where b.number&gt;=a.number ) from passing_number a order by a.number desc, a.id asc; æœ‰ä¸€ä¸ªpersonè¡¨ï¼Œä¸»é”®æ˜¯idï¼Œå¦‚ä¸‹: æœ‰ä¸€ä¸ªä»»åŠ¡(task)è¡¨å¦‚ä¸‹ï¼Œä¸»é”®ä¹Ÿæ˜¯idï¼Œå¦‚ä¸‹: è¯·ä½ æ‰¾åˆ°æ¯ä¸ªäººçš„ä»»åŠ¡æƒ…å†µï¼Œå¹¶ä¸”è¾“å‡ºå‡ºæ¥ï¼Œæ²¡æœ‰ä»»åŠ¡çš„ä¹Ÿè¦è¾“å‡ºï¼Œè€Œä¸”è¾“å‡ºç»“æœæŒ‰ç…§personçš„idå‡åºæ’åºï¼Œè¾“å‡ºæƒ…å†µå¦‚ä¸‹: SELECT person.id,person.name,task.content FROM person LEFT JOIN task ON person.id=task.person_id ORDER BY person.id ASC; ç°åœ¨æœ‰ä¸€ä¸ªéœ€æ±‚ï¼Œè®©ä½ ç»Ÿè®¡æ­£å¸¸ç”¨æˆ·å‘é€ç»™æ­£å¸¸ç”¨æˆ·é‚®ä»¶å¤±è´¥çš„æ¦‚ç‡: æœ‰ä¸€ä¸ªé‚®ä»¶(email)è¡¨ï¼Œidä¸ºä¸»é”®ï¼Œ typeæ˜¯æšä¸¾ç±»å‹ï¼Œæšä¸¾æˆå‘˜ä¸º(completedï¼Œno_completed)ï¼Œcompletedä»£è¡¨é‚®ä»¶å‘é€æ˜¯æˆåŠŸçš„ï¼Œno_completedä»£è¡¨é‚®ä»¶æ˜¯å‘é€å¤±è´¥çš„ã€‚ç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºä¸ºidä¸º2çš„ç”¨æˆ·åœ¨2020-01-11æˆåŠŸå‘é€äº†ä¸€å°é‚®ä»¶ç»™äº†idä¸º3çš„ç”¨æˆ·; â€¦ ç¬¬3è¡Œè¡¨ç¤ºä¸ºidä¸º1çš„ç”¨æˆ·åœ¨2020-01-11æ²¡æœ‰æˆåŠŸå‘é€ä¸€å°é‚®ä»¶ç»™äº†idä¸º4çš„ç”¨æˆ·; â€¦ ç¬¬6è¡Œè¡¨ç¤ºä¸ºidä¸º4çš„ç”¨æˆ·åœ¨2020-01-12æˆåŠŸå‘é€äº†ä¸€å°é‚®ä»¶ç»™äº†idä¸º1çš„ç”¨æˆ·; ä¸‹é¢æ˜¯ä¸€ä¸ªç”¨æˆ·(user)è¡¨ï¼Œidä¸ºä¸»é”®ï¼Œis_blacklistä¸º0ä»£è¡¨ä¸ºæ­£å¸¸ç”¨æˆ·ï¼Œis_blacklistä¸º1ä»£è¡¨ä¸ºé»‘åå•ç”¨æˆ·ï¼Œç®€å†µå¦‚ä¸‹: ç»“æœè¡¨ç¤º: 2020-01-11å¤±è´¥çš„æ¦‚ç‡ä¸º0.500ï¼Œå› ä¸ºemailçš„ç¬¬1æ¡æ•°æ®ï¼Œå‘é€çš„ç”¨æˆ·idä¸º2æ˜¯é»‘åå•ç”¨æˆ·ï¼Œæ‰€ä»¥ä¸è®¡å…¥ç»Ÿè®¡ï¼Œæ­£å¸¸ç”¨æˆ·å‘æ­£å¸¸ç”¨æˆ·æ€»å…±2æ¬¡ï¼Œä½†æ˜¯å¤±è´¥äº†1æ¬¡ï¼Œæ‰€ä»¥æ¦‚ç‡æ˜¯0.500; 2020-01-12æ²¡æœ‰å¤±è´¥çš„æƒ…å†µï¼Œæ‰€ä»¥æ¦‚ç‡ä¸º0.000. (æ³¨æ„: sqlite 1/2å¾—åˆ°çš„ä¸æ˜¯0.5ï¼Œå¾—åˆ°çš„æ˜¯0ï¼Œåªæœ‰1*1.0/2æ‰ä¼šå¾—åˆ°0.5ï¼Œsqliteå››èˆäº”å…¥çš„å‡½æ•°ä¸ºround) æ³¨æ„å…ˆä¹˜1.0ï¼Œä¸ç„¶ä¼šå¾—åˆ°0 SELECT e.date,ROUND( SUM(CASE e.type WHEN 'completed' THEN 0 WHEN 'no_completed' THEN 1 END)*1.0/COUNT(e.type),3) AS p FROM email e INNER JOIN user u1 ON e.send_id=u1.id INNER JOIN user u2 ON e.receive_id=u2.id WHERE u1.is_blacklist=0 AND u2.is_blacklist=0 GROUP BY e.date ORDER BY e.date; ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªç”¨æˆ·æœ€è¿‘ç™»å½•æ˜¯å“ªä¸€å¤©ã€‚ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ ã€‚ã€‚ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º3çš„ç”¨æˆ·åœ¨2020-10-13ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·æœ€è¿‘ä¸€å¤©ç™»å½•çš„æ—¥å­ï¼Œå¹¶ä¸”æŒ‰ç…§user_idå‡åºæ’åºï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹: è€å¸ˆè¯´è¿‡select çš„å­—æ®µè¦åŒ…å«åœ¨groupæˆ–è€…è¦ç”¨èšåˆå‡½æ•° SELECT user_id,MAX(date) d FROM login GROUP BY user_id ORDER BY user_id ASC; ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªç”¨æˆ·æœ€è¿‘ç™»å½•æ˜¯å“ªä¸€å¤©ï¼Œç”¨çš„æ˜¯ä»€ä¹ˆè®¾å¤‡. æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ ã€‚ã€‚ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º3çš„ç”¨æˆ·åœ¨2020-10-13ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ è¿˜æœ‰ä¸€ä¸ªç”¨æˆ·(user)è¡¨ï¼Œç®€å†µå¦‚ä¸‹: è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢æ¯ä¸ªç”¨æˆ·æœ€è¿‘ä¸€å¤©ç™»å½•çš„æ—¥å­ï¼Œç”¨æˆ·çš„åå­—ï¼Œä»¥åŠç”¨æˆ·ç”¨çš„è®¾å¤‡çš„åå­—ï¼Œå¹¶ä¸”æŸ¥è¯¢ç»“æœæŒ‰ç…§userçš„nameå‡åºæ’åºï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹ è€ç”Ÿå¸¸è°ˆï¼Œgroup byåselectååªèƒ½è·Ÿèšåˆå’Œgroup byçš„å­—æ®µ SELECT u.name,c.name,l.date FROM login l INNER JOIN user u ON l.user_id=u.id INNER JOIN client c ON l.client_id=c.id INNER JOIN (SELECT user_id,MAX(date) max_date FROM login GROUP BY user_id) AS T ON u.id=T.user_id WHERE l.date=T.max_date ORDER BY u.name ASC è¿™ä¸ªç­”æ¡ˆæ›´ç®€æ˜ select u.name,c.name,l1.date from login l1,user u,client c where l1.date=(select max(l2.date) from login l2 where l1.user_id=l2.user_id) and l1.user_id=u.id and l1.client_id=c.id order by u.name æ€»ç»“ä¸€ä¸‹ï¼Œé‡åˆ°å–æœ€å¤§æœ€å°å€¼çš„é—®é¢˜æˆ–è€…æ˜¯è®¡æ•°ï¼Œå¯ä»¥å¯¹ä¸»é”®è¿›è¡Œè¿æ¥ï¼Œç„¶åå¢åŠ åˆ¤æ–­æ¡ä»¶ï¼Œæ’åºå°±ç”¨å¤§äºå°äºç­‰äºï¼Œæœ€å¤§æœ€å°å°±ç”¨max min ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ–°ç™»å½•ç”¨æˆ·çš„æ¬¡æ—¥æˆåŠŸçš„ç•™å­˜ç‡ï¼Œ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç¬¬ä¸€æ¬¡æ–°ç™»å½•äº†ç‰›å®¢ç½‘ ã€‚ã€‚ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º3çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ ã€‚ã€‚ã€‚ æœ€å1è¡Œè¡¨ç¤ºidä¸º1çš„ç”¨æˆ·åœ¨2020-10-14ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢æ–°ç™»å½•ç”¨æˆ·æ¬¡æ—¥æˆåŠŸçš„ç•™å­˜ç‡ï¼Œå³ç¬¬1å¤©ç™»é™†ä¹‹åï¼Œç¬¬2å¤©å†æ¬¡ç™»é™†çš„æ¦‚ç‡,ä¿å­˜å°æ•°ç‚¹åé¢3ä½(3ä½ä¹‹åçš„å››èˆäº”å…¥)ï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹: (sqliteé‡ŒæŸ¥æ‰¾æŸä¸€å¤©çš„åä¸€å¤©çš„ç”¨æ³•æ˜¯:date(yyyy-mm-dd, â€˜+1 dayâ€™)ï¼Œå››èˆäº”å…¥çš„å‡½æ•°ä¸ºroundï¼Œsqlite 1/2å¾—åˆ°çš„ä¸æ˜¯0.5ï¼Œå¾—åˆ°çš„æ˜¯0ï¼Œåªæœ‰1*1.0/2æ‰ä¼šå¾—åˆ°0.5 mysqlé‡ŒæŸ¥æ‰¾æŸä¸€å¤©çš„åä¸€å¤©çš„ç”¨æ³•æ˜¯:DATE_ADD(yyyy-mm-dd,INTERVAL 1 DAY)ï¼Œå››èˆäº”å…¥çš„å‡½æ•°ä¸ºround) ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªæ—¥æœŸç™»å½•æ–°ç”¨æˆ·ä¸ªæ•°ï¼Œ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬1æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯æ–°ç”¨æˆ· ã€‚ã€‚ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-13ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬2æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯è€ç”¨æˆ· ã€‚ã€‚ æœ€å1è¡Œè¡¨ç¤ºidä¸º4çš„ç”¨æˆ·åœ¨2020-10-15ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬2æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯è€ç”¨æˆ·è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢æ¯ä¸ªæ—¥æœŸç™»å½•æ–°ç”¨æˆ·ä¸ªæ•°ï¼Œå¹¶ä¸”æŸ¥è¯¢ç»“æœæŒ‰ç…§æ—¥æœŸå‡åºæ’åºï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹: ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªæ—¥æœŸæ–°ç”¨æˆ·çš„æ¬¡æ—¥ç•™å­˜ç‡ã€‚ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬1æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯æ–°ç”¨æˆ· ã€‚ã€‚ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-13ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬2æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯è€ç”¨æˆ· ã€‚ã€‚ æœ€å1è¡Œè¡¨ç¤ºidä¸º4çš„ç”¨æˆ·åœ¨2020-10-15ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ï¼Œå› ä¸ºæ˜¯ç¬¬2æ¬¡ç™»å½•ï¼Œæ‰€ä»¥æ˜¯è€ç”¨æˆ· è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢æ¯ä¸ªæ—¥æœŸæ–°ç”¨æˆ·çš„æ¬¡æ—¥ç•™å­˜ç‡ï¼Œç»“æœä¿ç•™å°æ•°ç‚¹åé¢3ä½æ•°(3ä½ä¹‹åçš„å››èˆäº”å…¥)ï¼Œå¹¶ä¸”æŸ¥è¯¢ç»“æœæŒ‰ç…§æ—¥æœŸå‡åºæ’åºï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹: SELECT ROUND(COUNT(DISTINCT l2.user_id)*1.0/COUNT(DISTINCT l1.user_id),3) FROM login l1,login l2 WHERE l1.user_id=l2.user_id AND l2.date=date(l1.date, '+1 day')ï¼› ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªæ—¥æœŸç™»å½•æ–°ç”¨æˆ·ä¸ªæ•°ï¼Œ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: æ…¢æ…¢æƒ³å§ select a.date, sum(case when rank=1 then 1 else 0 end) new from (select date, row_number() over(partition by user_id order by date) rank from login) a group by date; select date,sum(t) from ( select date, case when (user_id,date) in (select user_id,min(date) from login group by user_id) then 1 else 0 end as t from login ) group by date order by date asc ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªæ—¥æœŸæ–°ç”¨æˆ·çš„æ¬¡æ—¥ç•™å­˜ç‡ã€‚ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: 70.âŒ ç‰›å®¢æ¯å¤©æœ‰å¾ˆå¤šäººç™»å½•ï¼Œè¯·ä½ ç»Ÿè®¡ä¸€ä¸‹ç‰›å®¢æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢åˆ·é¢˜ä¿¡æ¯ï¼ŒåŒ…æ‹¬: ç”¨æˆ·çš„åå­—ï¼Œä»¥åŠæˆªæ­¢åˆ°æŸå¤©ï¼Œç´¯è®¡æ€»å…±é€šè¿‡äº†å¤šå°‘é¢˜ã€‚ ä¸å­˜åœ¨æ²¡æœ‰ç™»å½•å´åˆ·é¢˜çš„æƒ…å†µï¼Œä½†æ˜¯å­˜åœ¨ç™»å½•äº†æ²¡åˆ·é¢˜çš„æƒ…å†µï¼Œä¸ä¼šå­˜åœ¨åˆ·é¢˜è¡¨é‡Œé¢ï¼Œæœ‰æäº¤ä»£ç æ²¡æœ‰é€šè¿‡çš„æƒ…å†µï¼Œä½†æ˜¯ä¼šè®°å½•åœ¨åˆ·é¢˜è¡¨é‡Œï¼Œåªä¸è¿‡é€šè¿‡æ•°ç›®æ˜¯0ã€‚ æœ‰ä¸€ä¸ªç™»å½•(login)è®°å½•è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º1çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ ã€‚ã€‚ã€‚ ç¬¬5è¡Œè¡¨ç¤ºidä¸º3çš„ç”¨æˆ·åœ¨2020-10-13ä½¿ç”¨äº†å®¢æˆ·ç«¯idä¸º2çš„è®¾å¤‡ç™»å½•äº†ç‰›å®¢ç½‘ æœ‰ä¸€ä¸ªåˆ·é¢˜ï¼ˆpassing_number)è¡¨ï¼Œç®€å†µå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºidä¸º2çš„ç”¨æˆ·åœ¨2020-10-12é€šè¿‡äº†4ä¸ªé¢˜ç›®ã€‚ ã€‚ã€‚ã€‚ ç¬¬3è¡Œè¡¨ç¤ºidä¸º1çš„ç”¨æˆ·åœ¨2020-10-13æäº¤äº†ä»£ç ä½†æ˜¯æ²¡æœ‰é€šè¿‡ä»»ä½•é¢˜ç›®ã€‚ ç¬¬4è¡Œè¡¨ç¤ºidä¸º4çš„ç”¨æˆ·åœ¨2020-10-13é€šè¿‡äº†2ä¸ªé¢˜ç›® è¿˜æœ‰ä¸€ä¸ªç”¨æˆ·(user)è¡¨ï¼Œç®€å†µå¦‚ä¸‹: è¯·ä½ å†™å‡ºä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢åˆ·é¢˜ä¿¡æ¯ï¼ŒåŒ…æ‹¬: ç”¨æˆ·çš„åå­—ï¼Œä»¥åŠæˆªæ­¢åˆ°æŸå¤©ï¼Œç´¯è®¡æ€»å…±é€šè¿‡äº†å¤šå°‘é¢˜ï¼Œå¹¶ä¸”æŸ¥è¯¢ç»“æœå…ˆæŒ‰ç…§æ—¥æœŸå‡åºæ’åºï¼Œå†æŒ‰ç…§å§“åå‡åºæ’åºï¼Œæœ‰ç™»å½•å´æ²¡æœ‰åˆ·é¢˜çš„å“ªä¸€å¤©çš„æ•°æ®ä¸éœ€è¦è¾“å‡ºï¼Œä¸Šé¢çš„ä¾‹å­æŸ¥è¯¢ç»“æœå¦‚ä¸‹: é¢˜ç›®çš„å«ä¹‰æ˜¯å„ä¸ªç”¨æˆ·åœ¨ä¸åŒæ—¶é—´ç´¯ç§¯ç­”é¢˜æ±‚å’Œï¼Œé‡‡ç”¨sumå‡½æ•°è¿›è¡Œå¼€çª—å¤„ç†ï¼Œå°†user_idè¿›è¡Œåˆ†åŒºï¼Œå†é€šè¿‡æ—¶é—´å‡åºæ’åºï¼Œè¿›è€Œå®ç°äº†åœ¨æ¯ä¸ªuser_idåˆ†åŒºä¸­ä»¥å‡åºæ—¥æœŸæ’åºçš„é€šè¿‡é¢˜æ•°çš„é€ä¸ªé€’åŠ  SELECT name,date, SUM(number) over(partition by user_id order by date) FROM passing_number p LEFT JOIN user u ON p.user_id=u.id ORDER BY date,name è¿˜æœ‰ä¸€ç§æ˜¯åŒä¸€å¼ è¡¨å†…è¿æ¥çš„æ–¹æ³•ï¼Œè¿™ä¸ªå¾—èŠ±æ—¶é—´ç†è§£ï¼Œå¯¹äºè¿™ç§åˆ†ç»„ç´¯åŠ å¾ˆæœ‰å¿…è¦ SELECT u.name,pn1.date,SUM(pn2.number) FROM user u INNER JOIN passing_number pn1 ON u.id=pn1.user_id INNER JOIN passing_number pn2 ON pn1.user_id=pn2.user_id WHERE pn1.date&gt;=pn2.date GROUP BY pn1.id,pn1.date ORDER BY pn1.date ASC,u.name ASC; ç‰›å®¢æ¯æ¬¡è€ƒè¯•å®Œï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæˆç»©è¡¨(grade)ï¼Œå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º1çš„ç”¨æˆ·é€‰æ‹©äº†C++å²—ä½å¹¶ä¸”è€ƒäº†11001åˆ† ã€‚ã€‚ã€‚ ç¬¬8è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º8çš„ç”¨æˆ·é€‰æ‹©äº†å‰ç«¯å²—ä½å¹¶ä¸”è€ƒäº†9999åˆ† è¯·ä½ å†™ä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢å„ä¸ªå²—ä½åˆ†æ•°çš„å¹³å‡æ•°ï¼Œå¹¶ä¸”æŒ‰ç…§åˆ†æ•°é™åºæ’åºï¼Œç»“æœä¿ç•™å°æ•°ç‚¹åé¢3ä½(3ä½ä¹‹åå››èˆäº”å…¥): SELECT job,ROUND(AVG(score),3) AS avg FROM grade GROUP BY job ORDER BY avg DESC; ç‰›å®¢æ¯æ¬¡è€ƒè¯•å®Œï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæˆç»©è¡¨(grade)ï¼Œå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º1çš„ç”¨æˆ·é€‰æ‹©äº†C++å²—ä½å¹¶ä¸”è€ƒäº†11001åˆ† ã€‚ã€‚ã€‚ ç¬¬8è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º8çš„ç”¨æˆ·é€‰æ‹©äº†å‰ç«¯å²—ä½å¹¶ä¸”è€ƒäº†9999åˆ† è¯·ä½ å†™ä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢ç”¨æˆ·åˆ†æ•°å¤§äºå…¶æ‰€åœ¨å·¥ä½œ(job)åˆ†æ•°çš„å¹³å‡åˆ†çš„æ‰€æœ‰gradeçš„å±æ€§ï¼Œå¹¶ä¸”ä»¥idçš„å‡åºæ’åºï¼Œå¦‚ä¸‹: æ„Ÿè§‰è‡ªå·±å†™çš„å¾ˆçƒ‚ SELECT g.id,g.job,g.score FROM grade g,(SELECT job,AVG(score) avg_score FROM grade GROUP BY job) AS t WHERE g.job=t.job AND g.score&gt;t.avg_score ORDER BY g.id ASC; è¿™ä¸ªå¼€çª—å‡½æ•°çœ‹èµ·æ¥å°±å¾ˆç®€æ´ SELECT id,job,score FROM (SELECT *,AVG(score) OVER (PARTITION BY job) avg_score FROM grade) WHERE score&gt;avg_score ORDER BY id; ç‰›å®¢æ¯æ¬¡ä¸¾åŠä¼ä¸šç¬”è¯•çš„æ—¶å€™ï¼Œä¼ä¸šä¸€èˆ¬éƒ½ä¼šæœ‰ä¸åŒçš„è¯­è¨€å²—ä½ï¼Œæ¯”å¦‚C++å·¥ç¨‹å¸ˆï¼ŒJAVAå·¥ç¨‹å¸ˆï¼ŒPythonå·¥ç¨‹å¸ˆï¼Œæ¯ä¸ªç”¨æˆ·ç¬”è¯•å®Œæœ‰ä¸åŒçš„åˆ†æ•°ï¼Œç°åœ¨æœ‰ä¸€ä¸ªåˆ†æ•°(grade)è¡¨ç®€åŒ–å¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º1çš„é€‰æ‹©äº†language_idä¸º1å²—ä½çš„æœ€åè€ƒè¯•å®Œçš„åˆ†æ•°ä¸º12000ï¼Œ â€¦ ç¬¬7è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º7çš„é€‰æ‹©äº†language_idä¸º2å²—ä½çš„æœ€åè€ƒè¯•å®Œçš„åˆ†æ•°ä¸º11000ï¼Œ ä¸åŒçš„è¯­è¨€å²—ä½(language)è¡¨ç®€åŒ–å¦‚ä¸‹: è¯·ä½ æ‰¾å‡ºæ¯ä¸ªå²—ä½åˆ†æ•°æ’åå‰2çš„ç”¨æˆ·ï¼Œå¾—åˆ°çš„ç»“æœå…ˆæŒ‰ç…§languageçš„nameå‡åºæ’åºï¼Œå†æŒ‰ç…§ç§¯åˆ†é™åºæ’åºï¼Œæœ€åæŒ‰ç…§gradeçš„idå‡åºæ’åºï¼Œå¾—åˆ°ç»“æœå¦‚ä¸‹: SELECT id,name,score FROM (SELECT g.id AS id,l.name AS name,g.score AS score,DENSE_RANK() OVER (PARTITION BY g.language_id ORDER BY score DESC) AS rank_score FROM grade g INNER JOIN language l ON g.language_id=l.id ORDER BY l.name ASC,score DESC,id ASC) WHERE rank_score&lt;=2 ç‰›å®¢æ¯æ¬¡è€ƒè¯•å®Œï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæˆç»©è¡¨(grade)ï¼Œå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º1çš„ç”¨æˆ·é€‰æ‹©äº†C++å²—ä½å¹¶ä¸”è€ƒäº†11001åˆ† ã€‚ã€‚ã€‚ ç¬¬8è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º8çš„ç”¨æˆ·é€‰æ‹©äº†å‰ç«¯å²—ä½å¹¶ä¸”è€ƒäº†9999åˆ† è¯·ä½ å†™ä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢å„ä¸ªå²—ä½åˆ†æ•°å‡åºæ’åˆ—ä¹‹åçš„ä¸­ä½æ•°ä½ç½®çš„èŒƒå›´ï¼Œå¹¶ä¸”æŒ‰jobå‡åºæ’åºï¼Œç»“æœå¦‚ä¸‹: è§£é‡Š: ç¬¬1è¡Œè¡¨ç¤ºCå²—ä½çš„ä¸­ä½æ•°ä½ç½®èŒƒå›´ä¸º[2,2]ï¼Œä¹Ÿå°±æ˜¯2ã€‚å› ä¸ºCå²—ä½æ€»å…±3ä¸ªäººï¼Œæ˜¯å¥‡æ•°ï¼Œæ‰€ä»¥ä¸­ä½æ•°ä½ç½®ä¸º2æ˜¯æ­£ç¡®çš„(å³ä½ç½®ä¸º2çš„10000æ˜¯ä¸­ä½æ•°) ç¬¬2è¡Œè¡¨ç¤ºJavaå²—ä½çš„ä¸­ä½æ•°ä½ç½®èŒƒå›´ä¸º[1,2]ã€‚å› ä¸ºJavaå²—ä½æ€»å…±2ä¸ªäººï¼Œæ˜¯å¶æ•°ï¼Œæ‰€ä»¥è¦çŸ¥é“ä¸­ä½æ•°ï¼Œéœ€è¦çŸ¥é“2ä¸ªä½ç½®çš„æ•°å­—ï¼Œè€Œå› ä¸ºåªæœ‰2ä¸ªäººï¼Œæ‰€ä»¥ä¸­ä½æ•°ä½ç½®ä¸º[1,2]æ˜¯æ­£ç¡®çš„(å³éœ€è¦çŸ¥é“ä½ç½®ä¸º1çš„12000ä¸ä½ç½®ä¸º2çš„13000æ‰èƒ½è®¡ç®—å‡ºä¸­ä½æ•°ä¸º12500) ç¬¬3è¡Œè¡¨ç¤ºå‰ç«¯å²—ä½çš„ä¸­ä½æ•°ä½ç½®èŒƒå›´ä¸º[2,2]ï¼Œä¹Ÿå°±æ˜¯2ã€‚å› ä¸ºå‰ç«¯å²—ä½æ€»å…±3ä¸ªäººï¼Œæ˜¯å¥‡æ•°ï¼Œæ‰€ä»¥ä¸­ä½æ•°ä½ç½®ä¸º2æ˜¯æ­£ç¡®çš„(å³ä½ç½®ä¸º2çš„11000æ˜¯ä¸­ä½æ•°) (æ³¨æ„: sqlite 1/2å¾—åˆ°çš„ä¸æ˜¯0.5ï¼Œå¾—åˆ°çš„æ˜¯0ï¼Œåªæœ‰1*1.0/2æ‰ä¼šå¾—åˆ°0.5ï¼Œsqliteå››èˆäº”å…¥çš„å‡½æ•°ä¸ºroundï¼Œsqliteä¸æ”¯æŒfloorå‡½æ•°ï¼Œæ”¯æŒcast(x as integer) å‡½æ•°ï¼Œä¸æ”¯æŒifå‡½æ•°ï¼Œæ”¯æŒcase when â€¦then â€¦else â€¦endå‡½æ•°) ç‰›å®¢æ¯æ¬¡è€ƒè¯•å®Œï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªæˆç»©è¡¨(grade)ï¼Œå¦‚ä¸‹: ç¬¬1è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º1çš„ç”¨æˆ·é€‰æ‹©äº†C++å²—ä½å¹¶ä¸”è€ƒäº†11001åˆ† ã€‚ã€‚ã€‚ ç¬¬8è¡Œè¡¨ç¤ºç”¨æˆ·idä¸º8çš„ç”¨æˆ·é€‰æ‹©äº†å‰ç«¯å²—ä½å¹¶ä¸”è€ƒäº†9999åˆ† è¯·ä½ å†™ä¸€ä¸ªsqlè¯­å¥æŸ¥è¯¢å„ä¸ªå²—ä½åˆ†æ•°çš„ä¸­ä½æ•°ä½ç½®ä¸Šçš„æ‰€æœ‰gradeä¿¡æ¯ï¼Œå¹¶ä¸”æŒ‰idå‡åºæ’åºï¼Œç»“æœå¦‚ä¸‹: ==================================================================================== åŠ›æ‰£åˆ·é¢˜ ==================================================================================== è¡¨1: Person Â±------------Â±--------+ | åˆ—å | ç±»å‹ | Â±------------Â±--------+ | PersonId | int | | FirstName | varchar | | LastName | varchar | Â±------------Â±--------+ PersonId æ˜¯ä¸Šè¡¨ä¸»é”® è¡¨2: Address Â±------------Â±--------+ | åˆ—å | ç±»å‹ | Â±------------Â±--------+ | AddressId | int | | PersonId | int | | City | varchar | | State | varchar | Â±------------Â±--------+ AddressId æ˜¯ä¸Šè¡¨ä¸»é”® ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ»¡è¶³æ¡ä»¶ï¼šæ— è®º person æ˜¯å¦æœ‰åœ°å€ä¿¡æ¯ï¼Œéƒ½éœ€è¦åŸºäºä¸Šè¿°ä¸¤è¡¨æä¾› person çš„ä»¥ä¸‹ä¿¡æ¯ï¼š FirstName, LastName, City, State SELECT p.FirstName,p.Lastname,a.City,a.State FROM Person p LEFT JOIN Address a on p.PersonId=a.PersonId; ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œè·å– Employee è¡¨ä¸­ç¬¬äºŒé«˜çš„è–ªæ°´ï¼ˆSalaryï¼‰ ã€‚ Â±â€”Â±-------+ | Id | Salary | Â±â€”Â±-------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | Â±â€”Â±-------+ ä¾‹å¦‚ä¸Šè¿° Employee è¡¨ï¼ŒSQLæŸ¥è¯¢åº”è¯¥è¿”å› 200 ä½œä¸ºç¬¬äºŒé«˜çš„è–ªæ°´ã€‚å¦‚æœä¸å­˜åœ¨ç¬¬äºŒé«˜çš„è–ªæ°´ï¼Œé‚£ä¹ˆæŸ¥è¯¢åº”è¿”å› nullã€‚ Â±--------------------+ | SecondHighestSalary | Â±--------------------+ | 200 | Â±--------------------+ SELECT MAX(e1.Salary) SecondHighestSalary FROM Employee e1 WHERE e1.Salary&lt;(SELECT MAX(e2.Salary) FROM Employee e2); ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œè·å– Employee è¡¨ä¸­ç¬¬ n é«˜çš„è–ªæ°´ï¼ˆSalaryï¼‰ã€‚ Â±â€”Â±-------+ | Id | Salary | Â±â€”Â±-------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | Â±â€”Â±-------+ ä¾‹å¦‚ä¸Šè¿° Employee è¡¨ï¼Œn = 2 æ—¶ï¼Œåº”è¿”å›ç¬¬äºŒé«˜çš„è–ªæ°´ 200ã€‚å¦‚æœä¸å­˜åœ¨ç¬¬ n é«˜çš„è–ªæ°´ï¼Œé‚£ä¹ˆæŸ¥è¯¢åº”è¿”å› nullã€‚ Â±-----------------------+ | getNthHighestSalary(2) | Â±-----------------------+ | 200 | Â±-----------------------+ æœäº†ï¼Œrank()æ˜¯mysqlè‡ªå¸¦çš„å‡½æ•°ï¼Œä¸èƒ½ç”¨äºå˜é‡å‘½å CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT BEGIN RETURN ( # Write your MySQL query statement below. SELECT DISTINCT Salary FROM ( SELECT DENSE_RANK() OVER(ORDER BY Salary DESC) AS rank_s, Salary FROM Employee ) AS t WHERE rank_s=N ); END ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢æ¥å®ç°åˆ†æ•°æ’åã€‚ å¦‚æœä¸¤ä¸ªåˆ†æ•°ç›¸åŒï¼Œåˆ™ä¸¤ä¸ªåˆ†æ•°æ’åï¼ˆRankï¼‰ç›¸åŒã€‚è¯·æ³¨æ„ï¼Œå¹³åˆ†åçš„ä¸‹ä¸€ä¸ªåæ¬¡åº”è¯¥æ˜¯ä¸‹ä¸€ä¸ªè¿ç»­çš„æ•´æ•°å€¼ã€‚æ¢å¥è¯è¯´ï¼Œåæ¬¡ä¹‹é—´ä¸åº”è¯¥æœ‰â€œé—´éš”â€ã€‚ Â±â€”Â±------+ | Id | Score | Â±â€”Â±------+ | 1 | 3.50 | | 2 | 3.65 | | 3 | 4.00 | | 4 | 3.85 | | 5 | 4.00 | | 6 | 3.65 | Â±â€”Â±------+ ä¾‹å¦‚ï¼Œæ ¹æ®ä¸Šè¿°ç»™å®šçš„ Scores è¡¨ï¼Œä½ çš„æŸ¥è¯¢åº”è¯¥è¿”å›ï¼ˆæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åˆ—ï¼‰ï¼š Â±------Â±-----+ | Score | Rank | Â±------Â±-----+ | 4.00 | 1 | | 4.00 | 1 | | 3.85 | 2 | | 3.65 | 3 | | 3.65 | 3 | | 3.50 | 4 | Â±------Â±-----+ é‡è¦æç¤ºï¼šå¯¹äº MySQL è§£å†³æ–¹æ¡ˆï¼Œå¦‚æœè¦è½¬ä¹‰ç”¨ä½œåˆ—åçš„ä¿ç•™å­—ï¼Œå¯ä»¥åœ¨å…³é”®å­—ä¹‹å‰å’Œä¹‹åä½¿ç”¨æ’‡å·ã€‚ä¾‹å¦‚ Rank SELECT Score,DENSE_RANK() OVER(ORDER BY Score DESC) AS 'Rank' FROM Scores; 180.â— ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼ŒæŸ¥æ‰¾æ‰€æœ‰è‡³å°‘è¿ç»­å‡ºç°ä¸‰æ¬¡çš„æ•°å­—ã€‚ Â±â€”Â±----+ | Id | Num | Â±â€”Â±----+ | 1 | 1 | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 1 | | 6 | 2 | | 7 | 2 | Â±â€”Â±----+ ä¾‹å¦‚ï¼Œç»™å®šä¸Šé¢çš„ Logs è¡¨ï¼Œ 1 æ˜¯å”¯ä¸€è¿ç»­å‡ºç°è‡³å°‘ä¸‰æ¬¡çš„æ•°å­—ã€‚ Â±----------------+ | ConsecutiveNums | Â±----------------+ | 1 | Â±----------------+ SELECT DISTINCT l1.num ConsecutiveNums FROM logs l1 INNER JOIN logs l2 ON l1.Num=l2.Num AND l1.Id-l2.Id between 0 and 2 GROUP BY l1.id HAVING COUNT(l2.id)&gt;2; SELECT DISTINCT l1.num ConsecutiveNums FROM logs l1 INNER JOIN logs l2 ON l1.Num=l2.Num GROUP BY l1.id HAVING SUM(l1.id-l2.id BETWEEN 0 AND 2)&gt;2; Employee è¡¨åŒ…å«æ‰€æœ‰å‘˜å·¥ï¼Œä»–ä»¬çš„ç»ç†ä¹Ÿå±äºå‘˜å·¥ã€‚æ¯ä¸ªå‘˜å·¥éƒ½æœ‰ä¸€ä¸ª Idï¼Œæ­¤å¤–è¿˜æœ‰ä¸€åˆ—å¯¹åº”å‘˜å·¥çš„ç»ç†çš„ Idã€‚ Â±â€”Â±------Â±-------Â±----------+ | Id | Name | Salary | ManagerId | Â±â€”Â±------Â±-------Â±----------+ | 1 | Joe | 70000 | 3 | | 2 | Henry | 80000 | 4 | | 3 | Sam | 60000 | NULL | | 4 | Max | 90000 | NULL | Â±â€”Â±------Â±-------Â±----------+ ç»™å®š Employee è¡¨ï¼Œç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œè¯¥æŸ¥è¯¢å¯ä»¥è·å–æ”¶å…¥è¶…è¿‡ä»–ä»¬ç»ç†çš„å‘˜å·¥çš„å§“åã€‚åœ¨ä¸Šé¢çš„è¡¨æ ¼ä¸­ï¼ŒJoe æ˜¯å”¯ä¸€ä¸€ä¸ªæ”¶å…¥è¶…è¿‡ä»–çš„ç»ç†çš„å‘˜å·¥ã€‚ Â±---------+ | Employee | Â±---------+ | Joe | Â±---------+ SELECT e1.name Employee FROM Employee e1 INNER JOIN Employee e2 ON e1.ManagerId=e2.Id WHERE e1.Salary&gt;e2.Salary; å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼ŒæŸ¥æ‰¾ Person è¡¨ä¸­æ‰€æœ‰é‡å¤çš„ç”µå­é‚®ç®±ã€‚ ç¤ºä¾‹ï¼š Â±â€”Â±--------+ | Id | Email | Â±â€”Â±--------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | Â±â€”Â±--------+ æ ¹æ®ä»¥ä¸Šè¾“å…¥ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š Â±--------+ | Email | Â±--------+ | a@b.com | Â±--------+ SELECT Email FROM Person GROUP BY Email HAVING COUNT(Email)&gt;1 ä¸‹é¢è¿™ä¸ªå€’æ˜¯ç”¨æ¥åˆ¤æ–­é‡å¤çš„å¥½åŠæ³• select distinct(p1.Email) from Person p1 inner join Person p2 on p1.Email = p2.Email and p1.Id != p2.Id; æŸç½‘ç«™åŒ…å«ä¸¤ä¸ªè¡¨ï¼ŒCustomers è¡¨å’Œ Orders è¡¨ã€‚ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ä»ä¸è®¢è´­ä»»ä½•ä¸œè¥¿çš„å®¢æˆ·ã€‚ Customers è¡¨ï¼š Â±â€”Â±------+ | Id | Name | Â±â€”Â±------+ | 1 | Joe | | 2 | Henry | | 3 | Sam | | 4 | Max | Â±â€”Â±------+ Orders è¡¨ï¼š Â±â€”Â±-----------+ | Id | CustomerId | Â±â€”Â±-----------+ | 1 | 3 | | 2 | 1 | Â±â€”Â±-----------+ ä¾‹å¦‚ç»™å®šä¸Šè¿°è¡¨æ ¼ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ï¼š Â±----------+ | Customers | Â±----------+ | Henry | | Max | Â±----------+ SELECT Name Customers FROM Customers WHERE Customers.Id NOT IN (SELECT CustomerId FROM Orders); SELECT c.Name Customers FROM Customers c LEFT JOIN Orders O ON c.Id=o.CustomerId WHERE o.ID IS NULL Employee è¡¨åŒ…å«æ‰€æœ‰å‘˜å·¥ä¿¡æ¯ï¼Œæ¯ä¸ªå‘˜å·¥æœ‰å…¶å¯¹åº”çš„ Id, salary å’Œ department Idã€‚ Â±â€”Â±------Â±-------Â±-------------+ | Id | Name | Salary | DepartmentId | Â±â€”Â±------Â±-------Â±-------------+ | 1 | Joe | 70000 | 1 | | 2 | Jim | 90000 | 1 | | 3 | Henry | 80000 | 2 | | 4 | Sam | 60000 | 2 | | 5 | Max | 90000 | 1 | Â±â€”Â±------Â±-------Â±-------------+ Department è¡¨åŒ…å«å…¬å¸æ‰€æœ‰éƒ¨é—¨çš„ä¿¡æ¯ã€‚ Â±â€”Â±---------+ | Id | Name | Â±â€”Â±---------+ | 1 | IT | | 2 | Sales | Â±â€”Â±---------+ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ‰¾å‡ºæ¯ä¸ªéƒ¨é—¨å·¥èµ„æœ€é«˜çš„å‘˜å·¥ã€‚å¯¹äºä¸Šè¿°è¡¨ï¼Œæ‚¨çš„ SQL æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹è¡Œï¼ˆè¡Œçš„é¡ºåºæ— å…³ç´§è¦ï¼‰ã€‚ Â±-----------Â±---------Â±-------+ | Department | Employee | Salary | Â±-----------Â±---------Â±-------+ | IT | Max | 90000 | | IT | Jim | 90000 | | Sales | Henry | 80000 | Â±-----------Â±---------Â±-------+ #DENSE_RANK() PARTITION BY å®Œç¾ç»“åˆ SELECT Department,Employee,Salary FROM (SELECT e.Name Employee,e.Salary Salary,d.Name Department, DENSE_RANK() OVER(PARTITION BY e.DepartmentId ORDER BY e.Salary DESC) AS s_rank FROM Employee e INNER JOIN Department d ON e.DepartmentId=d.Id) t WHERE t.s_rank=1; ä»¥åå¯¹äºè¿™ç§group by åæœ‰å­—æ®µä¸èƒ½å–å‡ºæ¥çš„ï¼Œç”¨è¿™ç§æ–¹æ³• IN,æˆ–è€…æ˜¯group byåéœ€è¦å½“ä½œæ¡ä»¶çš„ select d.Name Department,e.Name Employee,Salary from Employee e join Department d on e.DepartmentId=d.Id where(e.DepartmentId , Salary) IN( select DepartmentId, max(salary) from Employee group by DepartmentId ); Employee è¡¨åŒ…å«æ‰€æœ‰å‘˜å·¥ä¿¡æ¯ï¼Œæ¯ä¸ªå‘˜å·¥æœ‰å…¶å¯¹åº”çš„å·¥å· Idï¼Œå§“å Nameï¼Œå·¥èµ„ Salary å’Œéƒ¨é—¨ç¼–å· DepartmentId ã€‚ Â±â€”Â±------Â±-------Â±-------------+ | Id | Name | Salary | DepartmentId | Â±â€”Â±------Â±-------Â±-------------+ | 1 | Joe | 85000 | 1 | | 2 | Henry | 80000 | 2 | | 3 | Sam | 60000 | 2 | | 4 | Max | 90000 | 1 | | 5 | Janet | 69000 | 1 | | 6 | Randy | 85000 | 1 | | 7 | Will | 70000 | 1 | Â±â€”Â±------Â±-------Â±-------------+ Department è¡¨åŒ…å«å…¬å¸æ‰€æœ‰éƒ¨é—¨çš„ä¿¡æ¯ã€‚ Â±â€”Â±---------+ | Id | Name | Â±â€”Â±---------+ | 1 | IT | | 2 | Sales | Â±â€”Â±---------+ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ‰¾å‡ºæ¯ä¸ªéƒ¨é—¨è·å¾—å‰ä¸‰é«˜å·¥èµ„çš„æ‰€æœ‰å‘˜å·¥ã€‚ä¾‹å¦‚ï¼Œæ ¹æ®ä¸Šè¿°ç»™å®šçš„è¡¨ï¼ŒæŸ¥è¯¢ç»“æœåº”è¿”å›ï¼š Â±-----------Â±---------Â±-------+ | Department | Employee | Salary | Â±-----------Â±---------Â±-------+ | IT | Max | 90000 | | IT | Randy | 85000 | | IT | Joe | 85000 | | IT | Will | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | Â±-----------Â±---------Â±-------+ #PARTITION BY çœŸå¥½ç”¨ SELECT Department,Employee,Salary FROM (SELECT d.Name Department,e.Name Employee,e.Salary Salary, DENSE_RANK() OVER(PARTITION BY e.DepartmentId ORDER BY e.Salary DESC) AS s_rank FROM Employee e INNER JOIN Department d ON e.DepartmentId=d.Id) t WHERE t.s_rank&lt;=3; è‡ªè¿æ¥è§£æ³•ï¼Œè¿™ç§é€šè¿‡è‡ªè¿æ¥åˆ¤æ–­æ’åï¼Œä¸€å®šè¦è®°å¾—ä½¿ç”¨distinct select d.Name as Department,e.Name as Employee,e.Salary as Salary from Employee as e left join Department as d on e.DepartmentId = d.Id where e.Id in ( select e1.Id from Employee as e1 left join Employee as e2 on e1.DepartmentId = e2.DepartmentId and e1.Salary &lt; e2.Salary group by e1.Id having count(distinct e2.Salary) &lt;= 2 ) and e.DepartmentId in (select Id from Department) order by d.Id asc,e.Salary desc ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ¥åˆ é™¤ Person è¡¨ä¸­æ‰€æœ‰é‡å¤çš„ç”µå­é‚®ç®±ï¼Œé‡å¤çš„é‚®ç®±é‡Œåªä¿ç•™ Id æœ€å° çš„é‚£ä¸ªã€‚ Â±â€”Â±-----------------+ | Id | Email | Â±â€”Â±-----------------+ | 1 | john@example.com | | 2 | bob@example.com | | 3 | john@example.com | Â±â€”Â±-----------------+ Id æ˜¯è¿™ä¸ªè¡¨çš„ä¸»é”®ã€‚ ä¾‹å¦‚ï¼Œåœ¨è¿è¡Œä½ çš„æŸ¥è¯¢è¯­å¥ä¹‹åï¼Œä¸Šé¢çš„ Person è¡¨åº”è¿”å›ä»¥ä¸‹å‡ è¡Œ: Â±â€”Â±-----------------+ | Id | Email | Â±â€”Â±-----------------+ | 1 | john@example.com | | 2 | bob@example.com | Â±â€”Â±-----------------+ #mysqlåˆ é™¤æ—¶ï¼Œå…ˆç¼“å­˜å†åˆ é™¤ DELETE FROM Person WHERE Id NOT IN (SELECT Id FROM (SELECT MIN(Id) Id FROM Person GROUP BY Email) t) è¿™ç§ç†è§£ä¹Ÿå¾ˆå¥½ delete p1 from person p1 join person p2 on p1.email=p2.email and p1.id&gt;p2.id è¡¨ Weather Â±--------------Â±--------+ | Column Name | Type | Â±--------------Â±--------+ | id | int | | recordDate | date | | temperature | int | Â±--------------Â±--------+ id æ˜¯è¿™ä¸ªè¡¨çš„ä¸»é”® è¯¥è¡¨åŒ…å«ç‰¹å®šæ—¥æœŸçš„æ¸©åº¦ä¿¡æ¯ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæ¥æŸ¥æ‰¾ä¸ä¹‹å‰ï¼ˆæ˜¨å¤©çš„ï¼‰æ—¥æœŸç›¸æ¯”æ¸©åº¦æ›´é«˜çš„æ‰€æœ‰æ—¥æœŸçš„ id ã€‚ è¿”å›ç»“æœ ä¸è¦æ±‚é¡ºåº ã€‚ æŸ¥è¯¢ç»“æœæ ¼å¼å¦‚ä¸‹ä¾‹ï¼š Weather Â±â€”Â±-----------Â±------------+ | id | recordDate | Temperature | Â±â€”Â±-----------Â±------------+ | 1 | 2015-01-01 | 10 | | 2 | 2015-01-02 | 25 | | 3 | 2015-01-03 | 20 | | 4 | 2015-01-04 | 30 | Â±â€”Â±-----------Â±------------+ Result table: Â±â€”+ | id | Â±â€”+ | 2 | | 4 | Â±â€”+ 2015-01-02 çš„æ¸©åº¦æ¯”å‰ä¸€å¤©é«˜ï¼ˆ10 -&gt; 25ï¼‰ 2015-01-04 çš„æ¸©åº¦æ¯”å‰ä¸€å¤©é«˜ï¼ˆ20 -&gt; 30ï¼‰ select date_add(@dt, interval 1 day); -- add 1 day select date_add(@dt, interval 1 hour); -- add 1 hour select date_add(@dt, interval 1 minute); -- ... select date_add(@dt, interval 1 second); select date_add(@dt, interval 1 microsecond); select date_add(@dt, interval 1 week); select date_add(@dt, interval 1 month); select date_add(@dt, interval 1 quarter); select date_add(@dt, interval 1 year); select date_sub('1998-01-01 00:00:00', interval '1 1:1:1' day_second); MySQL datediff(date1,date2)ï¼šä¸¤ä¸ªæ—¥æœŸç›¸å‡ date1 - date2ï¼Œè¿”å›å¤©æ•°ã€‚ select datediff('2008-08-08', '2008-08-01'); -- 7 select datediff('2008-08-01', '2008-08-08'); -- -7 select str_to_date('08/09/2008', '%m/%d/%Y'); -- 2008-08-09 select str_to_date('08/09/08' , '%m/%d/%y'); -- 2008-08-09 select str_to_date('08.09.2008', '%m.%d.%Y'); -- 2008-08-09 select str_to_date('08:09:30', '%h:%i:%s'); -- 08:09:30 select str_to_date('08.09.2008 08:09:30', '%m.%d.%Y %h:%i:%s'); -- 2008-08-09 08:09:30 SELECT a.id FROM Weather a INNER JOIN Weather b ON a.recordDate=DATE_ADD(b.recordDate,INTERVAL 1 day) WHERE a.temperature&gt;b.temperature; Trips è¡¨ä¸­å­˜æ‰€æœ‰å‡ºç§Ÿè½¦çš„è¡Œç¨‹ä¿¡æ¯ã€‚æ¯æ®µè¡Œç¨‹æœ‰å”¯ä¸€é”® Idï¼ŒClient_Id å’Œ Driver_Id æ˜¯ Users è¡¨ä¸­ Users_Id çš„å¤–é”®ã€‚Status æ˜¯æšä¸¾ç±»å‹ï¼Œæšä¸¾æˆå‘˜ä¸º (â€˜completedâ€™, â€˜cancelled_by_driverâ€™, â€˜cancelled_by_clientâ€™)ã€‚ Â±â€”Â±----------Â±----------Â±--------Â±-------------------Â±---------+ | Id | Client_Id | Driver_Id | City_Id | Status |Request_at| Â±â€”Â±----------Â±----------Â±--------Â±-------------------Â±---------+ | 1 | 1 | 10 | 1 | completed |2013-10-01| | 2 | 2 | 11 | 1 | cancelled_by_driver|2013-10-01| | 3 | 3 | 12 | 6 | completed |2013-10-01| | 4 | 4 | 13 | 6 | cancelled_by_client|2013-10-01| | 5 | 1 | 10 | 1 | completed |2013-10-02| | 6 | 2 | 11 | 6 | completed |2013-10-02| | 7 | 3 | 12 | 6 | completed |2013-10-02| | 8 | 2 | 12 | 12 | completed |2013-10-03| | 9 | 3 | 10 | 12 | completed |2013-10-03| | 10 | 4 | 13 | 12 | cancelled_by_driver|2013-10-03| Â±â€”Â±----------Â±----------Â±--------Â±-------------------Â±---------+ Users è¡¨å­˜æ‰€æœ‰ç”¨æˆ·ã€‚æ¯ä¸ªç”¨æˆ·æœ‰å”¯ä¸€é”® Users_Idã€‚Banned è¡¨ç¤ºè¿™ä¸ªç”¨æˆ·æ˜¯å¦è¢«ç¦æ­¢ï¼ŒRole åˆ™æ˜¯ä¸€ä¸ªè¡¨ç¤ºï¼ˆâ€˜clientâ€™, â€˜driverâ€™, â€˜partnerâ€™ï¼‰çš„æšä¸¾ç±»å‹ã€‚ Â±---------Â±-------Â±-------+ | Users_Id | Banned | Role | Â±---------Â±-------Â±-------+ | 1 | No | client | | 2 | Yes | client | | 3 | No | client | | 4 | No | client | | 10 | No | driver | | 11 | No | driver | | 12 | No | driver | | 13 | No | driver | Â±---------Â±-------Â±-------+ å†™ä¸€æ®µ SQL è¯­å¥æŸ¥å‡º 2013å¹´10æœˆ1æ—¥ è‡³ 2013å¹´10æœˆ3æ—¥ æœŸé—´éç¦æ­¢ç”¨æˆ·çš„å–æ¶ˆç‡ã€‚åŸºäºä¸Šè¡¨ï¼Œä½ çš„ SQL è¯­å¥åº”è¿”å›å¦‚ä¸‹ç»“æœï¼Œå–æ¶ˆç‡ï¼ˆCancellation Rateï¼‰ä¿ç•™ä¸¤ä½å°æ•°ã€‚ å–æ¶ˆç‡çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š(è¢«å¸æœºæˆ–ä¹˜å®¢å–æ¶ˆçš„éç¦æ­¢ç”¨æˆ·ç”Ÿæˆçš„è®¢å•æ•°é‡) / (éç¦æ­¢ç”¨æˆ·ç”Ÿæˆçš„è®¢å•æ€»æ•°) Â±-----------Â±------------------+ | Day | Cancellation Rate | Â±-----------Â±------------------+ | 2013-10-01 | 0.33 | | 2013-10-02 | 0.00 | | 2013-10-03 | 0.50 | Â±-----------Â±------------------+ SELECT t.Request_at 'Day', ROUND(SUM(IF(t.Status='completed',0,1))/COUNT(*),2) 'Cancellation Rate' FROM Trips t INNER JOIN Users u ON t.Client_id=u.Users_Id AND u.Banned='No' WHERE t.Request_at BETWEEN '2013-10-01' AND '2013-10-03' GROUP BY t.Request_at æ´»åŠ¨è¡¨ Activityï¼š Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | Â±-------------Â±--------+ è¡¨çš„ä¸»é”®æ˜¯ (player_id, event_date)ã€‚ è¿™å¼ è¡¨å±•ç¤ºäº†ä¸€äº›æ¸¸æˆç©å®¶åœ¨æ¸¸æˆå¹³å°ä¸Šçš„è¡Œä¸ºæ´»åŠ¨ã€‚ æ¯è¡Œæ•°æ®è®°å½•äº†ä¸€åç©å®¶åœ¨é€€å‡ºå¹³å°ä¹‹å‰ï¼Œå½“å¤©ä½¿ç”¨åŒä¸€å°è®¾å¤‡ç™»å½•å¹³å°åæ‰“å¼€çš„æ¸¸æˆçš„æ•°ç›®ï¼ˆå¯èƒ½æ˜¯ 0 ä¸ªï¼‰ã€‚ å†™ä¸€æ¡ SQL æŸ¥è¯¢è¯­å¥è·å–æ¯ä½ç©å®¶ ç¬¬ä¸€æ¬¡ç™»é™†å¹³å°çš„æ—¥æœŸã€‚ SELECT player_id,MIN(event_date) first_login FROM Activity GROUP BY player_id; select distinct player_id,min(event_date) over(partition by player_id) as first_login from Activity Table: Activity Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | Â±-------------Â±--------+ (player_id, event_date) æ˜¯è¿™ä¸ªè¡¨çš„ä¸¤ä¸ªä¸»é”® è¿™ä¸ªè¡¨æ˜¾ç¤ºçš„æ˜¯æŸäº›æ¸¸æˆç©å®¶çš„æ¸¸æˆæ´»åŠ¨æƒ…å†µ æ¯ä¸€è¡Œæ˜¯åœ¨æŸå¤©ä½¿ç”¨æŸä¸ªè®¾å¤‡ç™»å‡ºä¹‹å‰ç™»å½•å¹¶ç©å¤šä¸ªæ¸¸æˆï¼ˆå¯èƒ½ä¸º0ï¼‰çš„ç©å®¶çš„è®°å½• è¯·ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œæè¿°æ¯ä¸€ä¸ªç©å®¶é¦–æ¬¡ç™»é™†çš„è®¾å¤‡åç§° SELECT a.player_id,a.device_id FROM (SELECT player_id,MIN(event_date) min_date FROM Activity GROUP BY player_id) t INNER JOIN Activity a ON t.player_id=a.player_id AND t.min_date=a.event_date è¿™ç§è”åˆæŸ¥è¯¢çš„å†™æ³•æ³¨æ„ç‚¹ SELECT player_id,device_id FROM activity WHERE (player_id,event_date) IN (SELECT player_id,MIN(event_date) FROM activity GROUP BY player_id) Table: Activity Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | Â±-------------Â±--------+ ï¼ˆplayer_idï¼Œevent_dateï¼‰æ˜¯æ­¤è¡¨çš„ä¸»é”®ã€‚ è¿™å¼ è¡¨æ˜¾ç¤ºäº†æŸäº›æ¸¸æˆçš„ç©å®¶çš„æ´»åŠ¨æƒ…å†µã€‚ æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªç©å®¶çš„è®°å½•ï¼Œä»–åœ¨æŸä¸€å¤©ä½¿ç”¨æŸä¸ªè®¾å¤‡æ³¨é”€ä¹‹å‰ç™»å½•å¹¶ç©äº†å¾ˆå¤šæ¸¸æˆï¼ˆå¯èƒ½æ˜¯ 0 ï¼‰ã€‚ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼ŒåŒæ—¶æŠ¥å‘Šæ¯ç»„ç©å®¶å’Œæ—¥æœŸï¼Œä»¥åŠç©å®¶åˆ°ç›®å‰ä¸ºæ­¢ç©äº†å¤šå°‘æ¸¸æˆã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æ­¤æ—¥æœŸä¹‹å‰ç©å®¶æ‰€ç©çš„æ¸¸æˆæ€»æ•°ã€‚è¯¦ç»†æƒ…å†µè¯·æŸ¥çœ‹ç¤ºä¾‹ã€‚ æ£’æ£’æ£’ï¼Œè‡ªå·±å†™å‡ºæ¥äº† SELECT a1.player_id,a1.event_date,SUM(a2.games_played) games_played_so_far FROM Activity a1 INNER JOIN Activity a2 ON a1.player_id=a2.player_id AND a1.event_date&gt;=a2.event_date GROUP BY a1.player_id,a1.event_date SELECT player_id,event_date,SUM(games_played) OVER(PARTITION BY player_id ORDER BY event_date ASC) games_played_so_far FROM Activityï¼› Table: Activity Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | player_id | int | | device_id | int | | event_date | date | | games_played | int | Â±-------------Â±--------+ ï¼ˆplayer_idï¼Œevent_dateï¼‰æ˜¯æ­¤è¡¨çš„ä¸»é”®ã€‚ è¿™å¼ è¡¨æ˜¾ç¤ºäº†æŸäº›æ¸¸æˆçš„ç©å®¶çš„æ´»åŠ¨æƒ…å†µã€‚ æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªç©å®¶çš„è®°å½•ï¼Œä»–åœ¨æŸä¸€å¤©ä½¿ç”¨æŸä¸ªè®¾å¤‡æ³¨é”€ä¹‹å‰ç™»å½•å¹¶ç©äº†å¾ˆå¤šæ¸¸æˆï¼ˆå¯èƒ½æ˜¯ 0ï¼‰ã€‚ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼ŒæŠ¥å‘Šåœ¨é¦–æ¬¡ç™»å½•çš„ç¬¬äºŒå¤©å†æ¬¡ç™»å½•çš„ç©å®¶çš„æ¯”ç‡ï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹åä¸¤ä½ã€‚æ¢å¥è¯è¯´ï¼Œæ‚¨éœ€è¦è®¡ç®—ä»é¦–æ¬¡ç™»å½•æ—¥æœŸå¼€å§‹è‡³å°‘è¿ç»­ä¸¤å¤©ç™»å½•çš„ç©å®¶çš„æ•°é‡ï¼Œç„¶åé™¤ä»¥ç©å®¶æ€»æ•°ã€‚ è‡ªå·±å†™çš„è¯´å®è¯æœ‰ç‚¹çƒ‚ SELECT ROUND(COUNT(DISTINCT t1.player_id)/(SELECT COUNT(DISTINCT t3.player_id) FROM Activity t3),2) fraction FROM Activity t1 INNER JOIN Activity t2 ON t1.player_id=t2.player_id AND DATE_ADD(t1.event_date,INTERVAL 1 DAY)=t2.event_date WHERE (t1.player_id,t1.event_date) IN( SELECT player_id,MIN(event_date) FROM Activity GROUP BY player_id ) è¿™ä¸ªå°±å¾ˆå¼ºï¼Œç”¨æœ€ä¸‹æ—¥æœŸ+1ï¼Œçœå»åˆ¤æ–­è¿ç»­æ—¥æœŸï¼Œé¦–æ¬¡ç™»å½•è¿ç»­æ—¥æœŸï¼Œå¯ä»¥ç”¨MINæ±‚å‡ºé¦–æ¬¡æ—¥æœŸï¼Œå†+1å°±æ˜¯æ¬¡æ—¥æ—¥æœŸï¼Œåˆ¤æ–­palyer_idï¼ŒåŠ è¿‡åçš„æ—¥æœŸåœ¨ä¸åœ¨å³å¯è®¡æ•°ã€‚ SELECT ROUND(COUNT(DISTINCT player_id)/(SELECT COUNT(distinct player_id) FROM Activity), 2) AS fraction FROM Activity WHERE (player_id,event_date) IN (SELECT player_id, Date(min(event_date)+1) FROM Activity GROUP BY player_id); select round(count(distinct(b.player_id))/(select count(distinct(player_id)) from activity),2) as fraction from activity b join (select player_id,min(event_date), min(event_date)+1 as date_2nd from activity group by 1 ) a on b.event_date = a.date_2nd and b.player_id = a.player_id Employee è¡¨åŒ…å«æ‰€æœ‰å‘˜å·¥ã€‚Employee è¡¨æœ‰ä¸‰åˆ—ï¼šå‘˜å·¥Idï¼Œå…¬å¸åå’Œè–ªæ°´ã€‚ Â±----Â±-----------Â±-------+ |Id | Company | Salary | Â±----Â±-----------Â±-------+ |1 | A | 2341 | |2 | A | 341 | |3 | A | 15 | |4 | A | 15314 | |5 | A | 451 | |6 | A | 513 | |7 | B | 15 | |8 | B | 13 | |9 | B | 1154 | |10 | B | 1345 | |11 | B | 1221 | |12 | B | 234 | |13 | C | 2345 | |14 | C | 2645 | |15 | C | 2645 | |16 | C | 2652 | |17 | C | 65 | Â±----Â±-----------Â±-------+ è¯·ç¼–å†™SQLæŸ¥è¯¢æ¥æŸ¥æ‰¾æ¯ä¸ªå…¬å¸çš„è–ªæ°´ä¸­ä½æ•°ã€‚æŒ‘æˆ˜ç‚¹ï¼šä½ æ˜¯å¦å¯ä»¥åœ¨ä¸ä½¿ç”¨ä»»ä½•å†…ç½®çš„SQLå‡½æ•°çš„æƒ…å†µä¸‹è§£å†³æ­¤é—®é¢˜ã€‚ æ±‚ä¸­ä½æ•°çš„æ€è·¯ï¼Œå…ˆå¯¹å„ç»„æ•°æ®è¿›è¡Œæ’åºï¼ŒåŒæ—¶æ±‚å‡ºå„ç»„æ•°æ®æ€»æ•°ï¼Œç„¶åå†åˆ¤æ–­æ’ååœ¨æ€»æ•°/2ä¸æ€»æ•°/2+1ä¹‹é—´ã€‚ Employee è¡¨åŒ…å«æ‰€æœ‰å‘˜å·¥å’Œä»–ä»¬çš„ç»ç†ã€‚æ¯ä¸ªå‘˜å·¥éƒ½æœ‰ä¸€ä¸ª Idï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€åˆ—æ˜¯ç»ç†çš„ Idã€‚ Â±-----Â±---------Â±----------Â±---------+ |Id |Name |Department |ManagerId | Â±-----Â±---------Â±----------Â±---------+ |101 |John |A |null | |102 |Dan |A |101 | |103 |James |A |101 | |104 |Amy |A |101 | |105 |Anne |A |101 | |106 |Ron |B |101 | Â±-----Â±---------Â±----------Â±---------+ ç»™å®š Employee è¡¨ï¼Œè¯·ç¼–å†™ä¸€ä¸ªSQLæŸ¥è¯¢æ¥æŸ¥æ‰¾è‡³å°‘æœ‰5åç›´æ¥ä¸‹å±çš„ç»ç†ã€‚ SELECT Name FROM Employee WHERE Id IN (SELECT ManagerId FROM Employee GROUP BY ManagerId HAVING COUNT(*)&gt;=5) Numbers è¡¨ä¿å­˜æ•°å­—çš„å€¼åŠå…¶é¢‘ç‡ã€‚ Â±---------Â±------------+ | Number | Frequency | Â±---------Â±------------| | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | Â±---------Â±------------+ åœ¨æ­¤è¡¨ä¸­ï¼Œæ•°å­—ä¸º 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3ï¼Œæ‰€ä»¥ä¸­ä½æ•°æ˜¯ (0 + 0) / 2 = 0ã€‚ Â±-------+ | median | Â±-------| | 0.0000 | Â±-------+ è¯·ç¼–å†™ä¸€ä¸ªæŸ¥è¯¢æ¥æŸ¥æ‰¾æ‰€æœ‰æ•°å­—çš„ä¸­ä½æ•°å¹¶å°†ç»“æœå‘½åä¸º median ã€‚ æ–°å¢ä¸¤åˆ—ï¼Œå°†ä»åå¾€å‰å’Œä»å‰å¾€åçš„é¢‘æ•°ç›¸åŠ ï¼Œä¸¤ä¸ªæ•°éƒ½éœ€è¦å¤§äºç­‰äºæ€»æ•°ä¸€åŠï¼Œå†å–å¹³å‡ é”™è¯¯ç­”æ¡ˆ SELECT AVG(T.num) AS median FROM (SELECT n1.Number num,Sum(n2.Frequency) as f_sum FROM Numbers n1 INNER JOIN Numbers n2 on n1.Number&gt;=n2.Number GROUP BY n1.Number ORDER BY n1.Number ASC) T WHERE T.f_sum BETWEEN (SELECT SUM(Frequency)/2 FROM Numbers) AND (SELECT SUM(Frequency)/2+1 FROM Numbers) è¡¨: Candidate Â±----Â±--------+ | id | Name | Â±----Â±--------+ | 1 | A | | 2 | B | | 3 | C | | 4 | D | | 5 | E | Â±----Â±--------+ è¡¨: Vote Â±----Â±-------------+ | id | CandidateId | Â±----Â±-------------+ | 1 | 2 | | 2 | 4 | | 3 | 3 | | 4 | 2 | | 5 | 5 | Â±----Â±-------------+ id æ˜¯è‡ªåŠ¨é€’å¢çš„ä¸»é”®ï¼Œ CandidateId æ˜¯ Candidate è¡¨ä¸­çš„ id. è¯·ç¼–å†™ sql è¯­å¥æ¥æ‰¾åˆ°å½“é€‰è€…çš„åå­—ï¼Œä¸Šé¢çš„ä¾‹å­å°†è¿”å›å½“é€‰è€… B. #group by key order by count(key) SELECT Name FROM Candidate WHERE id=( SELECT CandidateId FROM Vote GROUP BY CandidateId ORDER BY COUNT(CandidateId) DESC LIMIT 1); é€‰å‡ºæ‰€æœ‰ bonus &lt; 1000 çš„å‘˜å·¥çš„ name åŠå…¶ bonusã€‚ Employee è¡¨å• Â±------Â±-------Â±----------Â±-------+ | empId | name | supervisor| salary | Â±------Â±-------Â±----------Â±-------+ | 1 | John | 3 | 1000 | | 2 | Dan | 3 | 2000 | | 3 | Brad | null | 4000 | | 4 | Thomas | 3 | 4000 | Â±------Â±-------Â±----------Â±-------+ empId æ˜¯è¿™å¼ è¡¨å•çš„ä¸»å…³é”®å­— Bonus è¡¨å• Â±------Â±------+ | empId | bonus | Â±------Â±------+ | 2 | 500 | | 4 | 2000 | Â±------Â±------+ empId æ˜¯è¿™å¼ è¡¨å•çš„ä¸»å…³é”®å­— SELECT e.name name,b.bonus bonus FROM Employee e LEFT OUTER JOIN Bonus b ON e.empId=b.empId WHERE b.bonus&lt;1000 or b.bonus IS NULL; ä» survey_log è¡¨ä¸­è·å¾—å›ç­”ç‡æœ€é«˜çš„é—®é¢˜ï¼Œsurvey_log è¡¨åŒ…å«è¿™äº›åˆ—ï¼šid, action, question_id, answer_id, q_num, timestampã€‚ id è¡¨ç¤ºç”¨æˆ· idï¼›action æœ‰ä»¥ä¸‹å‡ ç§å€¼ï¼šâ€œshowâ€ï¼Œâ€œanswerâ€ï¼Œâ€œskipâ€ï¼›å½“ action å€¼ä¸º â€œanswerâ€ æ—¶ answer_id éç©ºï¼Œè€Œ action å€¼ä¸º â€œshowâ€ æˆ–è€… â€œskipâ€ æ—¶ answer_id ä¸ºç©ºï¼›q_num è¡¨ç¤ºå½“å‰ä¼šè¯ä¸­é—®é¢˜çš„ç¼–å·ã€‚ è¯·ç¼–å†™ SQL æŸ¥è¯¢æ¥æ‰¾åˆ°å…·æœ‰æœ€é«˜å›ç­”ç‡çš„é—®é¢˜ã€‚ è¿™ç§group by ååˆè¦å–æœ€å€¼çš„é—®é¢˜ï¼Œè®°å¾— order by ç„¶å limit 1 SELECT t.question_id survey_log FROM (SELECT question_id,COUNT(answer_id) num FROM survey_log GROUP BY question_id ORDER BY num DESC) AS t LIMIT 1; è¿™ä¸­group byåorder by countçš„å†™æ³• SELECT question_id survey_log FROM survey_log WHERE answer_id IS NOT NULL GROUP BY question_id ORDER BY COUNT(answer_id) DESC LIMIT 1; è¯·ä½ ç¼–å†™ SQL è¯­å¥ï¼Œå¯¹äºæ¯ä¸ªå‘˜å·¥ï¼ŒæŸ¥è¯¢ä»–é™¤æœ€è¿‘ä¸€ä¸ªæœˆï¼ˆå³æœ€å¤§æœˆï¼‰ä¹‹å¤–ï¼Œå‰©ä¸‹æ¯ä¸ªæœˆçš„è¿‘ä¸‰ä¸ªæœˆçš„ç´¯è®¡è–ªæ°´ï¼ˆä¸è¶³ä¸‰ä¸ªæœˆä¹Ÿè¦è®¡ç®—ï¼‰ã€‚ ç»“æœè¯·æŒ‰ Id å‡åºï¼Œç„¶åæŒ‰ Month é™åºæ˜¾ç¤ºã€‚ ç¤ºä¾‹ï¼š è¾“å…¥ï¼š Id Month Salary 1 1 20 2 1 20 1 2 30 2 2 30 3 2 40 1 3 40 3 3 60 1 4 60 3 4 70 rows 2 preceding:å°†å½“å‰è¡Œå’Œå®ƒå‰é¢çš„ä¸¤è¡Œåˆ’ä¸ºä¸€ä¸ªçª—å£ï¼Œå› æ­¤sumå‡½æ•°å°±ä½œ ç”¨åœ¨è¿™ä¸‰è¡Œä¸Šé¢ SELECT Id id,Month month,SUM(Salary) OVER(PARTITION BY Id ORDER BY Month ASC ROWS 2 PRECEDING) Salary FROM Employee WHERE (Id,Month) NOT IN (SELECT Id,MAX(Month) FROM Employee GROUP BY Id) ORDER BY Id ASC,Month DESC; SELECT a.Id AS id, a.Month AS month,SUM(b.Salary) AS Salary FROM Employee a, Employee b WHERE a.Id = b.Id AND a.Month &gt;= b.Month AND a.Month &lt; b.Month+3 #è¿™ä¸ªé™åˆ¶åœ¨3ä¸ªæœˆå†… AND (a.Id, a.Month) NOT IN (SELECT Id, MAX(Month) FROM Employee GROUP BY Id) GROUP BY a.Id, a.Month ORDER BY a.Id, a.Month DESC ä¸€æ‰€å¤§å­¦æœ‰ 2 ä¸ªæ•°æ®è¡¨ï¼Œåˆ†åˆ«æ˜¯ student å’Œ department ï¼Œè¿™ä¸¤ä¸ªè¡¨ä¿å­˜ç€æ¯ä¸ªä¸“ä¸šçš„å­¦ç”Ÿæ•°æ®å’Œé™¢ç³»æ•°æ®ã€‚ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼ŒæŸ¥è¯¢ department è¡¨ä¸­æ¯ä¸ªä¸“ä¸šçš„å­¦ç”Ÿäººæ•° ï¼ˆå³ä½¿æ²¡æœ‰å­¦ç”Ÿçš„ä¸“ä¸šä¹Ÿéœ€åˆ—å‡ºï¼‰ã€‚ å°†ä½ çš„æŸ¥è¯¢ç»“æœæŒ‰ç…§å­¦ç”Ÿäººæ•°é™åºæ’åˆ—ã€‚ å¦‚æœæœ‰ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šä¸“ä¸šæœ‰ç›¸åŒçš„å­¦ç”Ÿæ•°ç›®ï¼Œå°†è¿™äº›éƒ¨é—¨æŒ‰ç…§éƒ¨é—¨åå­—çš„å­—å…¸åºä»å°åˆ°å¤§æ’åˆ—ã€‚ student è¡¨æ ¼å¦‚ä¸‹ï¼š Column Name Type student_id Integer student_name String gender Character dept_id Integer å…¶ä¸­ï¼Œ student_id æ˜¯å­¦ç”Ÿçš„å­¦å·ï¼Œ student_name æ˜¯å­¦ç”Ÿçš„å§“åï¼Œ gender æ˜¯å­¦ç”Ÿçš„æ€§åˆ«ï¼Œ dept_id æ˜¯å­¦ç”Ÿæ‰€å±ä¸“ä¸šçš„ä¸“ä¸šç¼–å·ã€‚ department è¡¨æ ¼å¦‚ä¸‹ï¼š Column Name Type dept_id Integer dept_name String dept_id æ˜¯ä¸“ä¸šç¼–å·ï¼Œ dept_name æ˜¯ä¸“ä¸šåå­—ã€‚ åŒºåˆ†count(*ï¼‰å’Œcount(s.student_id)åŒºåˆ« å‰ä¸€ä¸ªæ˜¯æœ‰å¤šå°‘æ¡ä¸è€ƒè™‘å€¼ åä¸€ä¸ªåªç®—s.student_idè¿™ä¸€åˆ—çš„å€¼ä¸ä¼šè®¡ç®—null SELECT d.dept_name,COUNT(s.student_id) student_number FROM department d LEFT JOIN student s ON d.dept_id=s.dept_id GROUP BY d.dept_name ORDER BY student_number DESC,d.dept_name ASC; ç»™å®šè¡¨ customer ï¼Œé‡Œé¢ä¿å­˜äº†æ‰€æœ‰å®¢æˆ·ä¿¡æ¯å’Œä»–ä»¬çš„æ¨èäººã€‚ Â±-----Â±-----Â±----------+ | id | name | referee_id| Â±-----Â±-----Â±----------+ | 1 | Will | NULL | | 2 | Jane | NULL | | 3 | Alex | 2 | | 4 | Bill | NULL | | 5 | Zack | 1 | | 6 | Mark | 2 | Â±-----Â±-----Â±----------+ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œè¿”å›ä¸€ä¸ªç¼–å·åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­ç¼–å·çš„æ¨èäººçš„ç¼–å·éƒ½ ä¸æ˜¯ 2ã€‚ select name from customer where ifnull(referee_id,0)!=2; SELECT name FROM customer WHERE referee_id!=2 OR referee_id IS NULL å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œå°† 2016 å¹´ (TIV_2016) æ‰€æœ‰æˆåŠŸæŠ•èµ„çš„é‡‘é¢åŠ èµ·æ¥ï¼Œä¿ç•™ 2 ä½å°æ•°ã€‚ å¯¹äºä¸€ä¸ªæŠ•ä¿äººï¼Œä»–åœ¨ 2016 å¹´æˆåŠŸæŠ•èµ„çš„æ¡ä»¶æ˜¯ï¼š ä»–åœ¨ 2015 å¹´çš„æŠ•ä¿é¢ (TIV_2015) è‡³å°‘è·Ÿä¸€ä¸ªå…¶ä»–æŠ•ä¿äººåœ¨ 2015 å¹´çš„æŠ•ä¿é¢ç›¸åŒã€‚ ä»–æ‰€åœ¨çš„åŸå¸‚å¿…é¡»ä¸å…¶ä»–æŠ•ä¿äººéƒ½ä¸åŒï¼ˆä¹Ÿå°±æ˜¯è¯´ç»´åº¦å’Œç»åº¦ä¸èƒ½è·Ÿå…¶ä»–ä»»ä½•ä¸€ä¸ªæŠ•ä¿äººå®Œå…¨ç›¸åŒï¼‰ã€‚ è¾“å…¥æ ¼å¼: è¡¨ insurance æ ¼å¼å¦‚ä¸‹ï¼š Column Name Type PID INTEGER(11) TIV_2015 NUMERIC(15,2) TIV_2016 NUMERIC(15,2) LAT NUMERIC(5,2) LON NUMERIC(5,2) PID å­—æ®µæ˜¯æŠ•ä¿äººçš„æŠ•ä¿ç¼–å·ï¼Œ TIV_2015 æ˜¯è¯¥æŠ•ä¿äººåœ¨2015å¹´çš„æ€»æŠ•ä¿é‡‘é¢ï¼Œ TIV_2016 æ˜¯è¯¥æŠ•ä¿äººåœ¨2016å¹´çš„æŠ•ä¿é‡‘é¢ï¼Œ LAT æ˜¯æŠ•ä¿äººæ‰€åœ¨åŸå¸‚çš„ç»´åº¦ï¼Œ LON æ˜¯æŠ•ä¿äººæ‰€åœ¨åŸå¸‚çš„ç»åº¦ã€‚ åœ¨è¡¨ orders ä¸­æ‰¾åˆ°è®¢å•æ•°æœ€å¤šå®¢æˆ·å¯¹åº”çš„ customer_number ã€‚ æ•°æ®ä¿è¯è®¢å•æ•°æœ€å¤šçš„é¡¾å®¢æ°å¥½åªæœ‰ä¸€ä½ã€‚ è¡¨ orders å®šä¹‰å¦‚ä¸‹ï¼š Column Type order_number (PK) int customer_number int order_date date required_date date shipped_date date status char(15) comment char(200) é¢˜ç›®æ²¡çœ‹æ‡‚ SELECT customer_number FROM orders GROUP BY customer_number ORDER BY COUNT(order_number) DESC LIMIT 1; select customer_number from orders group by customer_number having count(order_number)&gt;=all( select count(order_number) from orders group by customer_number ) è¿™é‡Œæœ‰å¼  World è¡¨ Â±----------------Â±-----------Â±-----------Â±-------------Â±--------------+ | name | continent | area | population | gdp | Â±----------------Â±-----------Â±-----------Â±-------------Â±--------------+ | Afghanistan | Asia | 652230 | 25500100 | 20343000 | | Albania | Europe | 28748 | 2831741 | 12960000 | | Algeria | Africa | 2381741 | 37100000 | 188681000 | | Andorra | Europe | 468 | 78115 | 3712000 | | Angola | Africa | 1246700 | 20609294 | 100990000 | Â±----------------Â±-----------Â±-----------Â±-------------Â±--------------+ å¦‚æœä¸€ä¸ªå›½å®¶çš„é¢ç§¯è¶…è¿‡ 300 ä¸‡å¹³æ–¹å…¬é‡Œï¼Œæˆ–è€…äººå£è¶…è¿‡ 2500 ä¸‡ï¼Œé‚£ä¹ˆè¿™ä¸ªå›½å®¶å°±æ˜¯å¤§å›½å®¶ã€‚ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼Œè¾“å‡ºè¡¨ä¸­æ‰€æœ‰å¤§å›½å®¶çš„åç§°ã€äººå£å’Œé¢ç§¯ã€‚ SELECT w.name,w.population,w.area FROM World w WHERE w.area&gt;3000000 OR population&gt;25000000 æœ‰ä¸€ä¸ªcourses è¡¨ ï¼Œæœ‰: student (å­¦ç”Ÿ) å’Œ class (è¯¾ç¨‹)ã€‚ è¯·åˆ—å‡ºæ‰€æœ‰è¶…è¿‡æˆ–ç­‰äº5åå­¦ç”Ÿçš„è¯¾ã€‚ ä¾‹å¦‚ï¼Œè¡¨ï¼š Â±--------Â±-----------+ | student | class | Â±--------Â±-----------+ | A | Math | | B | English | | C | Math | | D | Biology | | E | Math | | F | Computer | | G | Math | | H | Math | | I | Math | Â±--------Â±-----------+ SELECT class FROM courses GROUP BY class HAVING COUNT(DISTINCT student)&gt;=5 åœ¨ Facebook æˆ–è€… Twitter è¿™æ ·çš„ç¤¾äº¤åº”ç”¨ä¸­ï¼Œäººä»¬ç»å¸¸ä¼šå‘å¥½å‹ç”³è¯·ä¹Ÿä¼šæ”¶åˆ°å…¶ä»–äººçš„å¥½å‹ç”³è¯·ã€‚ è¡¨ï¼šFriendRequest Â±---------------Â±--------+ | Column Name | Type | Â±---------------Â±--------+ | sender_id | int | | send_to_id | int | | request_date | date | Â±---------------Â±--------+ æ­¤è¡¨æ²¡æœ‰ä¸»é”®ï¼Œå®ƒå¯èƒ½åŒ…å«é‡å¤é¡¹ã€‚ è¯¥è¡¨åŒ…å«å‘é€è¯·æ±‚çš„ç”¨æˆ·çš„ ID ï¼Œæ¥å—è¯·æ±‚çš„ç”¨æˆ·çš„ ID ä»¥åŠè¯·æ±‚çš„æ—¥æœŸã€‚ è¡¨ï¼šRequestAccepted Â±---------------Â±--------+ | Column Name | Type | Â±---------------Â±--------+ | requester_id | int | | accepter_id | int | | accept_date | date | Â±---------------Â±--------+ æ­¤è¡¨æ²¡æœ‰ä¸»é”®ï¼Œå®ƒå¯èƒ½åŒ…å«é‡å¤é¡¹ã€‚ è¯¥è¡¨åŒ…å«å‘é€è¯·æ±‚çš„ç”¨æˆ·çš„ ID ï¼Œæ¥å—è¯·æ±‚çš„ç”¨æˆ·çš„ ID ä»¥åŠè¯·æ±‚é€šè¿‡çš„æ—¥æœŸã€‚ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ±‚å‡ºå¥½å‹ç”³è¯·çš„é€šè¿‡ç‡ï¼Œç”¨ 2 ä½å°æ•°è¡¨ç¤ºã€‚é€šè¿‡ç‡ç”±æ¥å—å¥½å‹ç”³è¯·çš„æ•°ç›®é™¤ä»¥ç”³è¯·æ€»æ•°ã€‚ æç¤ºï¼š é€šè¿‡çš„å¥½å‹ç”³è¯·ä¸ä¸€å®šéƒ½åœ¨è¡¨ friend_request ä¸­ã€‚ä½ åªéœ€è¦ç»Ÿè®¡æ€»çš„è¢«é€šè¿‡çš„ç”³è¯·æ•°ï¼ˆä¸ç®¡å®ƒä»¬åœ¨ä¸åœ¨è¡¨ FriendRequest ä¸­ï¼‰ï¼Œå¹¶å°†å®ƒé™¤ä»¥ç”³è¯·æ€»æ•°ï¼Œå¾—åˆ°é€šè¿‡ç‡ ä¸€ä¸ªå¥½å‹ç”³è¯·å‘é€è€…æœ‰å¯èƒ½ä¼šç»™æ¥å—è€…å‘å‡ æ¡å¥½å‹ç”³è¯·ï¼Œä¹Ÿæœ‰å¯èƒ½ä¸€ä¸ªå¥½å‹ç”³è¯·ä¼šè¢«é€šè¿‡å¥½å‡ æ¬¡ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œé‡å¤çš„å¥½å‹ç”³è¯·åªç»Ÿè®¡ä¸€æ¬¡ã€‚ å¦‚æœä¸€ä¸ªå¥½å‹ç”³è¯·éƒ½æ²¡æœ‰ï¼Œé€šè¿‡ç‡ä¸º 0.00 ã€‚ SELECT ROUND(IFNULL((SELECT COUNT(DISTINCT requester_id,accepter_id) FROM RequestAccepted)/ (SELECT COUNT(DISTINCT sender_id ,send_to_id ) FROM FriendRequest ),0),2) accept_rate è¡¨ï¼šStadium Â±--------------Â±--------+ | Column Name | Type | Â±--------------Â±--------+ | id | int | | visit_date | date | | people | int | Â±--------------Â±--------+ visit_date æ˜¯è¡¨çš„ä¸»é”® æ¯æ—¥äººæµé‡ä¿¡æ¯è¢«è®°å½•åœ¨è¿™ä¸‰åˆ—ä¿¡æ¯ä¸­ï¼šåºå· (id)ã€æ—¥æœŸ (visit_date)ã€ äººæµé‡ (people) æ¯å¤©åªæœ‰ä¸€è¡Œè®°å½•ï¼Œæ—¥æœŸéšç€ id çš„å¢åŠ è€Œå¢åŠ  ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ä»¥æ‰¾å‡ºæ¯è¡Œçš„äººæ•°å¤§äºæˆ–ç­‰äº 100 ä¸” id è¿ç»­çš„ä¸‰è¡Œæˆ–æ›´å¤šè¡Œè®°å½•ã€‚ è¿”å›æŒ‰ visit_date å‡åºæ’åˆ—çš„ç»“æœè¡¨ã€‚ åœ¨ Facebook æˆ–è€… Twitter è¿™æ ·çš„ç¤¾äº¤åº”ç”¨ä¸­ï¼Œäººä»¬ç»å¸¸ä¼šå‘å¥½å‹ç”³è¯·ä¹Ÿä¼šæ”¶åˆ°å…¶ä»–äººçš„å¥½å‹ç”³è¯·ã€‚ è¡¨ request_accepted å­˜å‚¨äº†æ‰€æœ‰å¥½å‹ç”³è¯·é€šè¿‡çš„æ•°æ®è®°å½•ï¼Œå…¶ä¸­ï¼Œ requester_id å’Œ accepter_id éƒ½æ˜¯ç”¨æˆ·çš„ç¼–å·ã€‚ requester_id accepter_id accept_date 1 2 2016_06-03 1 3 2016-06-08 2 3 2016-06-08 3 4 2016-06-09 å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ±‚å‡ºè°æ‹¥æœ‰æœ€å¤šçš„å¥½å‹å’Œä»–æ‹¥æœ‰çš„å¥½å‹æ•°ç›®ã€‚å¯¹äºä¸Šé¢çš„æ ·ä¾‹æ•°æ®ï¼Œç»“æœä¸ºï¼š id num 3 3 æ³¨æ„ï¼š ä¿è¯æ‹¥æœ‰æœ€å¤šå¥½å‹æ•°ç›®çš„åªæœ‰ 1 ä¸ªäººã€‚ å¥½å‹ç”³è¯·åªä¼šè¢«æ¥å—ä¸€æ¬¡ï¼Œæ‰€ä»¥ä¸ä¼šæœ‰ requester_id å’Œ accepter_id å€¼éƒ½ç›¸åŒçš„é‡å¤è®°å½•ã€‚ SELECT id,COUNT(id) num FROM ((SELECT requester_id id FROM request_accepted) UNION ALL (SELECT accepter_id id FROM request_accepted)) sub GROUP BY id ORDER BY num DESC LIMIT 1; select id, sum(num) num from ((select requester_id id, count(*) num from request_accepted group by requester_id) union all (select accepter_id id, count(*) num from request_accepted group by accepter_id)) t3 group by id order by num desc limit 1 å‡ ä¸ªæœ‹å‹æ¥åˆ°ç”µå½±é™¢çš„å”®ç¥¨å¤„ï¼Œå‡†å¤‡é¢„çº¦è¿ç»­ç©ºä½™åº§ä½ã€‚ ä½ èƒ½åˆ©ç”¨è¡¨ cinema ï¼Œå¸®ä»–ä»¬å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œè·å–æ‰€æœ‰ç©ºä½™åº§ä½ï¼Œå¹¶å°†å®ƒä»¬æŒ‰ç…§ seat_id æ’åºåè¿”å›å—ï¼Ÿ seat_id free 1 1 2 0 3 1 4 1 5 1 **è¿ç»­å€¼é—®é¢˜æ€ä¹ˆå¤„ç†?**é¦–å…ˆæ˜¯ä¸¤å¼ è¡¨è‡ªè”ç»“ï¼Œç„¶åä¸¤å¼ è¡¨çš„é¡ºåºä½œå·®å€¼ç­‰äº1ï¼Œå–ç»å¯¹å€¼çš„è¯å°±ä¸è®ºå‰åäº† SELECT DISTINCT a.seat_id FROM cinema a INNER JOIN cinema b ON abs(b.seat_id-a.seat_id)=1 WHERE b.free=1 AND a.free=1 ORDER BY a.seat_id ASC; è¿˜æœ‰è¿™ç§lagå‡½æ•°çš„ç”¨æ³•,lagæ˜¯æå–ä¹‹å‰çš„å†…å®¹ï¼Œleadæ˜¯æå–ä¹‹åçš„å†…å®¹ select seat_id from ( select seat_id , free , lag(free,1,999) over() pre_free, lead(free,1,999) over() next_free from cinema )tmp where free=1 and (pre_free=1 or next_free=1 ) order by seat_id ç»™å®š 3 ä¸ªè¡¨ï¼š salespersonï¼Œ companyï¼Œ ordersã€‚ è¾“å‡ºæ‰€æœ‰è¡¨ salesperson ä¸­ï¼Œæ²¡æœ‰å‘å…¬å¸ â€˜REDâ€™ é”€å”®ä»»ä½•ä¸œè¥¿çš„é”€å”®å‘˜ã€‚ ç¤ºä¾‹ï¼š è¾“å…¥ è¡¨ï¼š salesperson Â±---------Â±-----Â±-------Â±----------------Â±----------+ | sales_id | name | salary | commission_rate | hire_date | Â±---------Â±-----Â±-------Â±----------------Â±----------+ | 1 | John | 100000 | 6 | 4/1/2006 | | 2 | Amy | 120000 | 5 | 5/1/2010 | | 3 | Mark | 65000 | 12 | 12/25/2008| | 4 | Pam | 25000 | 25 | 1/1/2005 | | 5 | Alex | 50000 | 10 | 2/3/2007 | Â±---------Â±-----Â±-------Â±----------------Â±----------+ è¡¨ salesperson å­˜å‚¨äº†æ‰€æœ‰é”€å”®å‘˜çš„ä¿¡æ¯ã€‚æ¯ä¸ªé”€å”®å‘˜éƒ½æœ‰ä¸€ä¸ªé”€å”®å‘˜ç¼–å· sales_id å’Œä»–çš„åå­— name ã€‚ è¡¨ï¼š company Â±--------Â±-------Â±-----------+ | com_id | name | city | Â±--------Â±-------Â±-----------+ | 1 | RED | Boston | | 2 | ORANGE | New York | | 3 | YELLOW | Boston | | 4 | GREEN | Austin | Â±--------Â±-------Â±-----------+ è¡¨ company å­˜å‚¨äº†æ‰€æœ‰å…¬å¸çš„ä¿¡æ¯ã€‚æ¯ä¸ªå…¬å¸éƒ½æœ‰ä¸€ä¸ªå…¬å¸ç¼–å· com_id å’Œå®ƒçš„åå­— name ã€‚ è¡¨ï¼š orders Â±---------Â±-----------Â±--------Â±---------Â±-------+ | order_id | order_date | com_id | sales_id | amount | Â±---------Â±-----------Â±--------Â±---------Â±-------+ | 1 | 1/1/2014 | 3 | 4 | 100000 | | 2 | 2/1/2014 | 4 | 5 | 5000 | | 3 | 3/1/2014 | 1 | 1 | 50000 | | 4 | 4/1/2014 | 1 | 4 | 25000 | Â±---------Â±---------Â±--------Â±---------Â±-------+ è¡¨ orders å­˜å‚¨äº†æ‰€æœ‰çš„é”€å”®æ•°æ®ï¼ŒåŒ…æ‹¬é”€å”®å‘˜ç¼–å· sales_id å’Œå…¬å¸ç¼–å· com_id ã€‚ SELECT name FROM salesperson WHERE sales_id NOT IN (SELECT sales_id FROM orders o INNER JOIN company c ON o.com_id=c.com_id WHERE c.name='RED') ç»™å®šä¸€ä¸ªè¡¨ treeï¼Œid æ˜¯æ ‘èŠ‚ç‚¹çš„ç¼–å·ï¼Œ p_id æ˜¯å®ƒçˆ¶èŠ‚ç‚¹çš„ id ã€‚ Â±â€”Â±-----+ | id | p_id | Â±â€”Â±-----+ | 1 | null | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 2 | Â±â€”Â±-----+ æ ‘ä¸­æ¯ä¸ªèŠ‚ç‚¹å±äºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š å¶å­ï¼šå¦‚æœè¿™ä¸ªèŠ‚ç‚¹æ²¡æœ‰ä»»ä½•å­©å­èŠ‚ç‚¹ã€‚ æ ¹ï¼šå¦‚æœè¿™ä¸ªèŠ‚ç‚¹æ˜¯æ•´æ£µæ ‘çš„æ ¹ï¼Œå³æ²¡æœ‰çˆ¶èŠ‚ç‚¹ã€‚ å†…éƒ¨èŠ‚ç‚¹ï¼šå¦‚æœè¿™ä¸ªèŠ‚ç‚¹æ—¢ä¸æ˜¯å¶å­èŠ‚ç‚¹ä¹Ÿä¸æ˜¯æ ¹èŠ‚ç‚¹ã€‚ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œè¾“å‡ºæ‰€æœ‰èŠ‚ç‚¹çš„ç¼–å·å’ŒèŠ‚ç‚¹çš„ç±»å‹ï¼Œå¹¶å°†ç»“æœæŒ‰ç…§èŠ‚ç‚¹ç¼–å·æ’åºã€‚ ä¸€ä¸ªå°å­¦ç”Ÿ Tim çš„ä½œä¸šæ˜¯åˆ¤æ–­ä¸‰æ¡çº¿æ®µæ˜¯å¦èƒ½å½¢æˆä¸€ä¸ªä¸‰è§’å½¢ã€‚ ç„¶è€Œï¼Œè¿™ä¸ªä½œä¸šéå¸¸ç¹é‡ï¼Œå› ä¸ºæœ‰å‡ ç™¾ç»„çº¿æ®µéœ€è¦åˆ¤æ–­ã€‚ å‡è®¾è¡¨ triangle ä¿å­˜äº†æ‰€æœ‰ä¸‰æ¡çº¿æ®µçš„é•¿åº¦ xã€yã€z ï¼Œè¯·ä½ å¸® Tim å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ¥åˆ¤æ–­æ¯ç»„ xã€yã€z æ˜¯å¦å¯ä»¥ç»„æˆä¸€ä¸ªä¸‰è§’å½¢ï¼Ÿ è®°ä¸€ä¸‹è¿™ç§ case thenç”¨æ³•,ä¹Ÿå¯ä»¥ä¸æŒ‡å®šå˜é‡ï¼Œåªåœ¨WHENä¸­åˆ¤æ–­ã€‚ SELECT x,y,z, CASE WHEN x+y&gt;z AND x+z&gt;y and y+z&gt;x then 'Yes' ELSE 'No' END triangle FROM triangle select x, y, z, if(x+y&gt;z and x+z&gt;y and y+z&gt;x, 'Yes', 'No') as triangle from triangle è¡¨ point_2d ä¿å­˜äº†æ‰€æœ‰ç‚¹ï¼ˆå¤šäº 2 ä¸ªç‚¹ï¼‰çš„åæ ‡ (x,y) ï¼Œè¿™äº›ç‚¹åœ¨å¹³é¢ä¸Šä¸¤ä¸¤ä¸é‡åˆã€‚ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥æ‰¾åˆ°ä¸¤ç‚¹ä¹‹é—´çš„æœ€è¿‘è·ç¦»ï¼Œä¿ç•™ 2 ä½å°æ•°ã€‚ x y -1 -1 0 0 -1 -2 æœ€è¿‘è·ç¦»åœ¨ç‚¹ (-1,-1) å’Œ(-1,2) ä¹‹é—´ï¼Œè·ç¦»ä¸º 1.00 ã€‚æ‰€ä»¥è¾“å‡ºåº”è¯¥ä¸ºï¼š shortest 1.00 SELECT ROUND(MIN(SQRT(POWER(p1.x-p2.x,2)+POWER(P1.y-p2.y,2))),2) shortest FROM point_2d p1,point_2d p2 WHERE p1.x!=p2.x OR p1.y!=p2.y select round(min(sqrt(pow(t1.x-t2.x,2)+pow(t1.y-t2.y,2))),2) shortest from point_2d as t1,point_2d as t2 where (t1.x,t1.y) != (t2.x,t2.y) è¡¨ point ä¿å­˜äº†ä¸€äº›ç‚¹åœ¨ x è½´ä¸Šçš„åæ ‡ï¼Œè¿™äº›åæ ‡éƒ½æ˜¯æ•´æ•°ã€‚ å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ‰¾åˆ°è¿™äº›ç‚¹ä¸­æœ€è¿‘ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ã€‚ x -1 0 2 æœ€è¿‘è·ç¦»æ˜¾ç„¶æ˜¯ â€˜1â€™ ï¼Œæ˜¯ç‚¹ â€˜-1â€™ å’Œ â€˜0â€™ ä¹‹é—´çš„è·ç¦»ã€‚æ‰€ä»¥è¾“å‡ºåº”è¯¥å¦‚ä¸‹ï¼š shortest 1 æ³¨æ„ï¼šæ¯ä¸ªç‚¹éƒ½ä¸å…¶ä»–ç‚¹åæ ‡ä¸åŒï¼Œè¡¨ table ä¸ä¼šæœ‰é‡å¤åæ ‡å‡ºç°ã€‚ SELECT MIN(ABS(p1.x-p2.x)) shortest FROM point p1,point p2 WHERE p1.x!=p2.x åœ¨ facebook ä¸­ï¼Œè¡¨ follow ä¼šæœ‰ 2 ä¸ªå­—æ®µï¼š followee, follower ï¼Œåˆ†åˆ«è¡¨ç¤ºè¢«å…³æ³¨è€…å’Œå…³æ³¨è€…ã€‚ è¯·å†™ä¸€ä¸ª sql æŸ¥è¯¢è¯­å¥ï¼Œå¯¹æ¯ä¸€ä¸ªå…³æ³¨è€…ï¼ŒæŸ¥è¯¢å…³æ³¨ä»–çš„å…³æ³¨è€…çš„æ•°ç›®ã€‚ æ¯”æ–¹è¯´ï¼š Â±------------Â±-----------+ | followee | follower | Â±------------Â±-----------+ | A | B | | B | C | | B | D | | D | E | Â±------------Â±-----------+ åº”è¯¥è¾“å‡ºï¼š Â±------------Â±-----------+ | follower | num | Â±------------Â±-----------+ | B | 2 | | D | 1 | Â±------------Â±-----------+ è§£é‡Šï¼š B å’Œ D éƒ½åœ¨åœ¨ follower å­—æ®µä¸­å‡ºç°ï¼Œä½œä¸ºè¢«å…³æ³¨è€…ï¼ŒB è¢« C å’Œ D å…³æ³¨ï¼ŒD è¢« E å…³æ³¨ã€‚A ä¸åœ¨ follower å­—æ®µå†…ï¼Œæ‰€ä»¥Aä¸åœ¨è¾“å‡ºåˆ—è¡¨ä¸­ã€‚ æ³¨æ„ï¼š è¢«å…³æ³¨è€…æ°¸è¿œä¸ä¼šè¢«ä»– / å¥¹è‡ªå·±å…³æ³¨ã€‚ å°†ç»“æœæŒ‰ç…§å­—å…¸åºè¿”å›ã€‚ SELECT followee follower,COUNT(DISTINCT follower) num FROM follow WHERE followee IN (SELECT follower FROM follow) GROUP BY followee ç»™å¦‚ä¸‹ä¸¤ä¸ªè¡¨ï¼Œå†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ±‚å‡ºåœ¨æ¯ä¸€ä¸ªå·¥èµ„å‘æ”¾æ—¥ï¼Œæ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„ä¸å…¬å¸çš„å¹³å‡å·¥èµ„çš„æ¯”è¾ƒç»“æœ ï¼ˆé«˜ / ä½ / ç›¸åŒï¼‰ã€‚ è¡¨ï¼š salary id employee_id amount pay_date 1 1 9000 2017-03-31 2 2 6000 2017-03-31 3 3 10000 2017-03-31 4 1 7000 2017-02-28 5 2 6000 2017-02-28 6 3 8000 2017-02-28 employee_id å­—æ®µæ˜¯è¡¨ employee ä¸­ employee_id å­—æ®µçš„å¤–é”®ã€‚ employee_id department_id 1 1 2 2 3 2 SELECT t1.pay_date pay_month,t2.department_id, CASE WHEN t1.avg_amount&gt;t2.avg_amount THEN 'lower' WHEN t1.avg_amount=t2.avg_amount THEN 'same' WHEN t1.avg_amount&lt;t2.avg_amount THEN 'higher' END comparison FROM (SELECT DATE_FORMAT(pay_date,'%Y-%m') pay_date,AVG(amount) avg_amount FROM salary GROUP BY DATE_FORMAT(pay_date,'%Y-%m') ) t1 INNER JOIN (SELECT DATE_FORMAT(s.pay_date,'%Y-%m') pay_date,e.department_id,AVG(s.amount) avg_amount FROM salary s INNER JOIN employee e ON s.employee_id=e.employee_id GROUP BY DATE_FORMAT(s.pay_date,'%Y-%m'),e.department_id ) t2 ON t1.pay_date=t2.pay_date ä¸‹é¢è¿™ä¸ªå†™çš„æ›´å¥½,OVER(PARTITION BY department_id,LEFT(pay_date,7)) SELECT pay_month,department_id,CASE WHEN DEP_AVG_Salary=Mon_AVG_Salary THEN 'same' WHEN DEP_AVG_Salary&gt;Mon_AVG_Salary THEN 'higher' ELSE 'lower' END 'comparison' FROM (SELECT DISTINCT(LEFT(pay_date,7)) as 'pay_month', e.department_id, AVG(amount) OVER(PARTITION BY e.department_id,LEFT(pay_date,7)) as 'DEP_AVG_Salary', AVG(amount) OVER(PARTITION BY LEFT(pay_date,7)) as 'Mon_AVG_Salary' FROM salary s LEFT JOIN employee e ON s.employee_id=e.employee_id) t ORDER BY pay_month DESC; ä¸€æ‰€ç¾å›½å¤§å­¦æœ‰æ¥è‡ªäºšæ´²ã€æ¬§æ´²å’Œç¾æ´²çš„å­¦ç”Ÿï¼Œä»–ä»¬çš„åœ°ç†ä¿¡æ¯å­˜æ”¾åœ¨å¦‚ä¸‹ student è¡¨ä¸­ã€‚ name continent Jack America Pascal Europe Xi Asia Jane America å†™ä¸€ä¸ªæŸ¥è¯¢è¯­å¥å®ç°å¯¹å¤§æ´²ï¼ˆcontinentï¼‰åˆ—çš„ é€è§†è¡¨ æ“ä½œï¼Œä½¿å¾—æ¯ä¸ªå­¦ç”ŸæŒ‰ç…§å§“åçš„å­—æ¯é¡ºåºä¾æ¬¡æ’åˆ—åœ¨å¯¹åº”çš„å¤§æ´²ä¸‹é¢ã€‚è¾“å‡ºçš„æ ‡é¢˜åº”ä¾æ¬¡ä¸ºç¾æ´²ï¼ˆAmericaï¼‰ã€äºšæ´²ï¼ˆAsiaï¼‰å’Œæ¬§æ´²ï¼ˆEuropeï¼‰ã€‚æ•°æ®ä¿è¯æ¥è‡ªç¾æ´²çš„å­¦ç”Ÿä¸å°‘äºæ¥è‡ªäºšæ´²æˆ–è€…æ¬§æ´²çš„å­¦ç”Ÿã€‚ å¯¹äºæ ·ä¾‹è¾“å…¥ï¼Œå®ƒçš„å¯¹åº”è¾“å‡ºæ˜¯ï¼š America Asia Europe Jack Xi Pascal Jane **è¡Œåˆ—è½¬æ¢æ€ä¹ˆå¼„å‘¢ï¼Ÿ**å…ˆè®¡ç®—ROW_NUMBER() OVER(PARTITION BY continent ORDER BY name) åˆ†ç»„æ’åºï¼Œ ç„¶åGROUP BYï¼ŒMAX,å†CASEæ“ä½œã€‚ SELECT MAX(CASE WHEN continent='America' THEN name END) AS America, MAX(CASE WHEN continent='Asia' THEN name END) AS Asia, MAX(CASE WHEN continent='Europe' THEN name END) AS Europe FROM (SELECT name,continent,ROW_NUMBER() OVER(PARTITION BY continent ORDER BY name) rk FROM student) T GROUP BY rk; è¡¨ my_numbers çš„ num å­—æ®µåŒ…å«å¾ˆå¤šæ•°å­—ï¼Œå…¶ä¸­åŒ…æ‹¬å¾ˆå¤šé‡å¤çš„æ•°å­—ã€‚ ä½ èƒ½å†™ä¸€ä¸ª SQL æŸ¥è¯¢è¯­å¥ï¼Œæ‰¾åˆ°åªå‡ºç°è¿‡ä¸€æ¬¡çš„æ•°å­—ä¸­ï¼Œæœ€å¤§çš„ä¸€ä¸ªæ•°å­—å—ï¼Ÿ Â±â€“+ |num| Â±â€“+ | 8 | | 8 | | 3 | | 3 | | 1 | | 4 | | 5 | | 6 | SELECT MAX(num) num FROM (SELECT num,COUNT(num) n_num FROM my_numbers GROUP BY num) t WHERE n_num=1; SELECT CASE WHEN COUNT(*)=1 THEN num ELSE NULL END num FROM my_numbers GROUP BY num ORDER BY num DESC LIMIT 1 æŸåŸå¸‚å¼€äº†ä¸€å®¶æ–°çš„ç”µå½±é™¢ï¼Œå¸å¼•äº†å¾ˆå¤šäººè¿‡æ¥çœ‹ç”µå½±ã€‚è¯¥ç”µå½±é™¢ç‰¹åˆ«æ³¨æ„ç”¨æˆ·ä½“éªŒï¼Œä¸“é—¨æœ‰ä¸ª LEDæ˜¾ç¤ºæ¿åšç”µå½±æ¨èï¼Œä¸Šé¢å…¬å¸ƒç€å½±è¯„å’Œç›¸å…³ç”µå½±æè¿°ã€‚ ä½œä¸ºè¯¥ç”µå½±é™¢çš„ä¿¡æ¯éƒ¨ä¸»ç®¡ï¼Œæ‚¨éœ€è¦ç¼–å†™ä¸€ä¸ª SQLæŸ¥è¯¢ï¼Œæ‰¾å‡ºæ‰€æœ‰å½±ç‰‡æè¿°ä¸ºé boring (ä¸æ— èŠ) çš„å¹¶ä¸” id ä¸ºå¥‡æ•° çš„å½±ç‰‡ï¼Œç»“æœè¯·æŒ‰ç­‰çº§ rating æ’åˆ—ã€‚ ä¾‹å¦‚ï¼Œä¸‹è¡¨ cinema: Â±--------Â±----------Â±-------------Â±----------+ | id | movie | description | rating | Â±--------Â±----------Â±-------------Â±----------+ | 1 | War | great 3D | 8.9 | | 2 | Science | fiction | 8.5 | | 3 | irish | boring | 6.2 | | 4 | Ice song | Fantacy | 8.6 | | 5 | House card| Interesting| 9.1 | Â±--------Â±----------Â±-------------Â±----------+ SELECT * FROM cinema WHERE description!='boring' AND id%2=1 ORDER BY rating DESC ç»™å®šä¸€ä¸ª salary è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œæœ‰ m = ç”·æ€§ å’Œ f = å¥³æ€§ çš„å€¼ã€‚äº¤æ¢æ‰€æœ‰çš„ f å’Œ m å€¼ï¼ˆä¾‹å¦‚ï¼Œå°†æ‰€æœ‰ f å€¼æ›´æ”¹ä¸º mï¼Œåä¹‹äº¦ç„¶ï¼‰ã€‚è¦æ±‚åªä½¿ç”¨ä¸€ä¸ªæ›´æ–°ï¼ˆUpdateï¼‰è¯­å¥ï¼Œå¹¶ä¸”æ²¡æœ‰ä¸­é—´çš„ä¸´æ—¶è¡¨ã€‚ æ³¨æ„ï¼Œæ‚¨å¿…åªèƒ½å†™ä¸€ä¸ª Update è¯­å¥ï¼Œè¯·ä¸è¦ç¼–å†™ä»»ä½• Select è¯­å¥ã€‚ UPDATE salary SET sex=IF(sex='m','f','m') æ„Ÿè§‰caseå¯ä»¥å¤„ç†çš„é—®é¢˜æ›´å¤šä¸€ç‚¹ UPDATE salary SET sex=(CASE sex WHEN 'm' THEN 'f' ELSE 'm' END) Customer è¡¨ï¼š Â±------------Â±--------+ | Column Name | Type | Â±------------Â±--------+ | customer_id | int | | product_key | int | Â±------------Â±--------+ product_key æ˜¯ Customer è¡¨çš„å¤–é”®ã€‚ Product è¡¨ï¼š Â±------------Â±--------+ | Column Name | Type | Â±------------Â±--------+ | product_key | int | Â±------------Â±--------+ product_key æ˜¯è¿™å¼ è¡¨çš„ä¸»é”®ã€‚ å†™ä¸€æ¡ SQL æŸ¥è¯¢è¯­å¥ï¼Œä» Customer è¡¨ä¸­æŸ¥è¯¢è´­ä¹°äº† Product è¡¨ä¸­æ‰€æœ‰äº§å“çš„å®¢æˆ·çš„ idã€‚ SELECT customer_id FROM Customer GROUP BY customer_id HAVING COUNT(DISTINCT product_key)=( SELECT COUNT(DISTINCT product_key) FROM Product ) ActorDirector è¡¨ï¼š Â±------------Â±--------+ | Column Name | Type | Â±------------Â±--------+ | actor_id | int | | director_id | int | | timestamp | int | Â±------------Â±--------+ timestamp æ˜¯è¿™å¼ è¡¨çš„ä¸»é”®. å†™ä¸€æ¡SQLæŸ¥è¯¢è¯­å¥è·å–åˆä½œè¿‡è‡³å°‘ä¸‰æ¬¡çš„æ¼”å‘˜å’Œå¯¼æ¼”çš„ id å¯¹ (actor_id, director_id) SELECT actor_id,director_id FROM ActorDirector GROUP BY actor_id,director_id HAVING COUNT(*)&gt;=3 é”€å”®è¡¨ Salesï¼š Â±------------Â±------+ | Column Name | Type | Â±------------Â±------+ | sale_id | int | | product_id | int | | year | int | | quantity | int | | price | int | Â±------------Â±------+ (sale_id, year) æ˜¯é”€å”®è¡¨ Sales çš„ä¸»é”®. product_id æ˜¯å…³è”åˆ°äº§å“è¡¨ Product çš„å¤–é”®. æ³¨æ„: price è¡¨ç¤ºæ¯å•ä½ä»·æ ¼ äº§å“è¡¨ Productï¼š Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | product_id | int | | product_name | varchar | Â±-------------Â±--------+ product_id æ˜¯è¡¨çš„ä¸»é”®. å†™ä¸€æ¡SQL æŸ¥è¯¢è¯­å¥è·å– Sales è¡¨ä¸­æ‰€æœ‰äº§å“å¯¹åº”çš„ äº§å“åç§° product_name ä»¥åŠè¯¥äº§å“çš„æ‰€æœ‰ å”®å–å¹´ä»½ year å’Œ ä»·æ ¼ price ã€‚ SELECT p.product_name,s.year,s.price FROM Sales s INNER JOIN Product p ON s.product_id=p.product_id é”€å”®è¡¨ï¼šSales Â±------------Â±------+ | Column Name | Type | Â±------------Â±------+ | sale_id | int | | product_id | int | | year | int | | quantity | int | | price | int | Â±------------Â±------+ sale_id æ˜¯è¿™ä¸ªè¡¨çš„ä¸»é”®ã€‚ product_id æ˜¯ Product è¡¨çš„å¤–é”®ã€‚ è¯·æ³¨æ„ä»·æ ¼æ˜¯æ¯å•ä½çš„ã€‚ äº§å“è¡¨ï¼šProduct Â±-------------Â±--------+ | Column Name | Type | Â±-------------Â±--------+ | product_id | int | | product_name | varchar | Â±-------------Â±--------+ product_id æ˜¯è¿™ä¸ªè¡¨çš„ä¸»é”®ã€‚ ç¼–å†™ä¸€ä¸ª SQL æŸ¥è¯¢ï¼ŒæŒ‰äº§å“ id product_id æ¥ç»Ÿè®¡æ¯ä¸ªäº§å“çš„é”€å”®æ€»é‡ã€‚ SELECT product_id,SUM(quantity) total_quantity FROM Sales GROUP BY product_id","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"sql","slug":"æ•°æ®åˆ†æ/sql","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/sql/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"sql","slug":"sql","permalink":"https://mz2sj.github.io/tags/sql/"},{"name":"åˆ·é¢˜","slug":"åˆ·é¢˜","permalink":"https://mz2sj.github.io/tags/%E5%88%B7%E9%A2%98/"}]},{"title":"æ€¨å¿µ","slug":"æ€¨å¿µ","date":"2020-06-22T10:32:52.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/06/22/æ€¨å¿µ/","link":"","permalink":"https://mz2sj.github.io/2020/06/22/%E6%80%A8%E5%BF%B5/","excerpt":"","text":"è¿™æ˜¯è¿œç¦»åå¿ƒæƒ…çš„å¸–å­â€¦ å•Š,2020å¹´çš„å…­æœˆä¸‹æ—¬.å…­æœˆçš„é›¨ä½ ä»€ä¹ˆæ—¶å€™ç»“æŸå‘€?æ˜å¤©å’Œå¯¼å¸ˆèŠä»€ä¹ˆå‘¢?è¯´ä½ è‡ªå·±ä»€ä¹ˆä¹Ÿæ²¡å¹²å—?å‘µå‘µ.å¥½å¥½å¹²å§. æˆ‘çˆ±ç”Ÿæ´»,ç”Ÿæ´»çˆ±æˆ‘.","categories":[{"name":"å†è§åå¿ƒæƒ…","slug":"å†è§åå¿ƒæƒ…","permalink":"https://mz2sj.github.io/categories/%E5%86%8D%E8%A7%81%E5%9D%8F%E5%BF%83%E6%83%85/"}],"tags":[]},{"title":"01-javaæ‰«ç›²","slug":"01-javaæ‰«ç›²","date":"2020-06-17T15:06:01.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/06/17/01-javaæ‰«ç›²/","link":"","permalink":"https://mz2sj.github.io/2020/06/17/01-java%E6%89%AB%E7%9B%B2/","excerpt":"","text":"ç°åœ¨çœŸçš„å¥½è¿·èŒ«å‘€ï¼Œç ”ç©¶ç”Ÿçš„äº‹è®©è‡ªå·±åˆçƒ¦åˆæ€•ï¼Œå­¦å­¦javaå‹å‹æƒŠï¼Œè€å¤©ä¿ä½‘æˆ‘å§ã€‚ å…³é”®å­— è¢«javaè¯­è¨€èµ‹äºˆç‰¹å®šå«ä¹‰çš„å­—ï¼Œç»„æˆå…³é”®å­—å•è¯çš„å­—æ¯å…¨éƒ¨å°å†™ï¼Œgotoå’Œconstæ˜¯ä¿ç•™å­—ã€‚ æ ‡è¯†ç¬¦ æ ‡è¯†ç¬¦å°±æ˜¯ç»™ç±»ã€æ¥å£ã€æ–¹æ³•ã€å˜é‡ç­‰èµ·åå­—ã€‚ä¸èƒ½ä»¥æ•°å­—å¼€å¤´ï¼Œä¸èƒ½æ˜¯javaä¸­çš„å…³é”®å­—ï¼Œæ ‡è¯†ç¬¦ä¸¥æ ¼åŒºåˆ†å¤§å°å†™ã€‚æ ‡è¯†ç¬¦çš„è‹±æ–‡æ‹¼å†™ä¸­ï¼Œå¸¸é‡æ¯ä¸ªå­—æ¯éƒ½å¤§å†™ï¼Œç”¨_éš”å¼€ï¼Œä¾‹å¦‚STUDENT_MAX_AGEã€‚ mainæ–¹æ³• mainæ–¹æ³•æ˜¯ç¨‹åºçš„ä¸»æ–¹æ³•ï¼Œæ˜¯ç¨‹åºçš„å…¥å£ï¼Œè¢«jvmè°ƒç”¨ã€‚ åç è¡¥ç  è®¡ç®—æœºåœ¨æ“ä½œæ—¶ï¼Œé‡‡ç”¨æ•°æ®å¯¹åº”çš„äºŒè¿›åˆ¶çš„è¡¥ç æ¥è®¡ç®—ã€‚æ­£æ•°çš„åŸç æœ€é«˜ä½æ˜¯0ï¼Œè´Ÿæ•°çš„åŸç æœ€é«˜ä½æ˜¯1ï¼Œå…¶ä»–éƒ½æ˜¯æ•°å€¼ä¸ºã€‚ æ­£æ•°çš„åç ä¸åŸç ç›¸åŒï¼Œè´Ÿæ•°çš„åç ä¸åŸç ç¬¦å·ä½ç›¸åŒï¼Œæ•°å€¼ä½å–åã€‚ æ­£æ•°çš„è¡¥ç ä¸åŸç ç›¸åŒï¼Œè´Ÿæ•°çš„è¡¥ç æ˜¯åœ¨åç çš„åŸºç¡€ä¸ŠåŠ ä¸Š1. åŸºæœ¬æ•°æ®ç±»å‹ æ•´æ•° å…³é”®å­— å ç”¨å­—èŠ‚æ•° byte 1 short 2 int 4 long 8 æµ®ç‚¹æ•° å…³é”®å­— å ç”¨å­—èŠ‚æ•° float 4 double 8 å­—ç¬¦ å…³é”®å­— å ç”¨å­—èŠ‚æ•° char 2 å¸ƒå°” å…³é”®å­— å ç”¨å­—èŠ‚æ•° boolean 1 å¸ƒå°”ç±»å‹ä¸å‚ä¸è½¬æ¢ æ•´æ•°é»˜è®¤æ˜¯intç±»å‹ï¼Œæµ®ç‚¹æ•°é»˜è®¤æ˜¯doubleç±»å‹ã€‚é•¿æ•´å‹åç¼€ç”¨Læˆ–è€…læ ‡è®°ï¼Œå»ºè®®ä½¿ç”¨Lã€‚ å•ç²¾åº¦æµ®ç‚¹æ•°ç”¨Fæˆ–è€…fæ ‡è®°ï¼Œå»ºè®®ä½¿ç”¨Fã€‚ é»˜è®¤è½¬æ¢ byte,short,char-int-long-float-double byte,short,charç›¸äº’ä¹‹é—´ä¸èƒ½è½¬æ¢ï¼Œå®ƒä»¬å‚ä¸è¿ç®—é¦–å…ˆè½¬æ¢æˆintç±»å‹ã€‚ å…³äºç²¾åº¦çš„ä¸€äº›é—®é¢˜ å¦‚è¦å£°æ˜ä¸€ä¸ªå¸¸é‡ä¸º float å‹ï¼Œåˆ™éœ€åœ¨æ•°å­—åé¢åŠ  f æˆ– F å˜é‡ã€å¸¸é‡ç›¸åŠ  byte b1=3,b2=4,b; b=b1+b2; é”™è¯¯ï¼Œb1å’Œb2æ˜¯å˜é‡ï¼Œå˜é‡çš„å€¼ä¼šå˜åŒ–ï¼Œä¸ç¡®å®šå…·ä½“çš„å€¼ï¼Œæ‰€ä»¥é»˜è®¤ä½¿ç”¨intã€‚byteã€shortã€charç›¸åŠ ä¼šè½¬æ¢ä¸ºintç±»å‹åšè¿ç®—ï¼Œè½¬ä¸ºbyteä¼šé™ä½ç²¾åº¦ã€‚ b=3+4; æ­£ç¡®ï¼Œå¸¸é‡ç›¸åŠ ï¼Œå…ˆåšåŠ æ³•ï¼Œç„¶åçœ‹ç»“æœæ˜¯å¦åœ¨èµ‹å€¼çš„æ•°æ®ç±»å‹èŒƒå›´å†…ï¼Œåœ¨çš„è¯å³æ­£ç¡® å¼ºåˆ¶è½¬æ¢ ä»å¤§çš„ç±»å‹è½¬æ¢ä¸ºå°çš„ç±»å‹ ç›®æ ‡æ•°æ®ç±»å‹ å˜é‡=(ç›®æ ‡æ•°æ®ç±»å‹)(è¢«è½¬æ¢çš„æ•°æ®)","categories":[{"name":"java","slug":"java","permalink":"https://mz2sj.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://mz2sj.github.io/tags/java/"}]},{"title":"05-CVå…¥é—¨ä¹‹æ¨¡å‹é›†æˆ","slug":"05-CVå…¥é—¨ä¹‹æ¨¡å‹é›†æˆ","date":"2020-06-02T02:13:51.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/06/02/05-CVå…¥é—¨ä¹‹æ¨¡å‹é›†æˆ/","link":"","permalink":"https://mz2sj.github.io/2020/06/02/05-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90/","excerpt":"","text":"é›†æˆå­¦ä¹ æ–¹æ³• åœ¨æœºå™¨å­¦ä¹ ä¸­çš„é›†æˆå­¦ä¹ å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜é¢„æµ‹ç²¾åº¦ï¼Œå¸¸è§çš„é›†æˆå­¦ä¹ æ–¹æ³•æœ‰Stackingã€Baggingå’ŒBoostingï¼ŒåŒæ—¶è¿™äº›é›†æˆå­¦ä¹ æ–¹æ³•ä¸å…·ä½“éªŒè¯é›†åˆ’åˆ†è”ç³»ç´§å¯†ã€‚ ç”±äºæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èˆ¬éœ€è¦è¾ƒé•¿çš„è®­ç»ƒå‘¨æœŸï¼Œå¦‚æœç¡¬ä»¶è®¾å¤‡ä¸å…è®¸å»ºè®®é€‰å–ç•™å‡ºæ³•ï¼Œå¦‚æœéœ€è¦è¿½æ±‚ç²¾åº¦å¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯çš„æ–¹æ³•ã€‚ ä¸‹é¢å‡è®¾æ„å»ºäº†10æŠ˜äº¤å‰éªŒè¯ï¼Œè®­ç»ƒå¾—åˆ°10ä¸ªCNNæ¨¡å‹ã€‚ é‚£ä¹ˆåœ¨10ä¸ªCNNæ¨¡å‹å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼è¿›è¡Œé›†æˆï¼š å¯¹é¢„æµ‹çš„ç»“æœçš„æ¦‚ç‡å€¼è¿›è¡Œå¹³å‡ï¼Œç„¶åè§£ç ä¸ºå…·ä½“å­—ç¬¦ï¼› å¯¹é¢„æµ‹çš„å­—ç¬¦è¿›è¡ŒæŠ•ç¥¨ï¼Œå¾—åˆ°æœ€ç»ˆå­—ç¬¦ã€‚ é™¤äº†ä¸Šé¢è¿™ç§æ–¹æ³•å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼Œå°†ä¸åŒæ¨¡å‹çš„ç»“æœè¿›è¡Œé›†æˆã€‚ æ·±åº¦å­¦ä¹ ä¸­çš„é›†æˆ Dropout Dropoutå¯ä»¥ä½œä¸ºè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„ä¸€ç§æŠ€å·§ã€‚åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­ï¼Œé€šè¿‡éšæœºè®©ä¸€éƒ¨åˆ†çš„èŠ‚ç‚¹åœæ­¢å·¥ä½œã€‚åŒæ—¶åœ¨é¢„æµ‹çš„è¿‡ç¨‹ä¸­è®©æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½å…¶ä½œç”¨ã€‚ Dropoutç»å¸¸å‡ºç°åœ¨åœ¨å…ˆæœ‰çš„CNNç½‘ç»œä¸­ï¼Œå¯ä»¥æœ‰æ•ˆçš„ç¼“è§£æ¨¡å‹è¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œä¹Ÿå¯ä»¥åœ¨é¢„æµ‹æ—¶å¢åŠ æ¨¡å‹çš„ç²¾åº¦ã€‚ åŠ å…¥Dropoutåçš„ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š # å®šä¹‰æ¨¡å‹ class SVHN_Model1(nn.Module): def __init__(self): super(SVHN_Model1, self).__init__() # CNNæå–ç‰¹å¾æ¨¡å— self.cnn = nn.Sequential( nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)), nn.ReLU(), nn.Dropout(0.25), nn.MaxPool2d(2), nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)), nn.ReLU(), nn.Dropout(0.25), nn.MaxPool2d(2), ) # self.fc1 = nn.Linear(32*3*7, 11) self.fc2 = nn.Linear(32*3*7, 11) self.fc3 = nn.Linear(32*3*7, 11) self.fc4 = nn.Linear(32*3*7, 11) self.fc5 = nn.Linear(32*3*7, 11) self.fc6 = nn.Linear(32*3*7, 11) def forward(self, img): feat = self.cnn(img) feat = feat.view(feat.shape[0], -1) c1 = self.fc1(feat) c2 = self.fc2(feat) c3 = self.fc3(feat) c4 = self.fc4(feat) c5 = self.fc5(feat) c6 = self.fc6(feat) return c1, c2, c3, c4, c5, c6 TTA æµ‹è¯•é›†æ•°æ®æ‰©å¢ï¼ˆTest Time Augmentationï¼Œç®€ç§°TTAï¼‰ä¹Ÿæ˜¯å¸¸ç”¨çš„é›†æˆå­¦ä¹ æŠ€å·§ï¼Œæ•°æ®æ‰©å¢ä¸ä»…å¯ä»¥åœ¨è®­ç»ƒæ—¶å€™ç”¨ï¼Œè€Œä¸”å¯ä»¥åŒæ ·åœ¨é¢„æµ‹æ—¶å€™è¿›è¡Œæ•°æ®æ‰©å¢ï¼Œå¯¹åŒä¸€ä¸ªæ ·æœ¬é¢„æµ‹ä¸‰æ¬¡ï¼Œç„¶åå¯¹ä¸‰æ¬¡ç»“æœè¿›è¡Œå¹³å‡ã€‚ def predict(test_loader, model, tta=10): model.eval() test_pred_tta = None # TTA æ¬¡æ•° for _ in range(tta): test_pred = [] with torch.no_grad(): for i, (input, target) in enumerate(test_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) output = np.concatenate([c0.data.numpy(), c1.data.numpy(), c2.data.numpy(), c3.data.numpy(), c4.data.numpy(), c5.data.numpy()], axis=1) test_pred.append(output) test_pred = np.vstack(test_pred) if test_pred_tta is None: test_pred_tta = test_pred else: test_pred_tta += test_pred return test_pred_tta Snapshot æœ¬ç« çš„å¼€å¤´å·²ç»æåˆ°ï¼Œå‡è®¾æˆ‘ä»¬è®­ç»ƒäº†10ä¸ªCNNåˆ™å¯ä»¥å°†å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡ã€‚ä½†æ˜¯å‡å¦‚åªè®­ç»ƒäº†ä¸€ä¸ªCNNæ¨¡å‹ï¼Œå¦‚ä½•åšæ¨¡å‹é›†æˆå‘¢ï¼Ÿ åœ¨è®ºæ–‡Snapshot Ensemblesä¸­ï¼Œä½œè€…æå‡ºä½¿ç”¨cyclical learning rateè¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¿å­˜ç²¾åº¦æ¯”è¾ƒå¥½çš„ä¸€äº›checkopintï¼Œæœ€åå°†å¤šä¸ªcheckpointè¿›è¡Œæ¨¡å‹é›†æˆã€‚ ç”±äºåœ¨cyclical learning rateä¸­å­¦ä¹ ç‡çš„å˜åŒ–æœ‰å‘¨æœŸæ€§å˜å¤§å’Œå‡å°‘çš„è¡Œä¸ºï¼Œå› æ­¤CNNæ¨¡å‹å¾ˆæœ‰å¯èƒ½åœ¨è·³å‡ºå±€éƒ¨æœ€ä¼˜è¿›å…¥å¦ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜ã€‚åœ¨Snapshotè®ºæ–‡ä¸­ä½œè€…é€šè¿‡ä½¿ç”¨è¡¨æ˜ï¼Œæ­¤ç§æ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜æ¨¡å‹ç²¾åº¦ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚ ç»“æœåå¤„ç† åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­å¯èƒ½ä¼šæœ‰ä¸åŒçš„è§£å†³æ–¹æ¡ˆï¼Œä¸åŒæ€è·¯çš„æ¨¡å‹ä¸ä»…å¯ä»¥äº’ç›¸å€Ÿé‰´ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä¿®æ­£æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚ åœ¨æœ¬æ¬¡èµ›é¢˜ä¸­ï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ€è·¯å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåå¤„ç†ï¼š ç»Ÿè®¡å›¾ç‰‡ä¸­æ¯ä¸ªä½ç½®å­—ç¬¦å‡ºç°çš„é¢‘ç‡ï¼Œä½¿ç”¨è§„åˆ™ä¿®æ­£ç»“æœï¼› å•ç‹¬è®­ç»ƒä¸€ä¸ªå­—ç¬¦é•¿åº¦é¢„æµ‹æ¨¡å‹ï¼Œç”¨æ¥é¢„æµ‹å›¾ç‰‡ä¸­å­—ç¬¦ä¸ªæ•°ï¼Œå¹¶ä¿®æ­£ç»“æœã€‚","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"04-CVå…¥é—¨ä¹‹æ¨¡å‹è®­ç»ƒä¸éªŒè¯","slug":"04-CVå…¥é—¨ä¹‹æ¨¡å‹è®­ç»ƒä¸éªŒè¯","date":"2020-05-30T12:07:08.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/05/30/04-CVå…¥é—¨ä¹‹æ¨¡å‹è®­ç»ƒä¸éªŒè¯/","link":"","permalink":"https://mz2sj.github.io/2020/05/30/04-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81/","excerpt":"","text":"æ•°æ®é›†åˆ’åˆ† åœ¨æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åªèƒ½åˆ©ç”¨è®­ç»ƒæ•°æ®æ¥è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹å¹¶ä¸èƒ½æ¥è§¦åˆ°æµ‹è¯•é›†ä¸Šçš„æ ·æœ¬ã€‚å› æ­¤æ¨¡å‹å¦‚æœå°†è®­ç»ƒé›†å­¦çš„è¿‡å¥½ï¼Œæ¨¡å‹å°±ä¼šè®°ä½è®­ç»ƒæ ·æœ¬çš„ç»†èŠ‚ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æµ‹è¯•é›†çš„æ³›åŒ–æ•ˆæœè¾ƒå·®ï¼Œè¿™ç§ç°è±¡ç§°ä¸ºè¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰ã€‚ä¸è¿‡æ‹Ÿåˆç›¸å¯¹åº”çš„æ˜¯æ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æ‹Ÿåˆæ•ˆæœè¾ƒå·®ã€‚ å¦‚å›¾æ‰€ç¤ºï¼šéšç€æ¨¡å‹å¤æ‚åº¦å’Œæ¨¡å‹è®­ç»ƒè½®æ•°çš„å¢åŠ ï¼ŒCNNæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¯¯å·®ä¼šé™ä½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šçš„è¯¯å·®ä¼šé€æ¸é™ä½ï¼Œç„¶åé€æ¸å‡é«˜ï¼Œè€Œæˆ‘ä»¬ä¸ºäº†è¿½æ±‚çš„æ˜¯æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ç²¾åº¦è¶Šé«˜è¶Šå¥½ ã€‚ è§£å†³ä¸Šè¿°é—®é¢˜æœ€å¥½çš„è§£å†³æ–¹æ³•ï¼šæ„å»ºä¸€ä¸ªä¸æµ‹è¯•é›†å°½å¯èƒ½åˆ†å¸ƒä¸€è‡´çš„æ ·æœ¬é›†ï¼ˆå¯ç§°ä¸ºéªŒè¯é›†ï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­éªŒè¯æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„ç²¾åº¦ï¼Œå¹¶ä»¥æ­¤æ§åˆ¶æ¨¡å‹çš„è®­ç»ƒã€‚ å› ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†æ˜¯åˆ†å¼€çš„ï¼Œæ‰€ä»¥æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šé¢çš„ç²¾åº¦åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥åæ˜ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨åˆ’åˆ†éªŒè¯é›†çš„æ—¶å€™ï¼Œéœ€è¦æ³¨æ„éªŒè¯é›†çš„åˆ†å¸ƒåº”è¯¥ä¸æµ‹è¯•é›†å°½é‡ä¿æŒä¸€è‡´ï¼Œä¸ç„¶æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„ç²¾åº¦å°±å¤±å»äº†æŒ‡å¯¼æ„ä¹‰ã€‚ ç•™å‡ºæ³• ç›´æ¥å°†è®­ç»ƒé›†åˆ’åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œæ–°çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚è¿™ç§åˆ’åˆ†æ–¹å¼çš„ä¼˜ç‚¹æ˜¯æœ€ä¸ºç›´æ¥ç®€å•ï¼›ç¼ºç‚¹æ˜¯åªå¾—åˆ°äº†ä¸€ä»½éªŒè¯é›†ï¼Œæœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¿‡æ‹Ÿåˆã€‚ç•™å‡ºæ³•åº”ç”¨åœºæ™¯æ˜¯æ•°æ®é‡æ¯”è¾ƒå¤§çš„æƒ…å†µã€‚ äº¤å‰éªŒè¯æ³• å°†è®­ç»ƒé›†åˆ’åˆ†æˆKä»½ï¼Œå°†å…¶ä¸­çš„K-1ä»½ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä½™çš„1ä»½ä½œä¸ºéªŒè¯é›†ï¼Œå¾ªç¯Kè®­ç»ƒã€‚è¿™ç§åˆ’åˆ†æ–¹å¼æ˜¯æ‰€æœ‰çš„è®­ç»ƒé›†éƒ½æ˜¯éªŒè¯é›†ï¼Œæœ€ç»ˆæ¨¡å‹éªŒè¯ç²¾åº¦æ˜¯Kä»½å¹³å‡å¾—åˆ°ã€‚è¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ˜¯éªŒè¯é›†ç²¾åº¦æ¯”è¾ƒå¯é ï¼Œè®­ç»ƒKæ¬¡å¯ä»¥å¾—åˆ°Kä¸ªæœ‰å¤šæ ·æ€§å·®å¼‚çš„æ¨¡å‹ï¼›CVéªŒè¯çš„ç¼ºç‚¹æ˜¯éœ€è¦è®­ç»ƒKæ¬¡ï¼Œä¸é€‚åˆæ•°æ®é‡å¾ˆå¤§çš„æƒ…å†µã€‚ è‡ªåŠ©é‡‡æ ·æ³• é€šè¿‡æœ‰æ”¾å›çš„é‡‡æ ·æ–¹å¼å¾—åˆ°æ–°çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ¯æ¬¡çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ½æ˜¯æœ‰åŒºåˆ«çš„ã€‚è¿™ç§åˆ’åˆ†æ–¹å¼ä¸€èˆ¬é€‚ç”¨äºæ•°æ®é‡è¾ƒå°çš„æƒ…å†µã€‚ æ¨¡å‹è®­ç»ƒä¸éªŒè¯ æ”¾ä¸‹ä»£ç ï¼Œå…¶å®ä¹‹å‰ä»£ç å·²ç»å†™çš„å·®ä¸å¤šäº† train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=10, shuffle=True, num_workers=10, ) val_loader = torch.utils.data.DataLoader( val_dataset, batch_size=10, shuffle=False, num_workers=10, ) model = SVHN_Model1() criterion = nn.CrossEntropyLoss (size_average=False) optimizer = torch.optim.Adam(model.parameters(), 0.001) best_loss = 1000.0 for epoch in range(20): print('Epoch: ', epoch) train(train_loader, model, criterion, optimizer, epoch) val_loss = validate(val_loader, model, criterion) # è®°å½•ä¸‹éªŒè¯é›†ç²¾åº¦ if val_loss &lt; best_loss: best_loss = val_loss torch.save(model.state_dict(), './model.pt') def train(train_loader, model, criterion, optimizer, epoch): # åˆ‡æ¢æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ model.train() for i, (input, target) in enumerate(train_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) loss = criterion(c0, data[1][:, 0]) + \\ criterion(c1, data[1][:, 1]) + \\ criterion(c2, data[1][:, 2]) + \\ criterion(c3, data[1][:, 3]) + \\ criterion(c4, data[1][:, 4]) + \\ criterion(c5, data[1][:, 5]) loss /= 6 optimizer.zero_grad() loss.backward() optimizer.step() def validate(val_loader, model, criterion): # åˆ‡æ¢æ¨¡å‹ä¸ºé¢„æµ‹æ¨¡å‹ model.eval() val_loss = [] # ä¸è®°å½•æ¨¡å‹æ¢¯åº¦ä¿¡æ¯ with torch.no_grad(): for i, (input, target) in enumerate(val_loader): c0, c1, c2, c3, c4, c5 = model(data[0]) loss = criterion(c0, data[1][:, 0]) + \\ criterion(c1, data[1][:, 1]) + \\ criterion(c2, data[1][:, 2]) + \\ criterion(c3, data[1][:, 3]) + \\ criterion(c4, data[1][:, 4]) + \\ criterion(c5, data[1][:, 5]) loss /= 6 val_loss.append(loss.item()) return np.mean(val_loss) torch.save(model_object.state_dict(), 'model.pt') model.load_state_dict(torch.load(' model.pt')) æ¨¡å‹è°ƒå‚ åœ¨å‚åŠ æœ¬æ¬¡æ¯”èµ›çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å»ºè®®å¤§å®¶ä»¥å¦‚ä¸‹é€»è¾‘å®Œæˆï¼š 1.åˆæ­¥æ„å»ºç®€å•çš„CNNæ¨¡å‹ï¼Œä¸ç”¨ç‰¹åˆ«å¤æ‚ï¼Œè·‘é€šè®­ç»ƒã€éªŒè¯å’Œé¢„æµ‹çš„æµç¨‹ï¼› 2.ç®€å•CNNæ¨¡å‹çš„æŸå¤±ä¼šæ¯”è¾ƒå¤§ï¼Œå°è¯•å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œå¹¶è§‚å¯ŸéªŒè¯é›†ç²¾åº¦ï¼› 3.åœ¨å¢åŠ æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶å¢åŠ æ•°æ®æ‰©å¢æ–¹æ³•ï¼Œç›´è‡³éªŒè¯é›†ç²¾åº¦ä¸å˜ã€‚ okï¼Œæœ‰æ—¶é—´å†ä¿®æ”¹ã€‚","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"03-CVå…¥é—¨ä¹‹æ¨¡å‹æ„å»º","slug":"03-CVå…¥é—¨ä¹‹æ¨¡å‹æ„å»º","date":"2020-05-26T13:15:59.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/26/03-CVå…¥é—¨ä¹‹æ¨¡å‹æ„å»º/","link":"","permalink":"https://mz2sj.github.io/2020/05/26/03-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/","excerpt":"","text":"CNNä»‹ç» â€‹ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆç®€ç§°CNNï¼‰æ˜¯ä¸€ç±»ç‰¹æ®Šçš„äººå·¥ç¥ç»ç½‘ç»œï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­é‡è¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚CNNåœ¨å¾ˆå¤šé¢†åŸŸéƒ½è¡¨ç°ä¼˜å¼‚ï¼Œç²¾åº¦å’Œé€Ÿåº¦æ¯”ä¼ ç»Ÿè®¡ç®—å­¦ä¹ ç®—æ³•é«˜å¾ˆå¤šã€‚ç‰¹åˆ«æ˜¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒCNNæ˜¯è§£å†³å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢ã€ç‰©ä½“æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²çš„ä¸»æµæ¨¡å‹ã€‚ â€‹ CNNæ˜¯ä¸€ç§å±‚æ¬¡æ¨¡å‹ï¼Œè¾“å…¥çš„æ˜¯åŸå§‹çš„åƒç´ æ•°æ®ã€‚CNNé€šè¿‡å·ç§¯ï¼ˆconvolutionï¼‰ã€æ± åŒ–ï¼ˆpoolingï¼‰ã€éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆnon-linear activation functionï¼‰å’Œå…¨è¿æ¥å±‚ï¼ˆfully connected layerï¼‰æ„æˆã€‚ åˆ©ç”¨pytorchæˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿å®ç°ã€‚ #é¡ºåºä¾æ¬¡æ˜¯å·ç§¯ã€æ¿€æ´»å‡½æ•°ã€æ± åŒ– cnn=nn.Sequential( nn.Conv2d(3,16,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16,32,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), ) â€‹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒCNNå…·æœ‰ä¸€ç§ç«¯åˆ°ç«¯ï¼ˆEnd to Endï¼‰çš„æ€è·¯ã€‚åœ¨CNNè®­ç»ƒçš„è¿‡ç¨‹ä¸­æ˜¯ç›´æ¥ä»å›¾åƒåƒç´ åˆ°æœ€ç»ˆçš„è¾“å‡ºï¼Œå¹¶ä¸æ¶‰åŠåˆ°å…·ä½“çš„ç‰¹å¾æå–å’Œæ„å»ºæ¨¡å‹çš„è¿‡ç¨‹ï¼Œä¹Ÿä¸éœ€è¦äººå·¥çš„å‚ä¸ã€‚ æ¨¡å‹æ„å»º ä¸‹é¢å†™ä¸€ä¸‹ä»£ç çš„å®Œæ•´æµç¨‹ #æ•°æ® import os,sys,glob,shutil,json import cv2 from PIL import Image import numpy as np import torch from torch.utils.data.dataset import Dataset import torchvision.transforms as transforms class SVHNDataset(Dataset): def __init__(self,img_path,img_label,transform=None): self.img_path=img_path self.img_label=img_label if transform is not None: self.transform=transform else: self.transform=None def __getitem__(self,index): img=Image.open(self.img_path[index]).convert('RGB') if self.transform is not None: img=self.transform(img) lbl=np.array(self.img_label[index],dtype=np.int) lbl=list(lbl)+(5-len(lbl))*[10] return img,torch.from_numpy(np.array(lbl[:6])) def __len__(self): return len(self.img_path) train_path=glob.glob('./data/mchar_train/*.png') train_path.sort() train_json=json.load(open('./data/mchar_train.json')) train_label=[train_json[x]['label'] for x in train_json] data=SVHNDataset(train_path,train_label,transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.2,0.2,0.2), transforms.RandomRotation(5), ])) train_loader=torch.utils.data.DataLoader( SVHNDataset(train_path,train_label, transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.3,0.3,0.2), transforms.RandomRotation(5), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ])), batch_size=10, shuffle=False, num_workers=10 ) #æ¨¡å‹ class SVHN_Model1(nn.Module): def __init__(self): super(SVHN_Model1,self).__init__() self.cnn=nn.Sequential( nn.Conv2d(3,16,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16,32,kernel_size=(3,3),stride=(2,2)), nn.ReLU(), nn.MaxPool2d(2), ) self.fc1=nn.Linear(32*3*7,11) self.fc2=nn.Linear(32*3*7,11) self.fc3=nn.Linear(32*3*7,11) self.fc4=nn.Linear(32*3*7,11) self.fc5=nn.Linear(32*3*7,11) def forward(self,img): feat=self.cnn(img) feat=feat.view(feat.shape[0],-1) c1=self.fc1(feat) c2=self.fc2(feat) c3=self.fc3(feat) c4=self.fc4(feat) c5=self.fc5(feat) return c1,c2,c3,c4,c5 #ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ class SVHN_Model2(nn.Module): def __init__(self): super(SVHN_Model2,self).__init__() model_conv=models.resnet18(pretrained=True) model_conv.avgpool=nn.AdaptivedAvgPool2d(1) model_conv=nn.Sequential(*list(model_conv.children())[:-1]) self.cnn=model_conv self.fc1=nn.Linear(512,11) self.fc2=nn.Linear(512,11) self.fc3=nn.Linear(512,11) self.fc3=nn.Linear(512,11) self.fc4=nn.Linear(512,11) self.fc5=nn.Linear(512,11) def forward(self,img): feat=self.cnn(img) feat=feat.view(feat.shape[0],-1) c1=self.fc1(feat) c2=self.fc2(feat) c3=self.fc3(feat) c4=self.fc4(feat) c5=self.fc5(feat) return c1,c2,c3,c4,c5 #pipeline model=SVHN_Model1() model.cuda() %%time criterion=nn.CrossEntropyLoss() optimizer=torch.optim.Adam(model.parameters(),0.005) loss_plot,c0_plot=[],[] for epoch in range(10): for data in train_loader: data[0]=data[0].cuda() data[1]=data[1].cuda() c0,c1,c2,c3,c4=model(data[0]) loss=criterion(c0,data[1][:,0].long())+\\ criterion(c1,data[1][:,1].long())+\\ criterion(c2,data[1][:,2].long())+\\ criterion(c3,data[1][:,3].long())+\\ criterion(c4,data[1][:,4].long()) loss/=5 optimizer.zero_grad() loss.backward() optimizer.step() loss_plot.append(loss.item()) c0_plot.append((c0.argmax(1)==data[1][:,0]).sum().item()*1.0/c0.shape[0]) print(epoch) è‡³æ­¤å°±å®Œæˆäº†ï¼Œè¿™é‡Œçš„pipelineå¹¶ä¸å®Œæ•´ï¼ŒåªåŒ…å«äº†è®­ç»ƒçš„ä»£ç ï¼Œè¿˜æœªåŒ…å«æ¨¡å‹çš„é¢„æµ‹ï¼Œæ¨¡å‹çš„è®­ç»ƒä¸éªŒè¯ä¸‹æ¬¡å†è¯´ã€‚","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"},{"name":"CNN","slug":"CNN","permalink":"https://mz2sj.github.io/tags/CNN/"}]},{"title":"02-CVå…¥é—¨ä¹‹æ•°æ®è¯»å–ä¸æ‰©å¢","slug":"02-CVå…¥é—¨ä¹‹æ•°æ®è¯»å–ä¸æ‰©å¢","date":"2020-05-23T13:03:16.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/23/02-CVå…¥é—¨ä¹‹æ•°æ®è¯»å–ä¸æ‰©å¢/","link":"","permalink":"https://mz2sj.github.io/2020/05/23/02-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%89%A9%E5%A2%9E/","excerpt":"","text":"Pillow Pilowæ˜¯pythonä¸­è¿›è¡Œå›¾åƒå¤„ç†çš„ä¸€ä¸ªåº“ï¼Œè¯ä¸å¤šè¯´çœ‹ä»£ç ã€‚ from PIL import Image im=Image.open('./data/cat.png') im è‡ªå·±æˆªäº†colabé‡Œé¢çš„çŒ«çŒ«ï¼Œå¾ˆå¯çˆ±å‘€ from PIL import Image,ImageFilter im=Image.open('./data/cat.png') im2=im.filter(ImageFilter.BLUR) im2.save('blur.jpg','jpeg') im2 å¯çˆ±çš„çŒ«çŒ«å˜ç³Šäº† from PIL import Image im=Image.open('./data/cat.png') im.thumbnail((im.width//2,im.height//2)) im å¯çˆ±çš„çŒ«çŒ«å˜å°äº† å¯¹å›¾åƒå¤„ç†å¹¶ä¸ç†Ÿæ‚‰ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯æ¸…æ¥šéœ€è¦å¯¹å›¾åƒå¤„ç†åšå“ªäº›å·¥ä½œï¼Œç„¶åå†å»æŸ¥æ‰¾ç›¸å…³API Opencv 2 import cv2 import matplotlib.pyplot as plt img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB) plt.imshow(img) opencvè¯»å‡ºæ¥çš„æ˜¯æ•°ç»„ï¼Œéœ€è¦ç”±å…¶ä»–åŒ…å¯è§†åŒ– img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) plt.imshow(img) ä»£ç è¯´çš„æ˜¯è½¬æˆç°åº¦å›¾åƒï¼Œä¸çŸ¥é“æˆ‘æ€ä¹ˆå°±å˜æˆäº†è¿™ä¸ªäºšå­ img=cv2.imread('./data/cat.png') img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) edges=cv2.Canny(img,30,70) plt.imshow(edges) ç»™çŒ«çŒ«ææè¾¹ æ•°æ®æ‰©å¢ å›¾åƒçš„æ•°æ®æ‰©å¢æ–¹æ³•åŒ…æ‹¬å„ç§æ—‹è½¬ä»¥åŠè£å‰ªä»¥åŠå¯¹åƒç´ çš„æ“ä½œã€‚ ä»¥torchvisionä¸ºä¾‹ï¼Œå¸¸è§çš„æ•°æ®æ‰©å¢æ–¹æ³•åŒ…æ‹¬ï¼š transforms.CenterCrop å¯¹å›¾ç‰‡ä¸­å¿ƒè¿›è¡Œè£å‰ª transforms.ColorJitter å¯¹å›¾åƒé¢œè‰²çš„å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å’Œé›¶åº¦è¿›è¡Œå˜æ¢ transforms.FiveCrop å¯¹å›¾åƒå››ä¸ªè§’å’Œä¸­å¿ƒè¿›è¡Œè£å‰ªå¾—åˆ°äº”åˆ†å›¾åƒ transforms.Grayscale å¯¹å›¾åƒè¿›è¡Œç°åº¦å˜æ¢ transforms.Pad ä½¿ç”¨å›ºå®šå€¼è¿›è¡Œåƒç´ å¡«å…… transforms.RandomAffine éšæœºä»¿å°„å˜æ¢ transforms.RandomCrop éšæœºåŒºåŸŸè£å‰ª transforms.RandomHorizontalFlip éšæœºæ°´å¹³ç¿»è½¬ transforms.RandomRotation éšæœºæ—‹è½¬ transforms.RandomVerticalFlip éšæœºå‚ç›´ç¿»è½¬ æ–°æ‰‹ç”¨ç”¨torchvisionè¶³å¤Ÿå•¦ Dataset import os,sys,glob,shutil,json import cv2 from PIL import Image import numpy as np import torch from torch.utils.data.dataset import Dataset import torchvision.transforms as transforms class SVHNDataset(Dataset): def __init__(self,img_path,img_label,transform=None): self.img_path=img_path self.img_label=img_label if transform is not None: self.transform=transform else: self.transform=None def __getitem__(self,index): img=Image.open(self.img_path[index]).convert('RGB') if self.transform is not None: img=self.transform(img) lbl=np.array(self.img_label[index],dtype=np.int) lbl=list(lbl)+(5-len(lbl))*[10] return img,torch.from_numpy(np.array(lbl[:5])) def __len__(self): return len(self.img_path) pytorchè‡ªå®šä¹‰Datasetéœ€è¦ç»§æ‰¿è‡ªpytorchå®šä¹‰çš„Datasetç±»ï¼Œæˆ‘ä»¬è¦å®ç°å…¶ä¸­çš„__init__()ã€len()ã€getitem()æ–¹æ³•å³å¯ï¼Œå¯¹æ•°æ®çš„æ“ä½œä¸»è¦æ”¾åœ¨__getitem__()æ–¹æ³•ä¸­,æ¯”å¦‚ä¹‹å‰æåˆ°çš„æ•°æ®æ‰©å¢æˆ‘ä»¬ä¹Ÿæ˜¯æ”¾åœ¨__getitem__()é‡Œé¢è¿›è¡Œæ“ä½œã€‚ train_path=glob.glob('./data/mchar_train/*.png') train_path.sort() train_json=json.load(open('./data/mchar_train.json')) train_label=[train_json[x]['label'] for x in train_json] data=SVHNDataset(train_path,train_label,transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.2,0.2,0.2), transforms.RandomRotation(5), ])) éœ€è¦æ³¨æ„çš„æ˜¯Datasetæ¯æ¬¡åªèƒ½è¿”å›ä¸€æ¡æ•°æ®ï¼Œæƒ³è¦æ„æˆbatchå‹æ•°æ®ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥æ„å»ºdataloader DataLoader dataloaderçš„ä»£ç ä¸éœ€è¦æˆ‘ä»¬é‡æ„ï¼Œç›´æ¥è°ƒç”¨å³å¯ ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³é‡‡ç”¨ç‰¹æ®Šçš„é‡‡æ ·æ–¹æ³•ç­‰å®šåˆ¶åŒ–åŠŸèƒ½æ—¶ï¼Œè¿˜éœ€è¦æˆ‘ä»¬å¯¹ä»£ç è¿›è¡Œé‡æ„ã€‚ train_loader=torch.utils.data.DataLoader( SVHNDataset(train_path,train_label, transforms.Compose([ transforms.Resize((64,128)), transforms.ColorJitter(0.3,0.3,0.2), transforms.RandomRotation(5), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ])), batch_size=10, shuffle=False, num_workers=10 ) æˆ‘ä»¬å¯ä»¥é€šè¿‡ next(iter(train_loader)) æ¥æŸ¥çœ‹å•ä¸ªbatchçš„æ•°æ®æƒ…å†µ æ”¶å·¥ï¼ï¼ï¼","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"},{"name":"pytorch","slug":"pytorch","permalink":"https://mz2sj.github.io/tags/pytorch/"}]},{"title":"01-CVå…¥é—¨ä¹‹èµ›é¢˜ç†è§£","slug":"01-CVå…¥é—¨ä¹‹èµ›é¢˜ç†è§£","date":"2020-05-20T13:53:20.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/05/20/01-CVå…¥é—¨ä¹‹èµ›é¢˜ç†è§£/","link":"","permalink":"https://mz2sj.github.io/2020/05/20/01-CV%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/","excerpt":"","text":"ç›®æ ‡ è¯†åˆ«å›¾åƒä¸­çš„å­—ç¬¦ä¸² ä¸ºäº†ç®€åŒ–éš¾åº¦ï¼Œä¸»åŠæ–¹æä¾›äº†å­—ç¬¦åœ¨å›¾ç‰‡ä¸­çš„å­—ç¬¦æ¡†ä½ç½®ï¼Œæˆ‘ä»¬åªéœ€è¯†åˆ«å­—ç¬¦æ¡†ä¸­çš„å­—ç¬¦å³å¯ã€‚ ä¸‹é¢çœ‹è¯»å–æ•°æ®çš„ä¸€ä¸ªä¾‹å­ import numpy as np #è¯»å–å›¾ç‰‡çš„èµ·å§‹ç»“å°¾ä½ç½®åŠæ ‡ç­¾ def parse_json(d): arr=np.array([ d['top'],d['height'],d['left'],d['width'],d['label'] ]) arr=arr.astype(int) return arr import cv2 img=cv2.imread('./data/mchar_train/000008.png') arr=parse_json(train_json['000008.png']) import matplotlib.pyplot as plt #æ˜¾ç¤ºå›¾ç‰‡ plt.figure(figsize=(10,10)) plt.subplot(1,arr.shape[1]+1,1) plt.imshow(img) å¯ä»¥çœ‹åˆ°128ä¸‰ä¸ªæ•°å­—ï¼Œç”±äºå·²ç»ç»™æˆ‘ä»¬åˆ†å¥½äº†æ¯ä¸ªå­—ç¬¦çš„èŒƒå›´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬æŠŠæ¯ä¸ªå­—ç¬¦ç”»å‡ºæ¥ for idx in range(arr.shape[1]): plt.subplot(1,arr.shape[1],idx+1) plt.imshow(img[arr[0,idx]:arr[0,idx]+arr[1,idx],arr[2,idx]:arr[2,idx]+arr[3,idx]]) plt.title(arr[4,idx]) plt.xticks([]);plt.yticks([]) ç³Šçš„ä¸è¡Œ è§£é¢˜æ€è·¯ å®šé•¿å­—ç¬¦è¯†åˆ« å¯ä»¥å°†èµ›é¢˜æŠ½è±¡ä¸ºä¸€ä¸ªå®šé•¿å­—ç¬¦è¯†åˆ«é—®é¢˜ï¼Œåœ¨èµ›é¢˜æ•°æ®é›†ä¸­å¤§éƒ¨åˆ†å›¾åƒä¸­å­—ç¬¦ä¸ªæ•°ä¸º2-4ä¸ªï¼Œæœ€å¤šçš„å­—ç¬¦ ä¸ªæ•°ä¸º6ä¸ªã€‚ å› æ­¤å¯ä»¥å¯¹äºæ‰€æœ‰çš„å›¾åƒéƒ½æŠ½è±¡ä¸º6ä¸ªå­—ç¬¦çš„è¯†åˆ«é—®é¢˜ï¼Œå­—ç¬¦23å¡«å……ä¸º23XXXXï¼Œå­—ç¬¦231å¡«å……ä¸º231XXXã€‚ ç»è¿‡å¡«å……ä¹‹åï¼ŒåŸå§‹çš„èµ›é¢˜å¯ä»¥ç®€åŒ–äº†6ä¸ªå­—ç¬¦çš„åˆ†ç±»é—®é¢˜ã€‚åœ¨æ¯ä¸ªå­—ç¬¦çš„åˆ†ç±»ä¸­ä¼šè¿›è¡Œ11ä¸ªç±»åˆ«çš„åˆ†ç±»ï¼Œå‡å¦‚åˆ†ç±»ä¸ºå¡«å……å­—ç¬¦ï¼Œåˆ™è¡¨æ˜è¯¥å­—ç¬¦ä¸ºç©ºã€‚ ä¸å®šé•¿å­—ç¬¦è¯†åˆ« åœ¨å­—ç¬¦è¯†åˆ«ç ”ç©¶ä¸­ï¼Œæœ‰ç‰¹å®šçš„æ–¹æ³•æ¥è§£å†³æ­¤ç§ä¸å®šé•¿çš„å­—ç¬¦è¯†åˆ«é—®é¢˜ï¼Œæ¯”è¾ƒå…¸å‹çš„æœ‰CRNNå­—ç¬¦è¯†åˆ«æ¨¡å‹ã€‚ åœ¨æœ¬æ¬¡èµ›é¢˜ä¸­ç»™å®šçš„å›¾åƒæ•°æ®éƒ½æ¯”è¾ƒè§„æ•´ï¼Œå¯ä»¥è§†ä¸ºä¸€ä¸ªå•è¯æˆ–è€…ä¸€ä¸ªå¥å­ æ£€æµ‹å†è¯†åˆ« åœ¨èµ›é¢˜æ•°æ®ä¸­å·²ç»ç»™å‡ºäº†è®­ç»ƒé›†ã€éªŒè¯é›†ä¸­æ‰€æœ‰å›¾ç‰‡ä¸­å­—ç¬¦çš„ä½ç½®ï¼Œå› æ­¤å¯ä»¥é¦–å…ˆå°†å­—ç¬¦çš„ä½ç½®è¿›è¡Œè¯†åˆ«ï¼Œåˆ©ç”¨ç‰©ä½“æ£€æµ‹çš„æ€è·¯å®Œæˆã€‚ éœ€è¦ç”¨åˆ°ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œé‚£æ˜¯åè¯å•¦ï¼Œå…ˆæŠŠçœ¼å‰çš„å¼„å¥½ã€‚","categories":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/categories/CV/"},{"name":"æ¯”èµ›","slug":"CV/æ¯”èµ›","permalink":"https://mz2sj.github.io/categories/CV/%E6%AF%94%E8%B5%9B/"}],"tags":[{"name":"CV","slug":"CV","permalink":"https://mz2sj.github.io/tags/CV/"}]},{"title":"04-çˆ¬è™«","slug":"04-çˆ¬è™«","date":"2020-04-27T08:57:35.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/27/04-çˆ¬è™«/","link":"","permalink":"https://mz2sj.github.io/2020/04/27/04-%E7%88%AC%E8%99%AB/","excerpt":"","text":"chromeç›‘æ§ajaxè¯·æ±‚ æµè§ˆå™¨ä¸­è¾“å…¥è…¾è®¯æ–°é—»çš„ç½‘å€ï¼Œå‘ç°ä¸æ–­å‘ä¸‹æ‹‰æ»šåŠ¨æ¡å°±å¯ä»¥æ–°é—»ä¼šä¸æ–­å‡ºç°ï¼Œåˆæ­¥åˆ¤æ–­æ–°å‡ºç°çš„æ–°é—»æ˜¯é€šè¿‡ajaxåŠ è½½çš„ã€‚æ‰“å¼€chromeçš„å¼€å‘è€…å·¥å…·ï¼Œè¿›å…¥networkä¸­çš„jsç•Œé¢ï¼Œå°†jsè„šæœ¬æŒ‰åç§°æ’åºï¼Œç»§ç»­å‘ä¸‹æ‹‰åŠ¨æ»šåŠ¨æ¡ï¼Œå¯ä»¥å‘ç°æ–°å‡ºç°çš„è¯·æ±‚æœ‰è§„å¾‹ã€‚ rcd?è¿™ä¸ªè¯·æ±‚éšç€æ»šåŠ¨æ¡çš„æ‹–åŠ¨ä¸æ–­å¢åŠ ï¼Œæˆ‘ä»¬æŠŠå‰å‡ æ¡è¯·æ±‚å•ç‹¬æ‹¿å‡ºæ¥åˆ†æã€‚ https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=0&amp;expIds=&amp;callback=__jp1 https://pacaio.match.qq.com/irs/rcd?cid=4&amp;token=9513f1a78a663e1d25b46a826f248c3c&amp;ext=&amp;page=0&amp;expIds=&amp;callback=__jp2 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=1&amp;expIds=20200427A0I0UN|20200427002954|20200402012032|20200427A0CB27|20200427A0H4VM|20200427A0GJP8|20200427002536|20200426005210|20200427A0CW43|20200427A0H11D&amp;callback=__jp4 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=2&amp;expIds=20200427A0BLNR|20200427A08JMI|20200426005565|20200427A0BBTS|20200427A088DC|20200427A05064|20200427A04L6I|20200427A066PB|20200427V05K49&amp;callback=__jp5 https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page=3&amp;expIds=20200427V01DY9|20200427000076|20200426A0O227|20200427A00E00|20200427A02TN7|20200426A0PR2B|20200426A0PLML|20200426A0HWYG|20200426A0Q1GM|20200426A0PGG8&amp;callback=__jp6 å‡ºå»å¼€å¤´çš„è¯·æ±‚è§„å¾‹ä¸è§„èŒƒå¤–ï¼Œå¯ä»¥å‘ç°è¯·æ±‚çš„ä¸€äº›è§„å¾‹ï¼Œç¬¬ä¸‰æ¡ä¹‹åçš„è¯·æ±‚ï¼šcid=137,page=1 2 3...,callback=__jp4 5 6,ä»ç¬¬äºŒæ¡å¼€å§‹çœ‹çš„è¯ï¼Œpage=0 1 2 3 åˆç»¼åˆåˆ†æä¸€ä¸‹ï¼Œç»è¿‡è‡ªå·±çš„å°è¯•æˆ‘ä»¬å¯ä»¥æ„é€ å‡ºä¸‹é¢çš„è¯·æ±‚ url='https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page={}'.format(i) åªéœ€è¦æ›´æ”¹iå°±å¯ä»¥è·å–ä¸åŒé¡µé¢çš„å†…å®¹ï¼Œå¹¶ä¸”æ˜¯ç›´æ¥è¿”å›jsonï¼Œçœå»è§£æçš„éº»çƒ¦ã€‚ ä¸ç”¨seleniumå°±å¯ä»¥çˆ¬å–æ–°é—» ä¸‹é¢æ”¾ä¸Šè‡ªå·±çš„ä»£ç  requestsçˆ¬å–è…¾è®¯æ–°é—» import time import requests url_fmt='https://pacaio.match.qq.com/irs/rcd?cid=137&amp;token=d0f13d594edfc180f5bf6b845456f3ea&amp;id=&amp;ext=top&amp;page={}' title_list=[] tags_list=[] keywords_list=[] vurl_list=[] intro_list=[] category_list=[] source_list=[] bimg_list=[] for page in range(100): url=url_fmt.format(page) res=requests.get(url) data=res.json()['data'] for new in data: try: title=new['title'] tags=new['tags'] keywords=new['keywords'] vurl=new['vurl'] intro=new['intro'] category=new['category'] source=new['source'] bimg=new['bimg'] except: continue title_list.append(title) tags_list.append(tags) keywords_list.append(keywords) vurl_list.append(vurl) intro_list.append(intro) category_list.append(category) source_list.append(source) bimg_list.append(bimg) time.sleep(2) import pandas as pd news=pd.DataFrame({'title':title_list, 'tags':tags_list, 'keywords':keywords_list, 'vurl':vurl_list, 'intro':intro_list, 'category':category_list, 'source':source_list, 'bimg':bimg_list}) news.to_csv('news.csv') å¤§åŠŸå‘Šæˆï¼ï¼ï¼ seleniumæ¨¡æ‹Ÿæ‹–åŠ¨æ»šåŠ¨æ¡ å¯ä»¥è®©seleniumæ¨¡æ‹Ÿæ‹–åŠ¨æ»šåŠ¨æ¡ä½¿æ–°é—»æ›´æ–°ï¼Œåœ¨è¿™é‡Œå‘ç°å±•ç¤ºçš„æ–°é—»æ•°é‡æ˜¯æœ‰é™çš„ï¼Œæ‹–åŠ¨åˆ°ä¸€å®šç¨‹åº¦å°†ä¸èƒ½å†æ‹–åŠ¨ã€‚ import time from selenium import webdriver from lxml import etree #æ¨¡æ‹Ÿæ‰“å¼€ç•Œé¢ driver=webdriver.Chrome(r'C:\\Users\\MZ\\Downloads\\chromedriver_win32\\chromedriver.exe') driver.get('https://news.qq.com/') driver.maximize_window() time.sleep(3) current_window_1=driver.current_window_handle print(current_window_1) #æ— æ³•æ‹–åŠ¨æ—¶é€€å‡ºæ‹–åŠ¨ï¼Œä½¿ç”¨chromeè‡ªå¸¦copy xpath æ„é€ è§£æå¼ number_before=0 for i in range(50): driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight-100)\") html=driver.page_source tree=etree.HTML(html) number_after=len(tree.xpath('/html/body/div/div[4]/div[2]/div/div/ul[2]/li'))-1 if number_before==number_after: break time.sleep(1) number_before=number_after html=driver.page_source print(number_after) news_number=len(tree.xpath('/html/body/div/div[4]/div[2]/div/div/ul[2]/li'))-1 title_fmt='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/img/@alt' img_fmt='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/img/@src' news_src='/html/body/div/div[4]/div[2]/div/div/ul[2]/li[{}]/a/@href' title_list=[] img_list=[] src_list=[] for number in range(news_number): try: title=tree.xpath(title_fmt.format(number))[0] img=tree.xpath(img_fmt.format(number))[0] src=tree.xpath(news_src.format(number))[0] except: continue title_list.append(title) img_list.append(img) src_list.append(src) news=pd.DataFrame({'title':title_list,'url':src_list,'img':img_list}) news.to_csv('news.csv') news.head(10)","categories":[{"name":"çˆ¬è™«","slug":"çˆ¬è™«","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"è…¾è®¯æ–°é—»çˆ¬è™«","slug":"è…¾è®¯æ–°é—»çˆ¬è™«","permalink":"https://mz2sj.github.io/tags/%E8%85%BE%E8%AE%AF%E6%96%B0%E9%97%BB%E7%88%AC%E8%99%AB/"},{"name":"ajax","slug":"ajax","permalink":"https://mz2sj.github.io/tags/ajax/"}]},{"title":"03-çˆ¬è™«","slug":"03-çˆ¬è™«","date":"2020-04-25T14:00:44.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/25/03-çˆ¬è™«/","link":"","permalink":"https://mz2sj.github.io/2020/04/25/03-%E7%88%AC%E8%99%AB/","excerpt":"","text":"proxy ip ä»£ç† ä»£ç†ä¸»è¦ç”¨æ¥è§£å†³ipé¢‘ç¹è®¿é—®è¢«æœåŠ¡å™¨å°å·çš„é—®é¢˜ï¼Œåœ¨requestsä¸­ä½¿ç”¨ä»£ç†å¾ˆç®€å•.proxiesæ ¼å¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼š{â€˜httpâ€™: â€˜[http://IP:portâ€˜,â€˜httpsâ€™:'https://IP:portâ€˜}](http://xn--ip:port%2Chttps:'https-mc9nd6a//IP:portâ€˜}),æˆ‘ä»¬å°†å®ƒä¼ å…¥requestsä¸­çš„getæ–¹æ³•å°±å¯ä»¥ä½¿ç”¨ requests.get(url, headers = headers, proxies = proxies, timeout = 10) æˆ‘ä»¬å¯ä»¥ä»ç½‘ä¸Šæä¾›çš„ä¸€äº›ä»£ç†æ± ä¸­è·å–æœ‰æ•ˆçš„ä»£ç†ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š from bs4 import BeautifulSoup import requests import re import json def open_proxy_url(url): user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' headers = {'User-Agent': user_agent} try: r = requests.get(url, headers = headers, timeout = 10) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: print('æ— æ³•è®¿é—®ç½‘é¡µ' + url) def get_proxy_ip(response): proxy_ip_list = [] soup = BeautifulSoup(response, 'html.parser') proxy_ips = soup.find(id = 'ip_list').find_all('tr') for proxy_ip in proxy_ips: if len(proxy_ip.select('td')) &gt;=8: ip = proxy_ip.select('td')[1].text port = proxy_ip.select('td')[2].text protocol = proxy_ip.select('td')[5].text if protocol in ('HTTP','HTTPS','http','https'): proxy_ip_list.append(f'{protocol}://{ip}:{port}') return proxy_ip_list def open_url_using_proxy(url, proxy): user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' headers = {'User-Agent': user_agent} proxies = {} if proxy.startswith(('HTTPS','https')): proxies['https'] = proxy else: proxies['http'] = proxy try: r = requests.get(url, headers = headers, proxies = proxies, timeout = 10) r.raise_for_status() r.encoding = r.apparent_encoding return (r.text, r.status_code) except: print('æ— æ³•è®¿é—®ç½‘é¡µ' + url) print('æ— æ•ˆä»£ç†IP: ' + proxy) return False def check_proxy_avaliability(proxy): url = 'http://www.baidu.com' result = open_url_using_proxy(url, proxy) VALID_PROXY = False if result: text, status_code = result if status_code == 200: r_title = re.findall('&lt;title&gt;.*&lt;/title&gt;', text) if r_title: if r_title[0] == '&lt;title&gt;ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“&lt;/title&gt;': VALID_PROXY = True if VALID_PROXY: check_ip_url = 'https://jsonip.com/' try: text, status_code = open_url_using_proxy(check_ip_url, proxy) except: return print('æœ‰æ•ˆä»£ç†IP: ' + proxy) with open('valid_proxy_ip.txt','a') as f: f.writelines(proxy) try: source_ip = json.loads(text).get('ip') print(f'æºIPåœ°å€ä¸ºï¼š{source_ip}') print('='*40) except: print('è¿”å›çš„éjson,æ— æ³•è§£æ') print(text) else: print('æ— æ•ˆä»£ç†IP: ' + proxy) if __name__ == '__main__': proxy_url = 'https://www.xicidaili.com/' proxy_ip_filename = 'proxy_ip.txt' text = open(proxy_ip_filename, 'r').read() proxy_ip_list = get_proxy_ip(text) for proxy in proxy_ip_list: check_proxy_avaliability(proxy) selenium seleniumæ˜¯ä»€ä¹ˆï¼šä¸€ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ï¼ˆå¤§å®¶éƒ½æ˜¯è¿™ä¹ˆè¯´çš„ï¼‰ seleniumåº”ç”¨åœºæ™¯ï¼šç”¨ä»£ç çš„æ–¹å¼å»æ¨¡æ‹Ÿæµè§ˆå™¨æ“ä½œè¿‡ç¨‹ï¼ˆå¦‚ï¼šæ‰“å¼€æµè§ˆå™¨ã€åœ¨è¾“å…¥æ¡†é‡Œè¾“å…¥æ–‡å­—ã€å›è½¦ç­‰ï¼‰ï¼Œåœ¨çˆ¬è™«æ–¹é¢å¾ˆæœ‰å¿…è¦ seleniumæ¨¡æ‹Ÿæµè§ˆå™¨è¿›è¡Œæ“ä½œï¼Œéœ€è¦ç”µè„‘å®‰è£…ç›¸åº”çš„æµè§ˆå™¨å’Œæµè§ˆå™¨é©±åŠ¨ 1ã€å¯¼å…¥æ¨¡å—ï¼š from selenium import webdriver # å¯åŠ¨æµè§ˆå™¨éœ€è¦ç”¨åˆ° from selenium.webdriver.common.keys import Keys # æä¾›é”®ç›˜æŒ‰é”®æ”¯æŒï¼ˆæœ€åä¸€ä¸ªKè¦å¤§å†™ï¼‰ 2ã€åˆ›å»ºä¸€ä¸ªWebDriverå®ä¾‹ï¼š driver = webdriver.Chrome(\"chromedriveré©±åŠ¨ç¨‹åºè·¯å¾„\") 3ã€æ‰“å¼€ä¸€ä¸ªé¡µé¢: driver.get(\"http://www.python.org\") # è¿™ä¸ªæ—¶å€™chromedriverä¼šæ‰“å¼€ä¸€ä¸ªChromeæµè§ˆå™¨çª—å£ï¼Œæ˜¾ç¤ºçš„æ˜¯ç½‘å€æ‰€å¯¹åº”çš„é¡µé¢ 4ã€å…³é—­é¡µé¢ driver.close() # å…³é—­æµè§ˆå™¨ä¸€ä¸ªTab # or driver.quit() # å…³é—­æµè§ˆå™¨çª—å£ é«˜çº§ æŸ¥æ‰¾å…ƒç´  åœ¨æ‰“å¼€é¡µé¢å’Œå…³é—­é¡µé¢ä¸­é—´ï¼Œå°±æ˜¯å„ç§æ“ä½œï¼è€ŒæŸ¥æ‰¾å…ƒç´ è¿™ä¸€ç‚¹ï¼Œå’Œçˆ¬è™«å¸¸è§çš„HTMLé¡µé¢è§£æï¼Œå®šä½åˆ°å…·ä½“çš„æŸä¸ªå…ƒç´ åŸºæœ¬ä¸€æ ·ï¼Œåªä¸è¿‡ï¼Œè°ƒç”¨è€…æ˜¯driver element = driver.find_element_by_name(\"q\") é«˜çº§ é¡µé¢äº¤äº’ æ‰¾åˆ°å…ƒç´ åï¼Œå°±æ˜¯è¿›è¡Œâ€œäº¤äº’â€ï¼Œå¦‚é”®ç›˜è¾“å…¥ï¼ˆéœ€æå‰å¯¼å…¥æ¨¡å—ï¼‰ element.send_keys(â€œsome textâ€ï¼‰ # å¾€ä¸€ä¸ªå¯ä»¥è¾“å…¥å¯¹è±¡ä¸­è¾“å…¥â€œsome textâ€ #ç”šè‡³ element.send_keys(Keys.RETURNï¼‰ # æ¨¡æ‹Ÿé”®ç›˜å›è½¦ #ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ç§æ–¹å¼è¾“å…¥åä¼šä¸€ç›´å­˜åœ¨ï¼Œè€Œè¦æ¸…ç©ºæŸä¸ªæ–‡æœ¬æ¡†ä¸­çš„æ–‡å­—ï¼Œå°±éœ€è¦ï¼š element.clear() # æ¸…ç©ºelementå¯¹è±¡ä¸­çš„æ–‡å­— sessionå’Œcookie Session æ˜¯ä¼šè¯çš„æ„æ€ï¼Œä¼šè¯æ˜¯äº§ç”Ÿåœ¨æœåŠ¡ç«¯çš„ï¼Œç”¨æ¥ä¿å­˜å½“å‰ç”¨æˆ·çš„ä¼šè¯ä¿¡æ¯ï¼Œè€Œ Cookies æ˜¯ä¿å­˜åœ¨å®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰ï¼Œæœ‰äº† Cookie ä»¥åï¼Œå®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰å†æ¬¡è®¿é—®æœåŠ¡ç«¯çš„æ—¶å€™ï¼Œä¼šå°†è¿™ä¸ª Cookie å¸¦ä¸Šï¼Œè¿™æ—¶ï¼ŒæœåŠ¡ç«¯å¯ä»¥é€šè¿‡ Cookie æ¥è¯†åˆ«æœ¬æ¬¡è¯·æ±‚åˆ°åº•æ˜¯è°åœ¨è®¿é—®ã€‚ å¯ä»¥ç®€å•ç†è§£ä¸º Cookies ä¸­ä¿å­˜äº†ç™»å½•å‡­è¯ï¼Œæˆ‘ä»¬åªè¦æŒæœ‰è¿™ä¸ªå‡­è¯ï¼Œå°±å¯ä»¥åœ¨æœåŠ¡ç«¯ä¿æŒä¸€ä¸ªç™»å½•çŠ¶æ€ã€‚ åœ¨çˆ¬è™«ä¸­ï¼Œæœ‰æ—¶å€™é‡åˆ°éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®çš„ç½‘é¡µï¼Œåªéœ€è¦åœ¨ç™»å½•åè·å–äº† Cookies ï¼Œåœ¨ä¸‹æ¬¡è®¿é—®çš„æ—¶å€™å°†ç™»å½•åè·å–åˆ°çš„ Cookies æ”¾åœ¨è¯·æ±‚å¤´ä¸­ï¼Œè¿™æ—¶ï¼ŒæœåŠ¡ç«¯å°±ä¼šè®¤ä¸ºæˆ‘ä»¬çš„çˆ¬è™«æ˜¯ä¸€ä¸ªæ­£å¸¸ç™»å½•ç”¨æˆ·ã€‚ session é‚£ä¹ˆï¼ŒCookies æ˜¯å¦‚ä½•ä¿æŒä¼šè¯çŠ¶æ€çš„å‘¢ï¼Ÿ åœ¨å®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰ç¬¬ä¸€æ¬¡è¯·æ±‚æœåŠ¡ç«¯çš„æ—¶å€™ï¼ŒæœåŠ¡ç«¯ä¼šè¿”å›ä¸€ä¸ªè¯·æ±‚å¤´ä¸­å¸¦æœ‰ Set-Cookie å­—æ®µçš„å“åº”ç»™å®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰ï¼Œç”¨æ¥æ ‡è®°æ˜¯å“ªä¸€ä¸ªç”¨æˆ·ï¼Œå®¢æˆ·ç«¯ï¼ˆæµè§ˆå™¨ï¼‰ä¼šæŠŠè¿™ä¸ª Cookies ç»™ä¿å­˜èµ·æ¥ã€‚ æˆ‘ä»¬æ¥ä½¿ç”¨å·¥å…· PostMan æ¥è®¿é—®ä¸‹æŸä¸œçš„ç™»å½•é¡µï¼Œçœ‹ä¸‹è¿”å›çš„å“åº”å¤´ï¼š å½“æˆ‘ä»¬è¾“å…¥å¥½ç”¨æˆ·åå’Œå¯†ç æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šå°†è¿™ä¸ª Cookies æ”¾åœ¨è¯·æ±‚å¤´ä¸€èµ·å‘é€ç»™æœåŠ¡ç«¯ï¼Œè¿™æ—¶ï¼ŒæœåŠ¡ç«¯å°±çŸ¥é“æ˜¯è°åœ¨è¿›è¡Œç™»å½•æ“ä½œï¼Œå¹¶ä¸”å¯ä»¥åˆ¤æ–­è¿™ä¸ªäººè¾“å…¥çš„ç”¨æˆ·åå’Œå¯†ç å¯¹ä¸å¯¹ï¼Œå¦‚æœè¾“å…¥æ­£ç¡®ï¼Œåˆ™åœ¨æœåŠ¡ç«¯çš„ Session è®°å½•ä¸€ä¸‹è¿™ä¸ªäººå·²ç»ç™»å½•æˆåŠŸäº†ï¼Œä¸‹æ¬¡å†è¯·æ±‚çš„æ—¶å€™è¿™ä¸ªäººå°±æ˜¯ç™»å½•çŠ¶æ€äº†ã€‚ å¦‚æœå®¢æˆ·ç«¯ä¼ ç»™æœåŠ¡ç«¯çš„ Cookies æ˜¯æ— æ•ˆçš„ï¼Œæˆ–è€…è¿™ä¸ª Cookies æ ¹æœ¬ä¸æ˜¯ç”±è¿™ä¸ªæœåŠ¡ç«¯ä¸‹å‘çš„ï¼Œæˆ–è€…è¿™ä¸ª Cookies å·²ç»è¿‡æœŸäº†ï¼Œé‚£ä¹ˆæ¥ä¸‹é‡Œçš„è¯·æ±‚å°†ä¸å†èƒ½è®¿é—®éœ€è¦ç™»å½•åæ‰èƒ½è®¿é—®çš„é¡µé¢ã€‚ æ‰€ä»¥ï¼Œ Session å’Œ Cookies ä¹‹é—´æ˜¯éœ€è¦ç›¸äº’é…åˆçš„ï¼Œä¸€ä¸ªåœ¨æœåŠ¡ç«¯ï¼Œä¸€ä¸ªåœ¨å®¢æˆ·ç«¯ã€‚ cookie æˆ‘ä»¬ç™»é™†ä¸€ä¸ªç½‘ç«™ï¼ŒChrome ä¸­æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œé€‰æ‹© Application æ ‡ç­¾ï¼Œç‚¹å¼€ Cookies è¿™ä¸€æ ã€‚ Nameï¼šè¿™ä¸ªæ˜¯ Cookie çš„åå­—ã€‚ä¸€æ—¦åˆ›å»ºï¼Œè¯¥åç§°ä¾¿ä¸å¯æ›´æ”¹ã€‚ Valueï¼šè¿™ä¸ªæ˜¯ Cookie çš„å€¼ã€‚ Domainï¼šè¿™ä¸ªæ˜¯å¯ä»¥è®¿é—®è¯¥ Cookie çš„åŸŸåã€‚ä¾‹å¦‚ï¼Œå¦‚æœè®¾ç½®ä¸º .jd.com ï¼Œåˆ™æ‰€æœ‰ä»¥ jd.com ï¼Œç»“å°¾çš„åŸŸåéƒ½å¯ä»¥è®¿é—®è¯¥Cookieã€‚ Max Ageï¼šCookie å¤±æ•ˆçš„æ—¶é—´ï¼Œå•ä½ä¸ºç§’ï¼Œä¹Ÿå¸¸å’Œ Expires ä¸€èµ·ä½¿ç”¨ã€‚ Max Age å¦‚æœä¸ºæ­£æ•°ï¼Œåˆ™åœ¨ Max Age ç§’ä¹‹åå¤±æ•ˆï¼Œå¦‚æœä¸ºè´Ÿæ•°ï¼Œåˆ™å…³é—­æµè§ˆå™¨æ—¶ Cookie å³å¤±æ•ˆï¼Œæµè§ˆå™¨ä¹Ÿä¸ä¼šä¿å­˜è¯¥ Cookie ã€‚ Pathï¼šCookie çš„ä½¿ç”¨è·¯å¾„ã€‚å¦‚æœè®¾ç½®ä¸º /path/ ï¼Œåˆ™åªæœ‰è·¯å¾„ä¸º /path/ çš„é¡µé¢å¯ä»¥è®¿é—®è¯¥ Cookie ã€‚å¦‚æœè®¾ç½®ä¸º / ï¼Œåˆ™æœ¬åŸŸåä¸‹çš„æ‰€æœ‰é¡µé¢éƒ½å¯ä»¥è®¿é—®è¯¥ Cookie ã€‚ Sizeï¼šCookie çš„å¤§å°ã€‚ HTTPOnlyï¼šå¦‚æœæ­¤é¡¹æ‰“å‹¾ï¼Œé‚£ä¹ˆé€šè¿‡ JS è„šæœ¬å°†æ— æ³•è¯»å–åˆ° Cookie ä¿¡æ¯ï¼Œè¿™æ ·èƒ½æœ‰æ•ˆçš„é˜²æ­¢ XSS æ”»å‡»ï¼Œçªƒå– Cookie å†…å®¹ï¼Œå¯ä»¥å¢åŠ  Cookie çš„å®‰å…¨æ€§ã€‚ Secureï¼šå¦‚æœæ­¤é¡¹æ‰“å‹¾ï¼Œé‚£ä¹ˆè¿™ä¸ª Cookie åªèƒ½ç”¨ HTTPS åè®®å‘é€ç»™æœåŠ¡å™¨ï¼Œç”¨ HTTP åè®®æ˜¯ä¸å‘é€çš„ã€‚ é‚£ä¹ˆæœ‰çš„ç½‘ç«™ä¸ºä»€ä¹ˆè¿™æ¬¡å…³é—­äº†ï¼Œä¸‹æ¬¡æ‰“å¼€çš„æ—¶å€™è¿˜æ˜¯ç™»å½•çŠ¶æ€å‘¢ï¼Ÿ è¿™å°±è¦è¯´åˆ° Cookie çš„æŒä¹…åŒ–äº†ï¼Œå…¶å®ä¹Ÿä¸èƒ½è¯´æ˜¯æŒä¹…åŒ–ï¼Œå°±æ˜¯ Cookie å¤±æ•ˆçš„æ—¶é—´è®¾ç½®çš„é•¿ä¸€ç‚¹ï¼Œæ¯”å¦‚ç›´æ¥è®¾ç½®åˆ° 2099 å¹´å¤±æ•ˆï¼Œè¿™æ ·ï¼Œåœ¨æµè§ˆå™¨å…³é—­åï¼Œè¿™ä¸ª Cookie æ˜¯ä¼šä¿å­˜åœ¨æˆ‘ä»¬çš„ç¡¬ç›˜ä¸­çš„ï¼Œä¸‹æ¬¡æ‰“å¼€æµè§ˆå™¨ï¼Œä¼šå†ä»æˆ‘ä»¬çš„ç¡¬ç›˜ä¸­å°†è¿™ä¸ª Cookie è¯»å–å‡ºæ¥ï¼Œç”¨æ¥ç»´æŒç”¨æˆ·çš„ä¼šè¯çŠ¶æ€ã€‚ ç¬¬äºŒä¸ªé—®é¢˜äº§ç”Ÿäº†ï¼ŒæœåŠ¡ç«¯çš„ä¼šè¯ä¹Ÿä¼šæ— é™çš„ç»´æŒä¸‹å»ä¹ˆï¼Œå½“ç„¶ä¸ä¼šï¼Œè¿™å°±è¦åœ¨ Cookie å’Œ Session ä¸Šåšæ–‡ç« äº†ï¼Œ Cookie ä¸­å¯ä»¥ä½¿ç”¨åŠ å¯†çš„æ–¹å¼å°†ç”¨æˆ·åè®°å½•ä¸‹æ¥ï¼Œåœ¨ä¸‹æ¬¡å°† Cookies è¯»å–å‡ºæ¥ç”±è¯·æ±‚å‘é€åˆ°æœåŠ¡ç«¯åï¼ŒæœåŠ¡ç«¯æ‚„æ‚„çš„è‡ªå·±åˆ›å»ºä¸€ä¸ªç”¨æˆ·å·²ç»ç™»å½•çš„ä¼šè¯ï¼Œè¿™æ ·æˆ‘ä»¬åœ¨å®¢æˆ·ç«¯çœ‹èµ·æ¥å°±å¥½åƒè¿™ä¸ªç™»å½•ä¼šè¯æ˜¯ä¸€ç›´ä¿æŒçš„ã€‚ é‡è¦æ¦‚å¿µ å½“æˆ‘ä»¬å…³é—­æµè§ˆå™¨çš„æ—¶å€™ä¼šè‡ªåŠ¨é”€æ¯æœåŠ¡ç«¯çš„ä¼šè¯ï¼Œè¿™ä¸ªæ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºåœ¨å…³é—­æµè§ˆå™¨çš„æ—¶å€™ï¼Œæµè§ˆå™¨å¹¶ä¸ä¼šé¢å¤–çš„é€šçŸ¥æœåŠ¡ç«¯è¯´ï¼Œæˆ‘è¦å…³é—­äº†ï¼Œä½ æŠŠå’Œæˆ‘çš„ä¼šè¯é”€æ¯æ‰å§ã€‚ å› ä¸ºæœåŠ¡ç«¯çš„ä¼šè¯æ˜¯ä¿å­˜åœ¨å†…å­˜ä¸­çš„ï¼Œè™½ç„¶ä¸€ä¸ªä¼šè¯ä¸ä¼šå¾ˆå¤§ï¼Œä½†æ˜¯æ¶ä¸ä½ä¼šè¯å¤šå•Šï¼Œç¡¬ä»¶æ¯•ç«Ÿæ˜¯ä¼šæœ‰é™åˆ¶çš„ï¼Œä¸èƒ½æ— é™æ‰©å……ä¸‹å»çš„ï¼Œæ‰€ä»¥åœ¨æœåŠ¡ç«¯è®¾ç½®ä¼šè¯çš„è¿‡æœŸæ—¶é—´å°±éå¸¸æœ‰å¿…è¦ã€‚ å½“ç„¶ï¼Œæœ‰æ²¡æœ‰æ–¹å¼èƒ½è®©æµè§ˆå™¨åœ¨å…³é—­çš„æ—¶å€™åŒæ­¥çš„å…³é—­æœåŠ¡ç«¯çš„ä¼šè¯ï¼Œå½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è„šæœ¬è¯­è¨€ JS æ¥ç›‘å¬æµè§ˆå™¨å…³é—­çš„åŠ¨ä½œï¼Œå½“æµè§ˆå™¨è§¦å‘å…³é—­åŠ¨ä½œçš„æ—¶å€™ï¼Œç”± JS åƒæœåŠ¡ç«¯å‘èµ·ä¸€ä¸ªè¯·æ±‚æ¥é€šçŸ¥æœåŠ¡ç«¯é”€æ¯ä¼šè¯ã€‚ ç”±äºä¸åŒçš„æµè§ˆå™¨å¯¹ JS äº‹ä»¶çš„å®ç°æœºåˆ¶ä¸ä¸€è‡´ï¼Œä¸ä¸€å®šä¿è¯ JS èƒ½ç›‘å¬åˆ°æµè§ˆå™¨å…³é—­çš„åŠ¨ä½œï¼Œæ‰€ä»¥ç°åœ¨å¸¸ç”¨çš„æ–¹å¼è¿˜æ˜¯åœ¨æœåŠ¡ç«¯è‡ªå·±è®¾ç½®ä¼šè¯çš„è¿‡æœŸæ—¶é—´ æ¨¡æ‹Ÿç™»é™†163 import time from selenium import webdriver from selenium.webdriver.common.by import By \"\"\" ä½¿ç”¨seleniumè¿›è¡Œæ¨¡æ‹Ÿç™»é™† 1.åˆå§‹åŒ–ChromDriver 2.æ‰“å¼€163ç™»é™†é¡µé¢ 3.æ‰¾åˆ°ç”¨æˆ·åçš„è¾“å…¥æ¡†ï¼Œè¾“å…¥ç”¨æˆ·å 4.æ‰¾åˆ°å¯†ç æ¡†ï¼Œè¾“å…¥å¯†ç  5.æäº¤ç”¨æˆ·ä¿¡æ¯ \"\"\" name = '*' passwd = '*' driver = webdriver.Chrome('./chromedriver') driver.get('https://mail.163.com/') # å°†çª—å£è°ƒæ•´æœ€å¤§ driver.maximize_window() # ä¼‘æ¯5s time.sleep(5) current_window_1 = driver.current_window_handle print(current_window_1) button = driver.find_element_by_id('lbNormal') button.click() driver.switch_to.frame(driver.find_element_by_xpath(\"//iframe[starts-with(@id, 'x-URS-iframe')]\")) email = driver.find_element_by_name('email') #email = driver.find_element_by_xpath('//input[@name=\"email\"]') email.send_keys(name) password = driver.find_element_by_name('password') #password = driver.find_element_by_xpath(\"//input[@name='password']\") password.send_keys(passwd) submit = driver.find_element_by_id(\"dologin\") time.sleep(15) submit.click() time.sleep(10) print(driver.page_source) driver.quit() è¿™æ¬¡çš„å†…å®¹æœ‰ç‚¹å¤šï¼Œå¾ˆå¤šéƒ½æ˜¯ç‚¹äº†ä¸€ä¸‹ï¼Œå¹¶æ²¡æœ‰æ·±å…¥ä»‹ç»ï¼Œæƒ³è¦ç†Ÿç»ƒæŒæ¡ï¼Œè¿˜è¦å¤šåŠ ç»ƒä¹ ï¼Œå¤šå•ƒæ–‡æ¡£ã€‚ ä¸é¦™å›­è‡ªåŠ¨åŒ–ç™»é™† import requests from bs4 import BeautifulSoup from selenium import webdriver from selenium.webdriver.common.by import By from lxml import etree import time name='1876180****' passwd='dxy123456' driver=webdriver.Chrome(r'C:\\Users\\MZ\\Downloads\\chromedriver_win32\\chromedriver.exe') driver.get('http://www.dxy.cn/bbs/thread/626626#626626') driver.maximize_window() time.sleep(5) current_window_1=driver.current_window_handle print(current_window_1) #ç‚¹å‡»ç™»é™†æŒ‰é’® button=driver.find_element_by_xpath('//*[@id=\"J-eventBanner\"]/div/div/a[2]') button.click() #ç‚¹å‡»è´¦å·å¯†ç ç™»é™† button=driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[1]/a[2]') button.click() email=driver.find_element_by_id('username') email.send_keys(name) password=driver.find_element_by_name('password') password.send_keys(passwd) submit=driver.find_element_by_xpath('//*[@id=\"user\"]/div[1]/div[3]/button') time.sleep(3) submit.click() time.sleep(10) print(driver.page_source) è‡³æ­¤å³å¯è·å¾—ç™»é™†åçš„ç•Œé¢ä¿¡æ¯ï¼ŒæŠ½å–è¯„è®ºåº”è¯¥å’Œä¸Šæ¬¡æ‰€ç”¨æ–¹æ³•ä¸€æ ·ï¼Œæœ‰æ—¶é—´å†åšè¡¥å……ã€‚","categories":[{"name":"çˆ¬è™«","slug":"çˆ¬è™«","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"selenium","slug":"selenium","permalink":"https://mz2sj.github.io/tags/selenium/"},{"name":"cookie","slug":"cookie","permalink":"https://mz2sj.github.io/tags/cookie/"},{"name":"session","slug":"session","permalink":"https://mz2sj.github.io/tags/session/"},{"name":"ä»£ç†","slug":"ä»£ç†","permalink":"https://mz2sj.github.io/tags/%E4%BB%A3%E7%90%86/"}]},{"title":"02-çˆ¬è™«","slug":"02-çˆ¬è™«","date":"2020-04-23T11:49:12.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/23/02-çˆ¬è™«/","link":"","permalink":"https://mz2sj.github.io/2020/04/23/02-%E7%88%AC%E8%99%AB/","excerpt":"","text":"æ­£åˆ™è¡¨è¾¾å¼ æ­£åˆ™è¡¨è¾¾å¼è¯­æ³• . è¡¨ç¤ºä»»ä½•å•ä¸ªå­—ç¬¦ [ ] å­—ç¬¦é›†ï¼Œå¯¹å•ä¸ªå­—ç¬¦ç»™å‡ºå–å€¼èŒƒå›´ ï¼Œå¦‚[abc]è¡¨ç¤ºaã€bã€cï¼Œ[aâ€z]è¡¨ç¤ºaåˆ°zå•ä¸ªå­—ç¬¦ [^ ] éå­—ç¬¦é›†ï¼Œå¯¹å•ä¸ªå­—ç¬¦ç»™å‡ºæ’é™¤èŒƒå›´ ï¼Œå¦‚[^abc]è¡¨ç¤ºéaæˆ–bæˆ–cçš„å•ä¸ªå­—ç¬¦ * å‰ä¸€ä¸ªå­—ç¬¦0æ¬¡æˆ–æ— é™æ¬¡æ‰©å±•ï¼Œå¦‚abc* è¡¨ç¤º abã€abcã€abccã€abcccç­‰ + å‰ä¸€ä¸ªå­—ç¬¦1æ¬¡æˆ–æ— é™æ¬¡æ‰©å±• ï¼Œå¦‚abc+ è¡¨ç¤º abcã€abccã€abcccç­‰ ? å‰ä¸€ä¸ªå­—ç¬¦0æ¬¡æˆ–1æ¬¡æ‰©å±• ï¼Œå¦‚abc? è¡¨ç¤º abã€abc | å·¦å³è¡¨è¾¾å¼ä»»æ„ä¸€ä¸ª ï¼Œå¦‚abc|def è¡¨ç¤º abcã€def {m} æ‰©å±•å‰ä¸€ä¸ªå­—ç¬¦mæ¬¡ ï¼Œå¦‚ab{2}cè¡¨ç¤ºabbc {m,n} æ‰©å±•å‰ä¸€ä¸ªå­—ç¬¦mè‡³næ¬¡ï¼ˆå«nï¼‰ ï¼Œå¦‚ab{1,2}cè¡¨ç¤ºabcã€abbc ^ åŒ¹é…å­—ç¬¦ä¸²å¼€å¤´ ï¼Œå¦‚^abcè¡¨ç¤ºabcä¸”åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²çš„å¼€å¤´ $ åŒ¹é…å­—ç¬¦ä¸²ç»“å°¾ ï¼Œå¦‚abc$è¡¨ç¤ºabcä¸”åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²çš„ç»“å°¾ ( ) åˆ†ç»„æ ‡è®°ï¼Œå†…éƒ¨åªèƒ½ä½¿ç”¨ | æ“ä½œç¬¦ ï¼Œå¦‚(abc)è¡¨ç¤ºabcï¼Œ(abc|def)è¡¨ç¤ºabcã€defï¼Œä½¿ç”¨åˆ†ç»„é™¤äº†æ•´ä½“åŒ¹é…çš„ç¬¦å·å¤–ï¼Œå•ç‹¬çš„åˆ†ç»„ä¹Ÿä¼šåŒ¹é…å‡ºæ¥ã€‚ \\d æ•°å­—ï¼Œç­‰ä»·äº[0â€9] \\w å•è¯å­—ç¬¦ï¼Œç­‰ä»·äº[Aâ€Zaâ€z0â€9_] reåº“ re.search() åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­æœç´¢åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œè¿”å›matchå¯¹è±¡ re.search(pattern, string, flags=0) re.match() ä»ä¸€ä¸ªå­—ç¬¦ä¸²çš„å¼€å§‹ä½ç½®èµ·åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼ï¼Œè¿”å›matchå¯¹è±¡ re.match(pattern, string, flags=0) re.findall() æœç´¢å­—ç¬¦ä¸²ï¼Œä»¥åˆ—è¡¨ç±»å‹è¿”å›å…¨éƒ¨èƒ½åŒ¹é…çš„å­ä¸² re.findall(pattern, string, flags=0) re.split() å°†ä¸€ä¸ªå­—ç¬¦ä¸²æŒ‰ç…§æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç»“æœè¿›è¡Œåˆ†å‰²ï¼Œè¿”å›åˆ—è¡¨ç±»å‹ re.split(pattern, string, maxsplit=0, flags=0) re.finditer() æœç´¢å­—ç¬¦ä¸²ï¼Œè¿”å›ä¸€ä¸ªåŒ¹é…ç»“æœçš„è¿­ä»£ç±»å‹ï¼Œæ¯ä¸ªè¿­ä»£å…ƒç´ æ˜¯matchå¯¹è±¡ re.finditer(pattern, string, flags=0) re.sub() åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­æ›¿æ¢æ‰€æœ‰åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼çš„å­ä¸²ï¼Œè¿”å›æ›¿æ¢åçš„å­—ç¬¦ä¸² re.sub(pattern, repl, string, count=0, flags=0) flags : æ­£åˆ™è¡¨è¾¾å¼ä½¿ç”¨æ—¶çš„æ§åˆ¶æ ‡è®°ï¼š re.I --&gt; re.IGNORECASE : å¿½ç•¥æ­£åˆ™è¡¨è¾¾å¼çš„å¤§å°å†™ï¼Œ[Aâ€Z]èƒ½å¤ŸåŒ¹é…å°å†™å­—ç¬¦ re.M --&gt; re.MULTILINE : æ­£åˆ™è¡¨è¾¾å¼ä¸­çš„^æ“ä½œç¬¦èƒ½å¤Ÿå°†ç»™å®šå­—ç¬¦ä¸²çš„æ¯è¡Œå½“ä½œåŒ¹é…å¼€å§‹ re.S --&gt; re.DOTALL : æ­£åˆ™è¡¨è¾¾å¼ä¸­çš„.æ“ä½œç¬¦èƒ½å¤ŸåŒ¹é…æ‰€æœ‰å­—ç¬¦ï¼Œé»˜è®¤åŒ¹é…é™¤æ¢è¡Œå¤–çš„æ‰€æœ‰å­—ç¬¦ æ·˜å®å•†å“çˆ¬è™« import requests import re def getHTMLText(url): try: kv={'cookie':'thw=cn; cna=+xjxFvvjZFUCAbegU4BJw+f0; hng=CN%7Czh-CN%7CCNY%7C156; _samesite_flag_=true; cookie2=18ea7aba395e98b25e202aca290dfc51; t=063bd64499346909d85799069ecdfb0a; _tb_token_=e7b0d70737676; sgcookie=EQbeuix2KIOgXG2FFLZOJ; unb=2657451481; uc3=vt3=F8dBxGR2VD%2BIzyEg%2Beo%3D&amp;lg2=UtASsssmOIJ0bQ%3D%3D&amp;id2=UU6kVNSBaZ7V0Q%3D%3D&amp;nk2=oHW%2BtqmuGFM%3D; csg=4349f769; lgc=%5Cu5B5F%5Cu95471023; cookie17=UU6kVNSBaZ7V0Q%3D%3D; dnk=%5Cu5B5F%5Cu95471023; skt=490ef5758fe425d4; existShop=MTU4NzU0MjE2Mg%3D%3D; uc4=nk4=0%40oicDEdY%2Fq4hj7fRzY6bY5rGJXw%3D%3D&amp;id4=0%40U2xpViK2NwPK%2B6Z6pycbkB5Z8GcI; tracknick=%5Cu5B5F%5Cu95471023; _cc_=VFC%2FuZ9ajQ%3D%3D; _l_g_=Ug%3D%3D; sg=313; _nk_=%5Cu5B5F%5Cu95471023; cookie1=WqUIbiciTOhdw8TeNfR1ltACceU5jKLQIv2L5E78Zzo%3D; enc=5kK7r49vDFT6xXh5WeuK224BSMneKyQf4H%2F7R%2FSyjHeMJIz8BwUqOX967CJBECgM1E03U6Y1ORAfflhD%2FmV1Mg%3D%3D; JSESSIONID=F55DD890B8CDAD1F8644BE9DABD3A503; tfstk=cEb1BFvrHAD1s1bZ8tNebz9sepTVa6yBtl9O1MBYuf-uj7C6psAjzL_IQiDnJ3dC.; uc1=cookie16=VT5L2FSpNgq6fDudInPRgavC%2BQ%3D%3D&amp;cookie21=Vq8l%2BKCLjhS4UhJVbhgU&amp;cookie15=V32FPkk%2Fw0dUvg%3D%3D&amp;existShop=false&amp;pas=0&amp;cookie14=UoTUPcllKBuWuA%3D%3D; mt=ci=75_1; v=0; isg=BPDwL_0CWmxemQaV6E8w83tQwb5COdSDUALMBOpBvMsepZBPkkmkE0aX-a3FLoxb; l=eBxryhwuQJlCKyvkBOfaFurza77OSIRYYuPzaNbMiT5POS5B5QzGWZX3qAT6C3GVh6ByR3JMwUXJBeYBqQAonxv92j-la_kmn', 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'} r=requests.get(url,timeout=30,headers=kv) r.raise_for_status() r.encoding=r.apparent_encoding return r.text except: return 'çˆ¬å–å¤±è´¥' def parsePage(glist,html): try: price_list=re.findall('\\\"view_price\\\":\\\"\\d+\\.\\d+\\\"',html) name_list=re.findall(r'\\\"raw_title\\\":\\\"[a-zA-Z0-9_\\w\\s\\\\\\u3010\\u3011\\-*]+\\\"',html) for i in range(len(price_list)): price=eval(price_list[i].split(\":\")[1]) name=eval(name_list[i].split(\":\")[1]) # print(price,name) glist.append([price,name]) except: print('è§£æå¤±è´¥') def printGoodList(glist): tplt=\"{0:^4}\\t{1:6}\\t{2:^10}\" print(tplt.format('åºå·',\"å•†å“ä»·æ ¼\",'å•†å“åç§°')) count=0 for g in glist: count+=1 print(tplt.format(count,g[0],g[1])) goods_name='ä¹¦åŒ…' start_url='https://s.taobao.com/search?q='+goods_name info_list=[] page=3 count=0 for i in range(page): count+=1 try: url=start_url+\"&amp;s\"+str(44*i) html=getHTMLText(url) parsePage(info_list,html) print(\"\\rçˆ¬å–é¡µé¢å½“å‰è¿›åº¦:{:.2f}%\".format(count*100/page),end=\"\") except: continue æ­£åˆ™è¡¨è¾¾å¼çš„é—®é¢˜ä¸»è¦å‡ºç°åœ¨åŒ¹é…å•†å“åç§°ä¸Šé¢ï¼Œè‡ªå·±çš„è¡¨è¾¾å¼ä¸º\\&quot;raw_title\\&quot;:\\&quot;[a-zA-Z0-9_\\w\\s\\\\\\u3010\\u3011\\-*]+\\&quot;,ä¸»è¦å‡ºç°çš„é—®é¢˜æ˜¯å•†å“åç§°ä¼šå‡ºç°è®¸å¤šç‰¹æ®Šç¬¦å·ï¼Œæ¯”å¦‚ã€-ï¼Œç­‰ç­‰ï¼Œè‡ªå·±æƒ³ç”¨.*æ¥åŒ¹é…ï¼Œä½†æ˜¯.ä¼šåŒ¹é…æ‰€æœ‰å­—ç¬¦ï¼Œäº§ç”Ÿè¶Šç•Œé—®é¢˜ã€‚å‚è€ƒç­”æ¡ˆç»™çš„è§£å†³æ–¹æ¡ˆæ˜¯\\&quot;raw_title\\&quot;\\:\\&quot;.*?\\&quot;,åœ¨.*åé¢åŠ ä¸Šé—®å·å°±å¯ä»¥è§£å†³è¶Šç•Œé—®é¢˜ã€‚ BeautifulSoup BeautifulSoupçš„åŸºæœ¬å…ƒç´  Beautiful Soupåº“çš„ç†è§£ï¼š Beautiful Soupåº“æ˜¯è§£æã€éå†ã€ç»´æŠ¤â€œæ ‡ç­¾æ ‘â€çš„åŠŸèƒ½åº“ï¼Œå¯¹åº”ä¸€ä¸ªHTML/XMLæ–‡æ¡£çš„å…¨éƒ¨å†…å®¹ BeautifulSoupç±»çš„åŸºæœ¬å…ƒç´ : Tag æ ‡ç­¾ï¼Œæœ€åŸºæœ¬çš„ä¿¡æ¯ç»„ç»‡å•å…ƒï¼Œåˆ†åˆ«ç”¨&lt;&gt;å’Œæ ‡æ˜å¼€å¤´å’Œç»“å°¾ï¼› Name æ ‡ç­¾çš„åå­—ï¼Œâ€¦çš„åå­—æ˜¯'p'ï¼Œæ ¼å¼ï¼š.name; Attributes æ ‡ç­¾çš„å±æ€§ï¼Œå­—å…¸å½¢å¼ç»„ç»‡ï¼Œæ ¼å¼ï¼š.attrs; NavigableString æ ‡ç­¾å†…éå±æ€§å­—ç¬¦ä¸²ï¼Œ&lt;&gt;â€¦ä¸­å­—ç¬¦ä¸²ï¼Œæ ¼å¼ï¼š.string; Comment æ ‡ç­¾å†…å­—ç¬¦ä¸²çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œä¸€ç§ç‰¹æ®Šçš„Commentç±»å‹; # å¯¼å…¥bs4åº“ from bs4 import BeautifulSoup import requests # æŠ“å–é¡µé¢ r = requests.get('https://python123.io/ws/demo.html') # Demoç½‘å€ demo = r.text # æŠ“å–çš„æ•°æ® demo # è§£æHTMLé¡µé¢ soup = BeautifulSoup(demo, 'html.parser') # æŠ“å–çš„é¡µé¢æ•°æ®ï¼›bs4çš„è§£æå™¨ # æœ‰å±‚æ¬¡æ„Ÿçš„è¾“å‡ºè§£æåçš„HTMLé¡µé¢ print(soup.prettify()) æ ‡ç­¾ ç›´æ¥ä½¿ç”¨tagåè·å¾— soup.a soup.title è¿”å›çš„stringå¯¹è±¡å¯ä»¥ç»§ç»­è°ƒç”¨BeautifulSoupçš„å±‚çº§æ ‡ç­¾åŠŸèƒ½ æ ‡ç­¾çš„åå­— soup.a.name soup.a.parent.name soup.p.parent.name å¯ä»¥ç”¨æ¥æŸ¥çœ‹å±‚çº§æ ‡ç­¾çš„åå­—ï¼Œå½“å±‚çº§æ ‡ç­¾æ¯”è¾ƒå¤æ‚æ—¶å¯ä»¥ä½¿ç”¨ã€‚ æ ‡ç­¾çš„å±æ€§ tag = soup.a print(tag.attrs) print(tag.attrs['class']) print(type(tag.attrs)) æ³¨æ„ï¼Œè¿”å›çš„æ˜¯å…³äºæ ‡ç­¾çš„ä¸€ä¸ªåˆ—è¡¨ string print(soup.a.string) print(type(soup.a.string)) è¿”å›æ ‡ç­¾å†…çš„éå±æ€§å­—ç¬¦ä¸²ã€‚ prettify() print(soup.prettify()) è¿”å›å±‚æ¬¡åŒ–çš„æ ‡ç­¾ç»“æ„ åŸºäºbs4åº“çš„HTMLå†…å®¹éå†æ–¹æ³• HTMLåŸºæœ¬æ ¼å¼:&lt;&gt;â€¦æ„æˆäº†æ‰€å±å…³ç³»ï¼Œå½¢æˆäº†æ ‡ç­¾çš„æ ‘å½¢ç»“æ„ æ ‡ç­¾æ ‘çš„ä¸‹è¡Œéå† .contents å­èŠ‚ç‚¹çš„åˆ—è¡¨ï¼Œå°†``æ‰€æœ‰å„¿å­èŠ‚ç‚¹å­˜å…¥åˆ—è¡¨ .children å­èŠ‚ç‚¹çš„è¿­ä»£ç±»å‹ï¼Œä¸.contentsç±»ä¼¼ï¼Œç”¨äºå¾ªç¯éå†å„¿å­èŠ‚ç‚¹ .descendants å­å­™èŠ‚ç‚¹çš„è¿­ä»£ç±»å‹ï¼ŒåŒ…å«æ‰€æœ‰å­å­™èŠ‚ç‚¹ï¼Œç”¨äºå¾ªç¯éå† æ ‡ç­¾æ ‘çš„ä¸Šè¡Œé .parent èŠ‚ç‚¹çš„çˆ¶äº²æ ‡ç­¾ .parents èŠ‚ç‚¹å…ˆè¾ˆæ ‡ç­¾çš„è¿­ä»£ç±»å‹ï¼Œç”¨äºå¾ªç¯éå†å…ˆè¾ˆèŠ‚ç‚¹ æ ‡ç­¾æ ‘çš„å¹³è¡Œéå† .next_sibling è¿”å›æŒ‰ç…§HTMLæ–‡æœ¬é¡ºåºçš„ä¸‹ä¸€ä¸ªå¹³è¡ŒèŠ‚ç‚¹æ ‡ç­¾ .previous_sibling è¿”å›æŒ‰ç…§HTMLæ–‡æœ¬é¡ºåºçš„ä¸Šä¸€ä¸ªå¹³è¡ŒèŠ‚ç‚¹æ ‡ç­¾ .next_siblings è¿­ä»£ç±»å‹ï¼Œè¿”å›æŒ‰ç…§HTMLæ–‡æœ¬é¡ºåºçš„åç»­æ‰€æœ‰å¹³è¡ŒèŠ‚ç‚¹æ ‡ç­¾ .previous_siblings è¿­ä»£ç±»å‹ï¼Œè¿”å›æŒ‰ç…§HTMLæ–‡æœ¬é¡ºåºçš„å‰ç»­æ‰€æœ‰å¹³è¡ŒèŠ‚ç‚¹æ ‡ç­¾ æ ‡ç­¾æ ‘çš„ä¸‹è¡Œéå† import requests from bs4 import BeautifulSoup r=requests.get('http://python123.io/ws/demo.html') demo=r.text soup=BeautifulSoup(demo,'html.parser') print(soup.contents)# è·å–æ•´ä¸ªæ ‡ç­¾æ ‘çš„å„¿å­èŠ‚ç‚¹ print(soup.body.contents)#è¿”å›æ ‡ç­¾æ ‘çš„bodyæ ‡ç­¾ä¸‹çš„èŠ‚ç‚¹ print(soup.head)#è¿”å›headæ ‡ç­¾ for child in soup.body.children:#éå†å„¿å­èŠ‚ç‚¹ï¼Œä¸ä¼šéå†å„¿å­èŠ‚ç‚¹ä¸‹çš„èŠ‚ç‚¹ print(child) for child in soup.body.descendants:#éå†å­å­™èŠ‚ç‚¹ï¼Œéå†çˆ¶èŠ‚ç‚¹ä¸‹é¢æ‰€æœ‰èŠ‚ç‚¹ print(child) æ ‡ç­¾æ ‘çš„ä¸Šè¡Œéå† soup.title.parent soup.title.parent for parent in soup.a.parents: # éå†å…ˆè¾ˆçš„ä¿¡æ¯ if parent is None: print(parent) else: print(parent.name) æ ‡ç­¾æ ‘çš„å¹³è¡Œéå† æ³¨æ„ï¼š æ ‡ç­¾æ ‘çš„å¹³è¡Œéå†æ˜¯æœ‰æ¡ä»¶çš„ å¹³è¡Œéå†å‘ç”Ÿåœ¨åŒä¸€ä¸ªçˆ¶äº²èŠ‚ç‚¹çš„å„èŠ‚ç‚¹ä¹‹é—´ æ ‡ç­¾ä¸­çš„å†…å®¹ä¹Ÿæ„æˆäº†èŠ‚ç‚¹,è¿ç»­çš„å­—ç¬¦ä¸²ä¹Ÿæ˜¯èŠ‚ç‚¹ print(soup.a.next_sibling)#aæ ‡ç­¾çš„ä¸‹ä¸€ä¸ªæ ‡ç­¾ print(soup.a.next_sibling.next_sibling)#aæ ‡ç­¾çš„ä¸‹ä¸€ä¸ªæ ‡ç­¾çš„ä¸‹ä¸€ä¸ªæ ‡ç­¾ print(soup.a.previous_sibling)#aæ ‡ç­¾çš„å‰ä¸€ä¸ªæ ‡ç­¾ print(soup.a.previous_sibling.previous_sibling)#aæ ‡ç­¾çš„å‰ä¸€ä¸ªæ ‡ç­¾çš„å‰ä¸€ä¸ªæ ‡ç­¾ for sibling in soup.a.next_siblings:#éå†åç»­èŠ‚ç‚¹ print(sibling) for sibling in soup.a.previous_siblings:#éå†ä¹‹å‰çš„èŠ‚ç‚¹ print(sibling) åŸºäºbs4åº“çš„htmlæŸ¥æ‰¾æ–¹æ³• &lt;&gt;.find_all(name, attrs, recursive, string, **kwargs) å‚æ•°ï¼š âˆ™ name : å¯¹æ ‡ç­¾åç§°çš„æ£€ç´¢å­—ç¬¦ä¸² âˆ™ attrs: å¯¹æ ‡ç­¾å±æ€§å€¼çš„æ£€ç´¢å­—ç¬¦ä¸²ï¼Œå¯æ ‡æ³¨å±æ€§æ£€ç´¢ âˆ™ recursive: æ˜¯å¦å¯¹å­å­™å…¨éƒ¨æ£€ç´¢ï¼Œé»˜è®¤True âˆ™ string: &lt;&gt;â€¦&lt;/&gt;ä¸­å­—ç¬¦ä¸²åŒºåŸŸçš„æ£€ç´¢å­—ç¬¦ä¸² ç®€å†™ï¼š (..) ç­‰ä»·äº.find_all(â€¦) soup(â€¦) ç­‰ä»·äº soup.find_all(â€¦) æ‰©å±•æ–¹æ³•ï¼š &lt;&gt;.find() æœç´¢ä¸”åªè¿”å›ä¸€ä¸ªç»“æœï¼ŒåŒ.find_all()å‚æ•° &lt;&gt;.find_parents() åœ¨å…ˆè¾ˆèŠ‚ç‚¹ä¸­æœç´¢ï¼Œè¿”å›åˆ—è¡¨ç±»å‹ï¼ŒåŒ.find_all()å‚æ•° &lt;&gt;.find_parent() åœ¨å…ˆè¾ˆèŠ‚ç‚¹ä¸­è¿”å›ä¸€ä¸ªç»“æœï¼ŒåŒ.find()å‚æ•° &lt;&gt;.find_next_siblings() åœ¨åç»­å¹³è¡ŒèŠ‚ç‚¹ä¸­æœç´¢ï¼Œè¿”å›åˆ—è¡¨ç±»å‹ï¼ŒåŒ.find_all()å‚æ•° &lt;&gt;.find_next_sibling() åœ¨åç»­å¹³è¡ŒèŠ‚ç‚¹ä¸­è¿”å›ä¸€ä¸ªç»“æœï¼ŒåŒ.find()å‚æ•° &lt;&gt;.find_previous_siblings() åœ¨å‰åºå¹³è¡ŒèŠ‚ç‚¹ä¸­æœç´¢ï¼Œè¿”å›åˆ—è¡¨ç±»å‹ï¼ŒåŒ.find_all()å‚æ•° &lt;&gt;.find_previous_sibling() åœ¨å‰åºå¹³è¡ŒèŠ‚ç‚¹ä¸­è¿”å›ä¸€ä¸ªç»“æœï¼ŒåŒ.find()å‚æ•° import requests from bs4 import BeautifulSoup r = requests.get('http://python123.io/ws/demo.html') demo = r.text soup = BeautifulSoup(demo,'html.parser') soup # name : å¯¹æ ‡ç­¾åç§°çš„æ£€ç´¢å­—ç¬¦ä¸² soup.find_all('a') soup.find_all(['a', 'p']) # attrs: å¯¹æ ‡ç­¾å±æ€§å€¼çš„æ£€ç´¢å­—ç¬¦ä¸²ï¼Œå¯æ ‡æ³¨å±æ€§æ£€ç´¢ soup.find_all(\"p\",\"course\") soup.find_all(id=\"link\") # å®Œå…¨åŒ¹é…æ‰èƒ½åŒ¹é…åˆ° # recursive: æ˜¯å¦å¯¹å­å­™å…¨éƒ¨æ£€ç´¢ï¼Œé»˜è®¤True soup.find_all('p',recursive=False) # string: &lt;&gt;â€¦&lt;/&gt;ä¸­å­—ç¬¦ä¸²åŒºåŸŸçš„æ£€ç´¢å­—ç¬¦ä¸² soup.find_all(string = \"Basic Python\") # å®Œå…¨åŒ¹é…æ‰èƒ½åŒ¹é…åˆ° å¥½å¤§å­¦æ’åçˆ¬è™«ç»™ä½  çˆ¬å–urlï¼šhttp://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html çˆ¬å–æ€è·¯ï¼š ä»ç½‘ç»œä¸Šè·å–å¤§å­¦æ’åç½‘é¡µå†…å®¹ æå–ç½‘é¡µå†…å®¹ä¸­ä¿¡æ¯åˆ°åˆé€‚çš„æ•°æ®ç»“æ„ï¼ˆäºŒç»´æ•°ç»„ï¼‰-æ’åï¼Œå­¦æ ¡åç§°ï¼Œæ€»åˆ† åˆ©ç”¨æ•°æ®ç»“æ„å±•ç¤ºå¹¶è¾“å‡ºç»“æœ # å¯¼å…¥åº“ import requests from bs4 import BeautifulSoup import bs4 è‡ªå·±å’Œå®˜æ–¹ä»£ç æ¯”èµ·æ¥å°±æ¯”è¾ƒç®€é™‹äº† r=requests.get('http://www.zuihaodaxue.cn/zuihaodaxuepaiming2019.html') r.raise_for_status() r.encoding=r.apparent_encoding #è‡ªåŠ¨è·å–ç¼–ç ï¼Œå¯ä»¥çœå¥½å¤šäº‹ demo=r.text demo soup=BeautifulSoup(demo,'html.parser') print('{0:^8}\\t{1:^18}\\t{2:^18}'.format(\"æ’å\",\"å­¦æ ¡åç§°\",\"æ€»åˆ†\")) for tr in trs: rank=tr.find_all('td')[0].string school_name=tr.find_all('td')[1].string score=tr.find_all('td')[4].string print('{0:^8}\\t{1:^18}\\t{2:^18}'.format(rank,school_name,score)) Xpath Xpath è¯­æ³• XPathå³ä¸ºXMLè·¯å¾„è¯­è¨€ï¼ˆXML Path Languageï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§ç”¨æ¥ç¡®å®šXMLæ–‡æ¡£ä¸­æŸéƒ¨åˆ†ä½ç½®çš„è¯­è¨€ã€‚ åœ¨XPathä¸­ï¼Œæœ‰ä¸ƒç§ç±»å‹çš„èŠ‚ç‚¹ï¼šå…ƒç´ ã€å±æ€§ã€æ–‡æœ¬ã€å‘½åç©ºé—´ã€å¤„ç†æŒ‡ä»¤ã€æ³¨é‡Šä»¥åŠæ–‡æ¡£ï¼ˆæ ¹ï¼‰èŠ‚ç‚¹ã€‚ XMLæ–‡æ¡£æ˜¯è¢«ä½œä¸ºèŠ‚ç‚¹æ ‘æ¥å¯¹å¾…çš„ã€‚ XPathä½¿ç”¨è·¯å¾„è¡¨è¾¾å¼åœ¨XMLæ–‡æ¡£ä¸­é€‰å–èŠ‚ç‚¹ã€‚èŠ‚ç‚¹æ˜¯é€šè¿‡æ²¿ç€è·¯å¾„é€‰å–çš„ã€‚ä¸‹é¢åˆ—å‡ºäº†æœ€å¸¸ç”¨çš„è·¯å¾„è¡¨è¾¾å¼ï¼š nodename é€‰å–æ­¤èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ã€‚ / ä»æ ¹èŠ‚ç‚¹é€‰å–ã€‚ // ä»åŒ¹é…é€‰æ‹©çš„å½“å‰èŠ‚ç‚¹é€‰æ‹©æ–‡æ¡£ä¸­çš„èŠ‚ç‚¹ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬çš„ä½ç½®ã€‚ . é€‰å–å½“å‰èŠ‚ç‚¹ã€‚ â€¦ é€‰å–å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ã€‚ @ é€‰å–å±æ€§ã€‚ /text() æå–æ ‡ç­¾ä¸‹é¢çš„æ–‡æœ¬å†…å®¹ å¦‚ï¼š /æ ‡ç­¾å é€å±‚æå– /æ ‡ç­¾å æå–æ‰€æœ‰åä¸º&lt;&gt;çš„æ ‡ç­¾ //æ ‡ç­¾å[@å±æ€§=â€œå±æ€§å€¼â€] æå–åŒ…å«å±æ€§ä¸ºå±æ€§å€¼çš„æ ‡ç­¾ @å±æ€§å ä»£è¡¨å–æŸä¸ªå±æ€§åçš„å±æ€§å€¼ è¯¦ç»†å­¦ä¹ ï¼šhttps://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html ä½¿ç”¨lxmlè§£æ å¯¼å…¥åº“ï¼šfrom lxml import etree lxmlå°†htmlæ–‡æœ¬è½¬æˆxmlå¯¹è±¡ tree = etree.HTML(html) ç”¨æˆ·åç§°ï¼štree.xpath(â€™//div[@class=â€œauthâ€]/a/text()â€™) å›å¤å†…å®¹ï¼štree.xpath(â€™//td[@class=â€œpostbodyâ€]â€™) å› ä¸ºå›å¤å†…å®¹ä¸­æœ‰æ¢è¡Œç­‰æ ‡ç­¾ï¼Œæ‰€ä»¥éœ€è¦ç”¨string()æ¥è·å–æ•°æ®ã€‚ string()çš„è¯¦ç»†è§é“¾æ¥ï¼šhttps://www.cnblogs.com/CYHISTW/p/12312570.html Xpathä¸­text()ï¼Œstring()ï¼Œdata()çš„åŒºåˆ«å¦‚ä¸‹ï¼š text()ä»…ä»…è¿”å›æ‰€æŒ‡å…ƒç´ çš„æ–‡æœ¬å†…å®¹ã€‚ string()å‡½æ•°ä¼šå¾—åˆ°æ‰€æŒ‡å…ƒç´ çš„æ‰€æœ‰èŠ‚ç‚¹æ–‡æœ¬å†…å®¹ï¼Œè¿™äº›æ–‡æœ¬è®²ä¼šè¢«æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚ data()å¤§å¤šæ•°æ—¶å€™ï¼Œdata()å‡½æ•°å’Œstring()å‡½æ•°é€šç”¨ï¼Œè€Œä¸”ä¸å»ºè®®ç»å¸¸ä½¿ç”¨data()å‡½æ•°ï¼Œæœ‰æ•°æ®è¡¨æ˜ï¼Œè¯¥å‡½æ•°ä¼šå½±å“XPathçš„æ€§èƒ½ã€‚ ä¸é¦™å›­è¯„è®ºçˆ¬è™« chromeçš„å¼€å‘å·¥å…·é‡Œé¢æ˜¯æœ‰copy Xpathè¿™ä¸ªé€‰é¡¹çš„ # å¯¼å…¥åº“ from lxml import etree import requests url = \"http://www.dxy.cn/bbs/thread/626626#626626\" req = requests.get(url) html = req.text tree = etree.HTML(html) tree user = tree.xpath('//div[@class=\"auth\"]/a/text()') # print(user) content = tree.xpath('//td[@class=\"postbody\"]') results = [] for i in range(0, len(user)): # print(user[i].strip()+\":\"+content[i].xpath('string(.)').strip()) # print(\"*\"*80) # å› ä¸ºå›å¤å†…å®¹ä¸­æœ‰æ¢è¡Œç­‰æ ‡ç­¾ï¼Œæ‰€ä»¥éœ€è¦ç”¨string()æ¥è·å–æ•°æ® results.append(user[i].strip() + \": \" + content[i].xpath('string(.)').strip()) # æ‰“å°çˆ¬å–çš„ç»“æœ for i,result in zip(range(0, len(user)),results): print(\"user\"+ str(i+1) + \"-\" + result) print(\"*\"*100) æ¯”è¾ƒè®©è‡ªå·±å›°æƒ‘çš„æ˜¯text()å’Œstring(.)çš„ç”¨æ³•ï¼Œåšå®¢ä¸Šä»‹ç»è¯´text()è¿”å›çš„æ˜¯åŒ¹é…åˆ°çš„æ‰€æœ‰å…ƒç´ åŒ…å«æ–‡å­—çš„åˆ—è¡¨ï¼Œè€Œstring(.)è¿”å›çš„æ˜¯åˆå¹¶åçš„ç»“æœã€‚è€Œä¸”string(.)çš„ç”¨æ³•è¦åœ¨è·å–ä¸Šçº§å…ƒç´ åï¼Œå•ç‹¬å¯¹æ¯ä¸ªå…ƒç´ è°ƒç”¨.xpath(â€˜string(.)â€™)ã€‚åæ¥æˆ‘æ‰å‘ç°ï¼Œå¦‚æœæ–‡å­—ä¸­å†…åµŒæœ‰å…¶ä»–æ ‡ç­¾ï¼Œæ¯”å¦‚brç­‰ï¼Œå°±ä¼šå°†æ–‡å­—åˆ†å‰²æˆå•ç‹¬çš„å„ä¸ªåˆ—è¡¨å…ƒç´ ï¼Œå’Œä¸Šä¸€å±‚åŠçš„å…ƒç´ å¹¶åˆ—äº§ç”Ÿé—®é¢˜ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¯¹åŒä¸€å±‚çº§çš„å…ƒç´ ä½¿ç”¨string(.),è¿™æ ·ä¸ç®¡åŒçº§å…ƒç´ æ˜¯å¦æœ‰ä¸‹çº§å…ƒç´ ï¼Œéƒ½ä¼šç»Ÿä¸€æˆä¸€ä¸ªå…ƒç´ ã€‚ æ”¶å·¥~~~","categories":[{"name":"çˆ¬è™«","slug":"çˆ¬è™«","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"æ­£åˆ™è¡¨è¾¾å¼","slug":"æ­£åˆ™è¡¨è¾¾å¼","permalink":"https://mz2sj.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"bs4","slug":"bs4","permalink":"https://mz2sj.github.io/tags/bs4/"},{"name":"Xpath","slug":"Xpath","permalink":"https://mz2sj.github.io/tags/Xpath/"}]},{"title":"01-çˆ¬è™«å…¥é—¨","slug":"01-çˆ¬è™«","date":"2020-04-21T10:10:41.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/21/01-çˆ¬è™«/","link":"","permalink":"https://mz2sj.github.io/2020/04/21/01-%E7%88%AC%E8%99%AB/","excerpt":"","text":"HTTPè¯·æ±‚ å¯¹äºhttpè¯·æ±‚ï¼Œè®°å½•ä¸€ä¸‹å¸¸ç”¨çš„å‡ ç§ï¼š GETï¼šå‘æŒ‡å®šçš„èµ„æºå‘å‡ºâ€œæ˜¾ç¤ºâ€è¯·æ±‚ã€‚GETæ–¹æ³•åº”è¯¥åªç”¨äºè¯»å–æ•°æ® ï¼ŒæŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆåç§°/å€¼å¯¹ï¼‰æ˜¯åœ¨ GET è¯·æ±‚çš„ URL ä¸­å‘é€çš„ ã€‚ POSTï¼šå‘æŒ‡å®šèµ„æºæäº¤æ•°æ®ï¼Œè¯·æ±‚æœåŠ¡å™¨è¿›è¡Œå¤„ç†ï¼ˆä¾‹å¦‚æäº¤è¡¨å•æˆ–è€…ä¸Šä¼ æ–‡ä»¶ï¼‰ã€‚æ•°æ®è¢«åŒ…å«åœ¨è¯·æ±‚æ–‡æœ¬ä¸­ã€‚è¿™ä¸ªè¯·æ±‚å¯èƒ½ä¼šåˆ›å»ºæ–°çš„èµ„æºæˆ–ä¿®æ”¹ç°æœ‰èµ„æºï¼Œæˆ–äºŒè€…çš†æœ‰ã€‚ æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆåç§°/å€¼å¯¹ï¼‰æ˜¯åœ¨ POST è¯·æ±‚çš„ HTTP æ¶ˆæ¯ä¸»ä½“ä¸­å‘é€çš„ PUTï¼šå‘æŒ‡å®šèµ„æºä½ç½®ä¸Šä¼ è¾“æœ€æ–°å†…å®¹ã€‚ DELETEï¼šè¯·æ±‚æœåŠ¡å™¨åˆ é™¤Request-URLæ‰€æ ‡è¯†çš„èµ„æºã€‚ requests.get å®ç°åŸºæœ¬çš„requestsåº”ç”¨ï¼Œçˆ¬å–pythonä¹‹ç¦… import requests url='https://www.python.org/dev/peps/pep-0020/' res=request.get(url) text=res.text æ­¤æ—¶è¿”å›çš„textæ˜¯htmlåŸç”Ÿæ–‡æœ¬ï¼Œæƒ³è¦è·å–å¯¹åº”å†…å®¹å°±éœ€è¦è¿›è¡ŒåŒ¹é…äº†ã€‚ with open('zon_of_python.txt','w') as f: f.write(text[text.find('&lt;pre')+28:text.find('&lt;/pre&gt;')-1]) print(text[text.find('&lt;pre')+28:text.find('&lt;/pre')-1]) Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! pythonè‡ªå¸¦çš„urllibä¹Ÿèƒ½å®ç°ä¸Šè¿°åŠŸèƒ½ import urllib url = 'https://www.python.org/dev/peps/pep-0020/' res=urllib.request.urlopen(url).read().decode('utf-8') print(res[res.find('&lt;pre')+28:res.find('&lt;/pre')-1]) urllibæ˜æ˜¾è¦ç¹çäº›ã€‚ request.post request.getåªæ˜¯ä»ç½‘ç«™ä¸Šçˆ¬å–å·²æœ‰æ•°æ®ï¼Œå½“æˆ‘ä»¬éœ€è¦å’Œç½‘ç«™è¿›è¡Œæ•°æ®äº¤äº’æ—¶ï¼Œå°±è¦ç”¨åˆ°postæ–¹æ³•ã€‚ import requests def translate(word): url=\"http://fy.iciba.com/ajax.php?a=fy\" data={ 'f':'auto', 't':'atuo', 'w':word, } headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'} response=requests.post(url,data=data,headers=headers) json_data=response.json() return json_data def run(word): result=tranlate(word)['content']['out'] print(result) return result def main(): with open('zon_of_python.txt') as f: zh=[run(word) for word in f] with open('zon_of_python_zh-CN.txt','w') as g: for i in zh: g.write(i+'\\n') if __name__=='__main__': main() dataå†…å®¹å’Œæ ¼å¼çš„ç¡®å®šéœ€è¦ä»chromeçš„æ£€æŸ¥å·¥å…·networkä¸­çš„Headersè·å–ã€‚ çˆ¬å–è±†ç“£ç”µå½±top250 é¦–å…ˆè¦æ‰¾åˆ°æˆ‘ä»¬è¦çˆ¬å–çš„ä¸œè¥¿åœ¨ç½‘é¡µçš„ä½ç½®ï¼Œå€ŸåŠ©æ£€æŸ¥å·¥å…·ï¼Œå¯ä»¥æ‰¾åˆ°å¯¹ç”¨ä½ç½®çš„ä»£ç ï¼š å€ŸåŠ©åŒ¹é…è§„åˆ™å°±å¯ä»¥æ‹¿åˆ°æƒ³è¦çš„ä¸œè¥¿ï¼ˆpsï¼šè¿˜ä¸ä¼šåŒ¹é…å‘€ï¼Œåº”è¯¥æ˜¯è¦æ ¹æ®htmlå…ƒç´ çš„å±‚çº§å…³ç³»æ¥åŒ¹é…ï¼‰ã€‚å€ŸåŠ©æ£€æŸ¥å·¥å…·ä¸­çš„Copy XpathåŠŸèƒ½ï¼Œå¯ä»¥æ‰¾åˆ°ç”µå½±åå’Œå›¾ç‰‡è¶…é“¾æ¥çš„ä½ç½®ã€‚ è·å–ç”µå½±åå’Œå›¾ç‰‡è¶…é“¾æ¥çš„Xpathå…¬å¼ä¸º //*[@id=\"content\"]/div/div[1]/ol/li[23]/div/div[1]/a/img/@alt //*[@id=\"content\"]/div/div[1]/ol/li[23]/div/div[1]/a/img/@src å…¶ä¸­23æ˜¯ç”µå½±å¯¹åº”çš„å½“å‰é¡µæ¬¡åºï¼ŒèŒƒå›´æ˜¯1-25ï¼Œåˆšå¼€å§‹ä»¥ä¸ºæ˜¯ç”µå½±çš„æ’åè¿˜å‡ºé”™äº†ã€‚ ç”µå½±åˆ†å¸ƒåœ¨å¥½å‡ ä¸ªé¡µé¢ï¼Œæ‰€ä»¥è¿˜è¦æ‰¾å‡ºç¿»é¡µçš„è§„å¾‹ï¼Œå€ŸåŠ©æ£€æŸ¥å·¥å…·ç‚¹å‡»é¡µç ï¼Œæ‰¾åˆ°å¯¹åº”ä½ç½®çš„ä»£ç ï¼š å¯ä»¥å‘ç°å…¶ä¸­urlå¯¹åº”çš„æ•°å­—è§„å¾‹ï¼Œ0 25 50â€¦ from lxml import etree import time dic={} headers={ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36' } def get_name_imgpath(text): html = etree.HTML(text) for i in range(1,26): name = html.xpath('//*[@id=\"content\"]/div/div[1]/ol/li[{}]/div/div[1]/a/img/@alt'.format(i)) img_path=html.xpath('//*[@id=\"content\"]/div/div[1]/ol/li[{}]/div/div[1]/a/img/@src'.format(i)) print(name[0],img_path[0]) dic.update({name[0]:img_path[0]}) print('crawl start') for page in range(0,226,25): text=requests.get('https://movie.douban.com/top250?start={}&amp;filter='.format(page),headers=headers).text get_name_imgpath(text) time.sleep(1) print('crawl finish') print(dic) ç¨‹åºè¿è¡ŒæˆåŠŸ æ”¶å·¥ï¼ï¼ï¼","categories":[{"name":"çˆ¬è™«","slug":"çˆ¬è™«","permalink":"https://mz2sj.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"çˆ¬è™«","slug":"çˆ¬è™«","permalink":"https://mz2sj.github.io/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"04-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹æ¨¡å‹èåˆ","slug":"04-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹æ¨¡å‹èåˆ","date":"2020-04-04T11:39:31.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/04/04/04-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹æ¨¡å‹èåˆ/","link":"","permalink":"https://mz2sj.github.io/2020/04/04/04-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/","excerpt":"","text":"è™½ç„¶è¿™æ˜¯æœ€åä¸€æ¬¡æ‰“å¡çš„å†…å®¹ï¼Œä½†æ˜¯è‡ªå·±ç¦»æŒæ¡è¿™äº›å†…å®¹è¿˜å·®å¾—å¾ˆè¿œï¼Œè¿˜è¦ç»§ç»­åŠªåŠ›å‘€ï¼æ¸©æ•…è€ŒçŸ¥æ–°ã€‚ ensembleçš„æ–¹æ³• ç®€å•åŠ æƒèåˆ å›å½’ï¼ˆåˆ†ç±»æ¦‚ç‡ï¼‰ï¼šç®—æœ¯å¹³å‡èåˆï¼ˆArithmetic meanï¼‰ï¼Œå‡ ä½•å¹³å‡èåˆï¼ˆGeometric meanï¼‰ï¼› åˆ†ç±»ï¼šæŠ•ç¥¨ï¼ˆVoting) ç»¼åˆï¼šæ’åºèåˆ(Rank averaging)ï¼Œlogèåˆ stacking/blending æ„å»ºå¤šå±‚æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨é¢„æµ‹ç»“æœå†æ‹Ÿåˆé¢„æµ‹ boosting/bagging åœ¨xgboostï¼ŒAdaboostï¼ŒGBDTä¸­ä½¿ç”¨ï¼Œå¤šæ ‘çš„æå‡æ–¹æ³• Stacking ç†è®ºä»‹ç» å…ˆè®­ç»ƒå‡ ä¸ªå­¦ä¹ å™¨ï¼Œåœ¨è¿™äº›å­¦ä¹ å™¨é¢„æµ‹æ•°æ®çš„åŸºç¡€ä¸Šå†è®­ç»ƒæ–°çš„å­¦ä¹ å™¨ã€‚ å°†ä¸ªä½“å­¦ä¹ å™¨ç»“åˆåœ¨ä¸€èµ·çš„æ—¶å€™ä½¿ç”¨çš„æ–¹æ³•å«åšç»“åˆç­–ç•¥ã€‚å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æŠ•ç¥¨æ³•æ¥é€‰æ‹©è¾“å‡ºæœ€å¤šçš„ç±»ã€‚å¯¹äºå›å½’é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆ†ç±»å™¨è¾“å‡ºçš„ç»“æœæ±‚å¹³å‡å€¼ã€‚ åœ¨stackingæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æŠŠä¸ªä½“å­¦ä¹ å™¨å«åšåˆçº§å­¦ä¹ å™¨ï¼Œç”¨äºç»“åˆçš„å­¦ä¹ å™¨å«åšæ¬¡çº§å­¦ä¹ å™¨æˆ–å…ƒå­¦ä¹ å™¨ï¼ˆmeta-learnerï¼‰ï¼Œæ¬¡çº§å­¦ä¹ å™¨ç”¨äºè®­ç»ƒçš„æ•°æ®å«åšæ¬¡çº§è®­ç»ƒé›†ã€‚æ¬¡çº§è®­ç»ƒé›†æ˜¯åœ¨è®­ç»ƒé›†ä¸Šç”¨åˆçº§å­¦ä¹ å™¨å¾—åˆ°çš„ã€‚ å¯¹äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸é‚£ä¹ˆä¸€è‡´çš„æƒ…å†µï¼Œç”¨åˆå§‹æ¨¡å‹è®­ç»ƒçš„æ ‡ç­¾å†åˆ©ç”¨çœŸå®æ ‡ç­¾è¿›è¡Œå†è®­ç»ƒï¼Œæ¯«æ— ç–‘é—®ä¼šå¯¼è‡´ä¸€å®šçš„æ¨¡å‹è¿‡æ‹Ÿåˆè®­ç»ƒé›†ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ³•ï¼šæ¬¡çº§æ¨¡å‹å°½é‡é€‰æ‹©ç®€å•çš„çº¿æ€§æ¨¡å‹ åˆ©ç”¨KæŠ˜äº¤å‰éªŒè¯ã€‚ ä»£ç ç¤ºä¾‹ ç®€å•åŠ æƒå¹³å‡ ## ç”Ÿæˆä¸€äº›ç®€å•çš„æ ·æœ¬æ•°æ®ï¼Œtest_prei ä»£è¡¨ç¬¬iä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼ test_pre1 = [1.2, 3.2, 2.1, 6.2] test_pre2 = [0.9, 3.1, 2.0, 5.9] test_pre3 = [1.1, 2.9, 2.2, 6.0] # y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼ y_test_true = [1, 3, 2, 6] import numpy as np import pandas as pd ## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•° def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]): Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3) return Weighted_result from sklearn import metrics # å„æ¨¡å‹çš„é¢„æµ‹ç»“æœè®¡ç®—MAE print('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1)) print('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2)) print('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3)) w = [0.3,0.4,0.3] # å®šä¹‰æ¯”é‡æƒå€¼ Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w) print('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre)) å³ä½¿æ˜¯è¿™æ ·å¯¹å¤šä¸ªé¢„æµ‹ç»“æœçš„ç®€å•åŠ æƒï¼Œä¹Ÿå¯ä»¥ç»™å®éªŒç»“æœå¸¦æ¥æå‡ ## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•° def Mean_method(test_pre1,test_pre2,test_pre3): Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1) return Mean_result Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3) print('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre)) ## å®šä¹‰ç»“æœçš„åŠ æƒå¹³å‡å‡½æ•° def Median_method(test_pre1,test_pre2,test_pre3): Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1) return Median_result Median_pre = Median_method(test_pre1,test_pre2,test_pre3) print('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre)) stackingèåˆï¼ˆå›å½’ï¼‰ from sklearn import linear_model def Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression()): model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values,y_train_true) Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values) return Stacking_result ## ç”Ÿæˆä¸€äº›ç®€å•çš„æ ·æœ¬æ•°æ®ï¼Œtest_prei ä»£è¡¨ç¬¬iä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼ train_reg1 = [3.2, 8.2, 9.1, 5.2] train_reg2 = [2.9, 8.1, 9.0, 4.9] train_reg3 = [3.1, 7.9, 9.2, 5.0] # y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼ y_train_true = [3, 8, 9, 5] test_pre1 = [1.2, 3.2, 2.1, 6.2] test_pre2 = [0.9, 3.1, 2.0, 5.9] test_pre3 = [1.1, 2.9, 2.2, 6.0] # y_test_true ä»£è¡¨ç¬¬æ¨¡å‹çš„çœŸå®å€¼ y_test_true = [1, 3, 2, 6] model_L2= linear_model.LinearRegression() Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true, test_pre1,test_pre2,test_pre3,model_L2) print('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre)) æˆ‘ä»¬éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œå¯¹äºç¬¬äºŒå±‚Stackingçš„æ¨¡å‹ä¸å®œé€‰å–çš„è¿‡äºå¤æ‚ åˆ†ç±»æ¨¡å‹èåˆ from sklearn.datasets import make_blobs from sklearn import datasets from sklearn.tree import DecisionTreeClassifier import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from xgboost import XGBClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.datasets import make_moons from sklearn.metrics import accuracy_score,roc_auc_score from sklearn.model_selection import cross_val_score from sklearn.model_selection import StratifiedKFold Voting æŠ•ç¥¨æœºåˆ¶ åˆ†ä¸ºè½¯æŠ•ç¥¨å’Œç¡¬æŠ•ç¥¨ä¸¤ç§ ''' ç¡¬æŠ•ç¥¨ï¼šå¯¹å¤šä¸ªæ¨¡å‹ç›´æ¥è¿›è¡ŒæŠ•ç¥¨ï¼Œä¸åŒºåˆ†æ¨¡å‹ç»“æœçš„ç›¸å¯¹é‡è¦åº¦ï¼Œæœ€ç»ˆæŠ•ç¥¨æ•°æœ€å¤šçš„ç±»ä¸ºæœ€ç»ˆè¢«é¢„æµ‹çš„ç±»ã€‚ ''' iris = datasets.load_iris() x=iris.data y=iris.target x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7, colsample_bytree=0.6, objective='binary:logistic') clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4, min_samples_leaf=63,oob_score=True) clf3 = SVC(C=0.1) # ç¡¬æŠ•ç¥¨ eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard') for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']): scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy') print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) Accuracy: 0.97 (+/- 0.02) [XGBBoosting] Accuracy: 0.33 (+/- 0.00) [Random Forest] Accuracy: 0.95 (+/- 0.03) [SVM] Accuracy: 0.94 (+/- 0.04) [Ensemble] ''' è½¯æŠ•ç¥¨ï¼šå’Œç¡¬æŠ•ç¥¨åŸç†ç›¸åŒï¼Œå¢åŠ äº†è®¾ç½®æƒé‡çš„åŠŸèƒ½ï¼Œå¯ä»¥ä¸ºä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒæƒé‡ï¼Œè¿›è€ŒåŒºåˆ«æ¨¡å‹ä¸åŒçš„é‡è¦åº¦ã€‚ ''' x=iris.data y=iris.target x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic') clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4, min_samples_leaf=63,oob_score=True) clf3 = SVC(C=0.1, probability=True) # è½¯æŠ•ç¥¨ eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1]) clf1.fit(x_train, y_train) for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']): scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy') print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) åˆ†ç±»çš„Stackingã€Blendingèåˆ ''' 5-Fold Stacking ''' from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier import pandas as pd #åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›† data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] #æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹ clfs = [LogisticRegression(solver='lbfgs'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] #åˆ‡åˆ†ä¸€éƒ¨åˆ†æ•°æ®ä½œä¸ºæµ‹è¯•é›† X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020) dataset_blend_train = np.zeros((X.shape[0], len(clfs))) dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs))) #5æŠ˜stacking n_splits = 5 skf = StratifiedKFold(n_splits) skf = skf.split(X, y) for j, clf in enumerate(clfs): #ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹ dataset_blend_test_j = np.zeros((X_predict.shape[0], 5)) for i, (train, test) in enumerate(skf): #5-Foldäº¤å‰è®­ç»ƒï¼Œä½¿ç”¨ç¬¬iä¸ªéƒ¨åˆ†ä½œä¸ºé¢„æµ‹ï¼Œå‰©ä½™çš„éƒ¨åˆ†æ¥è®­ç»ƒæ¨¡å‹ï¼Œè·å¾—å…¶é¢„æµ‹çš„è¾“å‡ºä½œä¸ºç¬¬iéƒ¨åˆ†çš„æ–°ç‰¹å¾ã€‚ X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test] clf.fit(X_train, y_train) y_submission = clf.predict_proba(X_test)[:, 1] dataset_blend_train[test, j] = y_submission dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1] #å¯¹äºæµ‹è¯•é›†ï¼Œç›´æ¥ç”¨è¿™kä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼å‡å€¼ä½œä¸ºæ–°çš„ç‰¹å¾ã€‚ dataset_blend_test[:, j] = dataset_blend_test_j.mean(1) print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j])) clf = LogisticRegression(solver='lbfgs') clf.fit(dataset_blend_train, y) y_submission = clf.predict_proba(dataset_blend_test)[:, 1] print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission))) Blendingï¼Œå…¶å®å’ŒStackingæ˜¯ä¸€ç§ç±»ä¼¼çš„å¤šå±‚æ¨¡å‹èåˆçš„å½¢å¼ å…¶ä¸»è¦æ€è·¯æ˜¯æŠŠåŸå§‹çš„è®­ç»ƒé›†å…ˆåˆ†æˆä¸¤éƒ¨åˆ†ï¼Œæ¯”å¦‚70%çš„æ•°æ®ä½œä¸ºæ–°çš„è®­ç»ƒé›†ï¼Œå‰©ä¸‹30%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ã€‚ åœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åœ¨è¿™70%çš„æ•°æ®ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶åå»é¢„æµ‹é‚£30%æ•°æ®çš„labelï¼ŒåŒæ—¶ä¹Ÿé¢„æµ‹testé›†çš„labelã€‚ åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å°±ç›´æ¥ç”¨è¿™30%æ•°æ®åœ¨ç¬¬ä¸€å±‚é¢„æµ‹çš„ç»“æœåšä¸ºæ–°ç‰¹å¾ç»§ç»­è®­ç»ƒï¼Œç„¶åç”¨testé›†ç¬¬ä¸€å±‚é¢„æµ‹çš„labelåšç‰¹å¾ï¼Œç”¨ç¬¬äºŒå±‚è®­ç»ƒçš„æ¨¡å‹åšè¿›ä¸€æ­¥é¢„æµ‹ å…¶ä¼˜ç‚¹åœ¨äºï¼š 1.æ¯”stackingç®€å•ï¼ˆå› ä¸ºä¸ç”¨è¿›è¡Œkæ¬¡çš„äº¤å‰éªŒè¯æ¥è·å¾—stacker featureï¼‰ 2.é¿å¼€äº†ä¸€ä¸ªä¿¡æ¯æ³„éœ²é—®é¢˜ï¼šgenerlizerså’Œstackerä½¿ç”¨äº†ä¸ä¸€æ ·çš„æ•°æ®é›† ç¼ºç‚¹åœ¨äºï¼š 1.ä½¿ç”¨äº†å¾ˆå°‘çš„æ•°æ®ï¼ˆç¬¬äºŒé˜¶æ®µçš„blenderåªä½¿ç”¨training set10%çš„é‡ï¼‰ 2.blenderå¯èƒ½ä¼šè¿‡æ‹Ÿåˆ 3.stackingä½¿ç”¨å¤šæ¬¡çš„äº¤å‰éªŒè¯ä¼šæ¯”è¾ƒç¨³å¥ ''' Blending ''' #åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›† #åˆ›å»ºè®­ç»ƒçš„æ•°æ®é›† data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] #æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹ clfs = [LogisticRegression(solver='lbfgs'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] #åˆ‡åˆ†ä¸€éƒ¨åˆ†æ•°æ®ä½œä¸ºæµ‹è¯•é›† X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020) #åˆ‡åˆ†è®­ç»ƒæ•°æ®é›†ä¸ºd1,d2ä¸¤éƒ¨åˆ† X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020) dataset_d1 = np.zeros((X_d2.shape[0], len(clfs))) dataset_d2 = np.zeros((X_predict.shape[0], len(clfs))) for j, clf in enumerate(clfs): #ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹ clf.fit(X_d1, y_d1) y_submission = clf.predict_proba(X_d2)[:, 1] dataset_d1[:, j] = y_submission #å¯¹äºæµ‹è¯•é›†ï¼Œç›´æ¥ç”¨è¿™kä¸ªæ¨¡å‹çš„é¢„æµ‹å€¼ä½œä¸ºæ–°çš„ç‰¹å¾ã€‚ dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1] print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j])) #èåˆä½¿ç”¨çš„æ¨¡å‹ clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30) clf.fit(dataset_d1, y_d2) y_submission = clf.predict_proba(dataset_d2)[:, 1] print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission))) åˆ†ç±»çš„Stackingèåˆ(åˆ©ç”¨mlxtend) !pip install mlxtend import warnings warnings.filterwarnings('ignore') import itertools import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import StackingClassifier from sklearn.model_selection import cross_val_score from mlxtend.plotting import plot_learning_curves from mlxtend.plotting import plot_decision_regions # ä»¥pythonè‡ªå¸¦çš„é¸¢å°¾èŠ±æ•°æ®é›†ä¸ºä¾‹ iris = datasets.load_iris() X, y = iris.data[:, 1:3], iris.target clf1 = KNeighborsClassifier(n_neighbors=1) clf2 = RandomForestClassifier(random_state=1) clf3 = GaussianNB() lr = LogisticRegression() sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr) label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier'] clf_list = [clf1, clf2, clf3, sclf] fig = plt.figure(figsize=(10,8)) gs = gridspec.GridSpec(2, 2) grid = itertools.product([0,1],repeat=2) clf_cv_mean = [] clf_cv_std = [] for clf, label, grd in zip(clf_list, label, grid): scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy') print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label)) clf_cv_mean.append(scores.mean()) clf_cv_std.append(scores.std()) clf.fit(X, y) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X=X, y=y, clf=clf) plt.title(label) plt.show() å…¶ä»–æ–¹æ³• å°†ç‰¹å¾æ”¾è¿›æ¨¡å‹ä¸­é¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœå˜æ¢å¹¶ä½œä¸ºæ–°çš„ç‰¹å¾åŠ å…¥åŸæœ‰ç‰¹å¾ä¸­å†ç»è¿‡æ¨¡å‹é¢„æµ‹ç»“æœ ï¼ˆStackingå˜åŒ–ï¼‰ def Ensemble_add_feature(train,test,target,clfs): # n_flods = 5 # skf = list(StratifiedKFold(y, n_folds=n_flods)) train_ = np.zeros((train.shape[0],len(clfs*2))) test_ = np.zeros((test.shape[0],len(clfs*2))) for j,clf in enumerate(clfs): '''ä¾æ¬¡è®­ç»ƒå„ä¸ªå•æ¨¡å‹''' # print(j, clf) '''ä½¿ç”¨ç¬¬1ä¸ªéƒ¨åˆ†ä½œä¸ºé¢„æµ‹ï¼Œç¬¬2éƒ¨åˆ†æ¥è®­ç»ƒæ¨¡å‹ï¼Œè·å¾—å…¶é¢„æµ‹çš„è¾“å‡ºä½œä¸ºç¬¬2éƒ¨åˆ†çš„æ–°ç‰¹å¾ã€‚''' # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test] clf.fit(train,target) y_train = clf.predict(train) y_test = clf.predict(test) ## æ–°ç‰¹å¾ç”Ÿæˆ train_[:,j*2] = y_train**2 test_[:,j*2] = y_test**2 train_[:, j+1] = np.exp(y_train) test_[:, j+1] = np.exp(y_test) # print(\"val auc Score: %f\" % r2_score(y_predict, dataset_d2[:, j])) print('Method ',j) train_ = pd.DataFrame(train_) test_ = pd.DataFrame(test_) return train_,test_ from sklearn.model_selection import cross_val_score, train_test_split from sklearn.linear_model import LogisticRegression clf = LogisticRegression() data_0 = iris.data data = data_0[:100,:] target_0 = iris.target target = target_0[:100] x_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3) x_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test) #æ¨¡å‹èåˆä¸­ä½¿ç”¨åˆ°çš„å„ä¸ªå•æ¨¡å‹ clfs = [LogisticRegression(), RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'), ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'), GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)] New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs) clf = LogisticRegression() # clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30) clf.fit(New_train, y_train) y_emb = clf.predict_proba(New_test)[:, 1] print(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb))) æœ¬æ¬¡èµ›é¢˜ import pandas as pd import numpy as np import warnings import matplotlib import matplotlib.pyplot as plt import seaborn as sns warnings.filterwarnings('ignore') %matplotlib inline import itertools import matplotlib.gridspec as gridspec from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier # from mlxtend.classifier import StackingClassifier from sklearn.model_selection import cross_val_score, train_test_split # from mlxtend.plotting import plot_learning_curves # from mlxtend.plotting import plot_decision_regions from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import train_test_split from sklearn import linear_model from sklearn import preprocessing from sklearn.svm import SVR from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA import lightgbm as lgb import xgboost as xgb from sklearn.model_selection import GridSearchCV,cross_val_score from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error ## æ•°æ®è¯»å– Train_data = pd.read_csv('data/used_car_train_20200313.csv', sep=' ') TestA_data = pd.read_csv('data/used_car_testA_20200313.csv', sep=' ') print(Train_data.shape) print(TestA_data.shape) numerical_cols = Train_data.select_dtypes(exclude = 'object').columns print(numerical_cols) feature_cols = [col for col in numerical_cols if col not in ['SaleID','name','regDate','price']] X_data = Train_data[feature_cols] Y_data = Train_data['price'] X_test = TestA_data[feature_cols] print('X train shape:',X_data.shape) print('X test shape:',X_test.shape) def Sta_inf(data): print('_min',np.min(data)) print('_max:',np.max(data)) print('_mean',np.mean(data)) print('_ptp',np.ptp(data)) print('_std',np.std(data)) print('_var',np.var(data)) print('Sta of label:') Sta_inf(Y_data) X_data = X_data.fillna(-1) X_test = X_test.fillna(-1) def build_model_lr(x_train,y_train): reg_model = linear_model.LinearRegression() reg_model.fit(x_train,y_train) return reg_model def build_model_ridge(x_train,y_train): reg_model = linear_model.Ridge(alpha=0.8)#alphas=range(1,100,5) reg_model.fit(x_train,y_train) return reg_model def build_model_lasso(x_train,y_train): reg_model = linear_model.LassoCV() reg_model.fit(x_train,y_train) return reg_model def build_model_gbdt(x_train,y_train): estimator =GradientBoostingRegressor(loss='ls',subsample= 0.85,max_depth= 5,n_estimators = 100) param_grid = { 'learning_rate': [0.05,0.08,0.1,0.2], } gbdt = GridSearchCV(estimator, param_grid,cv=3) gbdt.fit(x_train,y_train) print(gbdt.best_params_) # print(gbdt.best_estimator_ ) return gbdt def build_model_xgb(x_train,y_train): model = xgb.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.8,\\ colsample_bytree=0.9, max_depth=5) #, objective ='reg:squarederror' model.fit(x_train, y_train) return model def build_model_lgb(x_train,y_train): estimator = lgb.LGBMRegressor(num_leaves=63,n_estimators = 100) param_grid = { 'learning_rate': [0.01, 0.05, 0.1], } gbm = GridSearchCV(estimator, param_grid) gbm.fit(x_train, y_train) return gbm XGBoostäº”æŠ˜äº¤å‰å›å½’éªŒè¯ ## xgb xgr = xgb.XGBRegressor(n_estimators=120, learning_rate=0.1, subsample=0.8,\\ colsample_bytree=0.9, max_depth=7) # ,objective ='reg:squarederror' scores_train = [] scores = [] ## 5æŠ˜äº¤å‰éªŒè¯æ–¹å¼ sk=StratifiedKFold(n_splits=5,shuffle=True,random_state=0) for train_ind,val_ind in sk.split(X_data,Y_data): train_x=X_data.iloc[train_ind].values train_y=Y_data.iloc[train_ind] val_x=X_data.iloc[val_ind].values val_y=Y_data.iloc[val_ind] xgr.fit(train_x,train_y) pred_train_xgb=xgr.predict(train_x) pred_xgb=xgr.predict(val_x) score_train = mean_absolute_error(train_y,pred_train_xgb) scores_train.append(score_train) score = mean_absolute_error(val_y,pred_xgb) scores.append(score) print('Train mae:',np.mean(score_train)) print('Val mae',np.mean(scores)) åˆ’åˆ†æ•°æ®é›†ï¼Œå¹¶ç”¨å¤šç§æ–¹æ³•è®­ç»ƒå’Œé¢„æµ‹ ## Split data with val x_train,x_val,y_train,y_val = train_test_split(X_data,Y_data,test_size=0.3) ## Train and Predict print('Predict LR...') model_lr = build_model_lr(x_train,y_train) val_lr = model_lr.predict(x_val) subA_lr = model_lr.predict(X_test) print('Predict Ridge...') model_ridge = build_model_ridge(x_train,y_train) val_ridge = model_ridge.predict(x_val) subA_ridge = model_ridge.predict(X_test) print('Predict Lasso...') model_lasso = build_model_lasso(x_train,y_train) val_lasso = model_lasso.predict(x_val) subA_lasso = model_lasso.predict(X_test) print('Predict GBDT...') model_gbdt = build_model_gbdt(x_train,y_train) val_gbdt = model_gbdt.predict(x_val) subA_gbdt = model_gbdt.predict(X_test) ä¸€èˆ¬æ¯”èµ›ä¸­æ•ˆæœæœ€ä¸ºæ˜¾è‘—çš„æ–¹æ³• print('predict XGB...') model_xgb = build_model_xgb(x_train,y_train) val_xgb = model_xgb.predict(x_val) subA_xgb = model_xgb.predict(X_test) print('predict lgb...') model_lgb = build_model_lgb(x_train,y_train) val_lgb = model_lgb.predict(x_val) subA_lgb = model_lgb.predict(X_test) print('Sta inf of lgb:') Sta_inf(subA_lgb) åŠ æƒèåˆ def Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]): Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3) return Weighted_result ## Init the Weight w = [0.3,0.4,0.3] ## æµ‹è¯•éªŒè¯é›†å‡†ç¡®åº¦ val_pre = Weighted_method(val_lgb,val_xgb,val_gbdt,w) MAE_Weighted = mean_absolute_error(y_val,val_pre) print('MAE of Weighted of val:',MAE_Weighted) ## é¢„æµ‹æ•°æ®éƒ¨åˆ† subA = Weighted_method(subA_lgb,subA_xgb,subA_gbdt,w) print('Sta inf:') Sta_inf(subA) ## ç”Ÿæˆæäº¤æ–‡ä»¶ sub = pd.DataFrame() sub['SaleID'] = X_test.index sub['price'] = subA sub.to_csv('./sub_Weighted.csv',index=False) ## ä¸ç®€å•çš„LRï¼ˆçº¿æ€§å›å½’ï¼‰è¿›è¡Œå¯¹æ¯” val_lr_pred = model_lr.predict(x_val) MAE_lr = mean_absolute_error(y_val,val_lr_pred) print('MAE of lr:',MAE_lr) Stackingèåˆ ## ç¬¬ä¸€å±‚ train_lgb_pred = model_lgb.predict(x_train) train_xgb_pred = model_xgb.predict(x_train) train_gbdt_pred = model_gbdt.predict(x_train) Strak_X_train = pd.DataFrame() Strak_X_train['Method_1'] = train_lgb_pred Strak_X_train['Method_2'] = train_xgb_pred Strak_X_train['Method_3'] = train_gbdt_pred Strak_X_val = pd.DataFrame() Strak_X_val['Method_1'] = val_lgb Strak_X_val['Method_2'] = val_xgb Strak_X_val['Method_3'] = val_gbdt Strak_X_test = pd.DataFrame() Strak_X_test['Method_1'] = subA_lgb Strak_X_test['Method_2'] = subA_xgb Strak_X_test['Method_3'] = subA_gbdt ## level2-method model_lr_Stacking = build_model_lr(Strak_X_train,y_train) ## è®­ç»ƒé›† train_pre_Stacking = model_lr_Stacking.predict(Strak_X_train) print('MAE of Stacking-LR:',mean_absolute_error(y_train,train_pre_Stacking)) ## éªŒè¯é›† val_pre_Stacking = model_lr_Stacking.predict(Strak_X_val) print('MAE of Stacking-LR:',mean_absolute_error(y_val,val_pre_Stacking)) ## é¢„æµ‹é›† print('Predict Stacking-LR...') subA_Stacking = model_lr_Stacking.predict(Strak_X_test) subA_Stacking[subA_Stacking&lt;10]=10 ## å»é™¤è¿‡å°çš„é¢„æµ‹å€¼ sub = pd.DataFrame() sub['SaleID'] = X_test.index sub['price'] = subA_Stacking sub.to_csv('./sub_Stacking.csv',index=False) print('Sta inf:') Sta_inf(subA_Stacking) ç»éªŒæ€»ç»“ ç»“æœå±‚é¢çš„èåˆï¼Œè¿™ç§æ˜¯æœ€å¸¸è§çš„èåˆæ–¹æ³•ï¼Œå…¶å¯è¡Œçš„èåˆæ–¹æ³•ä¹Ÿæœ‰å¾ˆå¤šï¼Œæ¯”å¦‚æ ¹æ®ç»“æœçš„å¾—åˆ†è¿›è¡ŒåŠ æƒèåˆï¼Œè¿˜å¯ä»¥åšLogï¼Œexpå¤„ç†ç­‰ã€‚åœ¨åšç»“æœèåˆçš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„æ¡ä»¶æ˜¯æ¨¡å‹ç»“æœçš„å¾—åˆ†è¦æ¯”è¾ƒè¿‘ä¼¼ï¼Œç„¶åç»“æœçš„å·®å¼‚è¦æ¯”è¾ƒå¤§ï¼Œè¿™æ ·çš„ç»“æœèåˆå¾€å¾€æœ‰æ¯”è¾ƒå¥½çš„æ•ˆæœæå‡ã€‚ ç‰¹å¾å±‚é¢çš„èåˆï¼Œè¿™ä¸ªå±‚é¢å…¶å®æ„Ÿè§‰ä¸å«èåˆï¼Œå‡†ç¡®è¯´å¯ä»¥å«åˆ†å‰²ï¼Œå¾ˆå¤šæ—¶å€™å¦‚æœæˆ‘ä»¬ç”¨åŒç§æ¨¡å‹è®­ç»ƒï¼Œå¯ä»¥æŠŠç‰¹å¾è¿›è¡Œåˆ‡åˆ†ç»™ä¸åŒçš„æ¨¡å‹ï¼Œç„¶ååœ¨åé¢è¿›è¡Œæ¨¡å‹æˆ–è€…ç»“æœèåˆæœ‰æ—¶ä¹Ÿèƒ½äº§ç”Ÿæ¯”è¾ƒå¥½çš„æ•ˆæœã€‚ æ¨¡å‹å±‚é¢çš„èåˆï¼Œæ¨¡å‹å±‚é¢çš„èåˆå¯èƒ½å°±æ¶‰åŠæ¨¡å‹çš„å †å å’Œè®¾è®¡ï¼Œæ¯”å¦‚åŠ Stakingå±‚ï¼Œéƒ¨åˆ†æ¨¡å‹çš„ç»“æœä½œä¸ºç‰¹å¾è¾“å…¥ç­‰ï¼Œè¿™äº›å°±éœ€è¦å¤šå®éªŒå’Œæ€è€ƒäº†ï¼ŒåŸºäºæ¨¡å‹å±‚é¢çš„èåˆæœ€å¥½ä¸åŒæ¨¡å‹ç±»å‹è¦æœ‰ä¸€å®šçš„å·®å¼‚ï¼Œç”¨åŒç§æ¨¡å‹ä¸åŒçš„å‚æ•°çš„æ”¶ç›Šä¸€èˆ¬æ˜¯æ¯”è¾ƒå°çš„ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"æ¯”èµ›","slug":"æ•°æ®åˆ†æ/æ¯”èµ›","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%AF%94%E8%B5%9B/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"03-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹å»ºæ¨¡è°ƒå‚","slug":"03-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹å»ºæ¨¡è°ƒå‚","date":"2020-04-01T13:22:06.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/04/01/03-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹å»ºæ¨¡è°ƒå‚/","link":"","permalink":"https://mz2sj.github.io/2020/04/01/03-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82/","excerpt":"","text":"ä»Šå¤©è¦æ€»ç»“çš„æ˜¯å»ºæ¨¡è°ƒå‚éƒ¨åˆ†ï¼Œå¯è°“é‡ä¸­ä¹‹é‡ï¼ŒDataWhaleå®˜æ–¹çš„å‚è€ƒä»£ç ååˆ†è¯¦ç»†ï¼Œå¤Ÿè‡ªå·±æ¶ˆåŒ–å¥½ä¸€é˜µå­äº†ã€‚ æ•°æ®å¤„ç† ä½œè€…å‘æˆ‘ä»¬åˆ†äº«äº†ä¸€ç§å‡å°‘å†…å­˜ç©ºé—´å ç”¨çš„æ–¹æ³•ï¼Œå…¶æ€æƒ³æ˜¯å¯¹äºæ¯ä¸ªSerieså°†æ•°æ®ç±»å‹è½¬åŒ–ä¸ºåˆé€‚çš„ç²¾åº¦ï¼Œå¯¹äºéæ•°å€¼å‹æ•°æ®è½¬åŒ–ä¸ºcategoryç±»å‹æ•°æ®ã€‚ def reduce_mem_usage(df): \"\"\" iterate through all the columns of a dataframe and modify the data type to reduce memory usage. \"\"\" start_mem = df.memory_usage().sum() print('Memory usage of dataframe is {:.2f} MB'.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype('category') end_mem = df.memory_usage().sum() print('Memory usage after optimization is: {:.2f} MB'.format(end_mem)) print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem)) return df å¯ä»¥çœ‹å‡ºï¼Œæ”¾å¼ƒä¸å¿…è¦çš„ç²¾åº¦ç¡®å®èƒ½èŠ‚çœä¸å°‘å†…å­˜ç©ºé—´ã€‚ ç®€å•å»ºæ¨¡ä¸åˆ†æ å¯¹äºæ•°å€¼æ•°æ®çš„é¢„æµ‹ï¼Œé¦–å…ˆé‡‡ç”¨äº†æœ€ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ã€‚ from sklearn.linear_model import LinearRegression model = LinearRegression(normalize=True) model = model.fit(train_X, train_y) sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True) å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®çº¿æ€§æ¨¡å‹å¯¹åº”çš„æƒé‡æ¥åˆ¤æ–­å±æ€§çš„é‡è¦ç¨‹åº¦ï¼Œæƒé‡è¶Šå¤§ï¼Œå±æ€§ä¸é¢„æµ‹ç»“æœè¶Šç›¸å…³ã€‚ from matplotlib import pyplot as plt subsample_index=np.random.randint(low=0,high=len(train_y),size=50) plt.scatter(train_X['v_9'][subsample_index],train_y[subsample_index],color='black') plt.scatter(train_X['v_9'][subsample_index],model.predict(train_X.loc[subsample_index]),color='blue') plt.xlabel('v_9') plt.ylabel('price') plt.legend(['True Price','Predicted Price'],loc='upper right') print('The predicted price is obvious different from true price') plt.show() ä½†æ˜¯æˆ‘ä»¬é€‰å–æƒé‡æœ€å¤§çš„å±æ€§å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹åå‘ç°å’ŒçœŸå®æ•°æ®å·®åˆ«åå¤§ï¼Œé‚£ä¹ˆå¾ˆå¯èƒ½æ˜¯æ¨¡å‹é™¤äº†é—®é¢˜ å¯¹æˆ‘ä»¬è¦é¢„æµ‹çš„æ•°æ®ä½œå›¾ import seaborn as sns print('It is clear to see the price shows a exponential distribution') plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train_y) plt.subplot(1,2,2) sns.distplot(train_y[train_y&lt;np.quantile(train_y,0.9)]) æ•°æ®å‘ˆé•¿å°¾åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œlogå˜æ¢ train_y_ln = np.log(train_y + 1) import seaborn as sns print('The transformed price seems like normal distribution') plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train_y_ln) plt.subplot(1,2,2) sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, 0.9)]) é•¿å°¾ç°è±¡æ˜æ˜¾ç¼“è§£ï¼Œå†ä½œé¢„æµ‹ï¼Œé€‰å–æœ€ç›¸å…³çš„æƒé‡ã€‚ plt.scatter(train_X['v_9'][subsample_index],train_y[subsample_index],color='black') plt.scatter(train_X['v_9'][subsample_index],np.exp(model.predict(train_X.loc[subsample_index])),color='blue') plt.xlabel('v_9') plt.ylabel('price') plt.legend(['True Price','Predicted Price'],loc='upper right') print('The predicted price seems normal after np.log(transforming') plt.show() è¿™æ—¶æ‹Ÿåˆæ•ˆæœè¦å¥½å¾—å¤šã€‚ äº¤å‰éªŒè¯ äº¤å‰éªŒè¯å°†æ•°æ®åˆ†æˆè‹¥å¹²ä»½ï¼Œæ¯æ¬¡å–ä¸€ä»½ä½œéªŒè¯é›†ï¼Œå…¶ä½™çš„ä½œè®­ç»ƒé›†ï¼Œè¿™æ ·å¯ä»¥å°†æ¨¡å‹æ‰€æœ‰æ•°æ®éƒ½æœºå‹è®­ç»ƒï¼Œå¹¶ä¸”å¯¹æ¨¡å‹æ³›åŒ–ç»“æœè¿›è¡Œæ£€éªŒã€‚ from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_absolute_error, make_scorer def log_transfer(func): def wrapper(y, yhat): result = func(np.log(y), np.nan_to_num(np.log(yhat))) return result return wrapper scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error))) print('AVG:', np.mean(scores)) scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error)) print('AVG:', np.mean(scores)) scores = pd.DataFrame(scores.reshape(1,-1)) scores.columns = ['cv' + str(x) for x in range(1, 6)] scores.index = ['MAE'] scores zåœ¨è¿™é‡Œå°±æ•å°†æ•°æ®è¾“å‡ºä¸€ä¸€åˆ—å‡ºäº†ã€‚ ä½†æ˜¯ä½¿ç”¨äº¤å‰éªŒè¯æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°æ•°æ®çš„çœŸå®æƒ…å†µï¼Œæ¯”å¦‚æ•°æ®ä¸æ—¶é—´ç›¸å…³æ—¶ï¼Œé‚£ä¹ˆæŠŠåæ—¶é—´é åçš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œæ—¶é—´é å‰çš„ä½œä¸ºéªŒè¯é›†ï¼Œé¢ å€’äº†é¡ºåºï¼Œä¸åˆé€»è¾‘ã€‚åœ¨å®é™…å»ºæ¨¡æ—¶ï¼Œè¦æ³¨æ„è€ƒè™‘è¿™ä¸€ç‚¹ã€‚ ç»˜åˆ¶å­¦ä¹ ç‡éªŒè¯æ›²çº¿ å¯ä»¥ä½¿æˆ‘ä»¬æ¸…æ¥šçš„çœ‹å‡ºè®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®å¾—åˆ†ï¼ˆæŸå¤±ï¼‰çš„ä¸‹é™æƒ…å†µã€‚ from sklearn.model_selection import learning_curve,validation_curve def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )): plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel('Training example') plt.ylabel('score') train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error)) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid()#åŒºåŸŸ plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\") plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label=\"Training score\") plt.plot(train_sizes, test_scores_mean,'o-',color=\"g\", label=\"Cross-validation score\") plt.legend(loc=\"best\") return plt plot_learning_curve(LinearRegression(),'linear_model',train_X[:1000],train_y_ln[:1000],ylim=(0.0,0.5),cv=5,n_jobs=1) å¤šç§æ¨¡å‹å¯¹æ¯” åœ¨äº¤å‰éªŒè¯çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨å¤šä¸ªæ¨¡å‹ from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso models = [LinearRegression(), Ridge(), Lasso()] result = dict() for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') è¿˜å¯ä»¥é€šè¿‡å¸¦æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œç‰¹å¾ç­›é€‰ã€‚ model = Ridge().fit(train_X, train_y_ln) print('intercept:'+ str(model.intercept_)) sns.barplot(abs(model.coef_), continuous_feature_names) model = Lasso().fit(train_X, train_y_ln) print('intercept:'+ str(model.intercept_)) sns.barplot(abs(model.coef_), continuous_feature_names) éçº¿æ€§æ¨¡å‹ å‰é¢è¯´äº†é‚£ä¹ˆå¤šçº¿æ€§å›å½’çš„æ¨¡å‹ï¼Œä¸‹é¢ä»‹ç»éçº¿æ€§æ¨¡å‹ from sklearn.linear_model import LinearRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.neural_network import MLPRegressor from xgboost.sklearn import XGBRegressor from lightgbm.sklearn import LGBMRegressor models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), MLPRegressor(solver='lbfgs', max_iter=100), XGBRegressor(n_estimators = 100, objective='reg:squarederror'), LGBMRegressor(n_estimators = 100)] result = dict() for model in models: model_name = str(model).split('(')[0] scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)) result[model_name] = scores print(model_name + ' is finished') result = pd.DataFrame(result) result.index = ['cv' + str(x) for x in range(1, 6)] result å¯ä»¥çœ‹å‡ºéšæœºæ£®æ—è¿˜æ˜¯é¡¶çš„ã€‚ æ¨¡å‹è°ƒå‚ è´ªå¿ƒè°ƒå‚ ## LGBçš„å‚æ•°é›†åˆï¼š objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair'] num_leaves = [3,5,10,15,20,40, 55] max_depth = [3,5,10,15,20,40, 55] bagging_fraction = [] feature_fraction = [] drop_rate = [] best_obj = dict() for obj in objective: model = LGBMRegressor(objective=obj) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_obj[obj] = score best_leaves = dict() for leaves in num_leaves: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_leaves[leaves] = score best_depth = dict() for depth in max_depth: model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0], max_depth=depth) score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) best_depth[depth] = score sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())]) ç½‘æ ¼è°ƒå‚ from sklearn.model_selection import GridSearchCV parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth} model = LGBMRegressor() clf = GridSearchCV(model, parameters, cv=5) clf = clf.fit(train_X, train_y) model = LGBMRegressor(objective='regression', num_leaves=55, max_depth=15) np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))) ç½‘æ ¼è°ƒå‚æ¯”è¾ƒå¥½ç†è§£ï¼Œæˆ‘ä»¬äº‹å…ˆè¾“å…¥æˆ‘ä»¬è®¤ä¸ºåˆç†çš„å‚æ•°ï¼Œå¯¹æ‰€æœ‰å‚æ•°çš„ç»„åˆè¿›è¡Œå®éªŒï¼Œæ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„å‚æ•°ç»„åˆã€‚ è´å¶æ–¯è°ƒå‚ from bayes_opt import BayesianOptimization def rf_cv(num_leaves, max_depth, subsample, min_child_samples): val = cross_val_score( LGBMRegressor(objective = 'regression_l1', num_leaves=int(num_leaves), max_depth=int(max_depth), subsample = subsample, min_child_samples = int(min_child_samples) ), X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error) ).mean() return 1 - val rf_bo = BayesianOptimization( rf_cv, { 'num_leaves': (2, 100), 'max_depth': (2, 100), 'subsample': (0.1, 1), 'min_child_samples' : (2, 100) } ) å¯¹äºè´å¶æ–¯è°ƒå‚å’Œè´ªå¿ƒè°ƒå‚ï¼Œè‡ªå·±ç¡®å®ä¸å¤Ÿäº†è§£ï¼Œè¿˜éœ€è¦å†ä¸“é—¨å­¦ä¹ ä¸€ä¸‹ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"æ¯”èµ›","slug":"æ¯”èµ›","permalink":"https://mz2sj.github.io/tags/%E6%AF%94%E8%B5%9B/"}]},{"title":"02-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹ç‰¹å¾å·¥ç¨‹","slug":"02-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹ç‰¹å¾å·¥ç¨‹","date":"2020-03-28T11:32:28.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/03/28/02-äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹ä¹‹ç‰¹å¾å·¥ç¨‹/","link":"","permalink":"https://mz2sj.github.io/2020/03/28/02-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","excerpt":"","text":"ä»Šå¤©è¦åšçš„ç¬”è®°æ˜¯ç‰¹å¾å·¥ç¨‹ï¼Œæ„Ÿè§‰è¿™æ¬¡å®˜æ–¹ç»™çš„ä»£ç ä¸æ˜¯ç‰¹åˆ«å…¨ã€‚æœ€è¿‘åœ¨å¿™ä½œä¸šå’Œkaggleçš„æ¯”èµ›ï¼Œåé¢è‡ªå·±æœ‰æ—¶é—´ä¼šå†å®Œå–„ä¸€ä¸‹ã€‚è‡ªå·±å°†æŒ‰ç…§ç‰¹å¾å·¥ç¨‹çš„å¸¸è§ä»»åŠ¡è¿›è¡Œä»‹ç»ã€‚ å¼‚å¸¸å¤„ç† æ•°æ®æ”¶é›†çš„è¿‡ç¨‹ä¸­å¯èƒ½ä¼šäº§ç”Ÿè„æ•°æ®ï¼Œå¯¼è‡´äº§ç”Ÿç¦»ç¾¤å€¼ï¼Œæ•°æ®åˆ†å¸ƒæ˜æ˜¾ä¸åˆç†ï¼Œè¿™æ—¶å€™æˆ‘ä»¬è¦å¯¹æ•°æ®è¿›è¡Œå¼‚å¸¸å¤„ç†ã€‚å¸¸è§çš„å¼‚å¸¸å¤„ç†æ–¹æ³•æœ‰ï¼š é€šè¿‡ç®±å‹å›¾åˆ é™¤å¼‚å¸¸å€¼ å®˜æ–¹æä¾›äº†ç›¸å…³ä»£ç  def outliers_proc(data,col_name,scale=3): def box_plot_outliers(data_ser,box_scale): iqr=box_scale*(data_ser.quantile(0.75)-data_ser.quantile(0.25)) val_low=data_ser.quantile(0.25)-iqr val_up=data_ser.quantile(0.75)+iqr rule_low=(data_ser&lt;val_low) rule_up=(data_ser&gt;val_up) return (rule_low,rule_up),(val_low,val_up) data_n=data.copy() data_series=data_n[col_name] rule,value=box_plot_outliers(data_series,box_scale=scale) index=np.arange(data_series.shape[0])[rule[0]|rule[1]] print('Delete number is :{}'.format(len(index))) data_n=data_n.drop(index) data_n.reset_index(drop=True,inplace=True) print('Now column number is :{}'.format(data_n.shape[0])) index_low=np.arange(data_series.shape[0])[rule[0]] outliers=data_series.iloc[index_low] print('Description of data less than the lower bound is :') print(pd.Series(outliers).describe()) index_up=np.arange(data_series.shape[0])[rule[1]] outliers=data_series.iloc[index_up] print('Description of data larger than the upper bound is:') print(pd.Series(outliers).describe()) fig,ax=plt.subplots(1,2,figsize=(10,7)) sns.boxplot(y=data[col_name],data=data,palette='Set1',ax=ax[0]) sns.boxplot(y=data_n[col_name],data=data_n,palette='Set1',ax=ax[1]) return data_n å¯ä»¥çœ‹å‡ºå¼‚å¸¸å€¼å¾—åˆ°äº†æœ‰æ•ˆå‰”é™¤ï¼Œä¸è¿‡è¿™æ®µä»£ç ä¸æ˜¯å¤ªæ‡‚ï¼Œæœ‰æ—¶é—´è¿˜è¦å†ç ”ç©¶ä¸€ä¸‹ã€‚ BOX-COX è½¬æ¢ï¼ˆå¤„ç†æœ‰ååˆ†å¸ƒï¼‰ å¾…å®Œå–„ ï¼Œæˆ‘ä»¬å¸Œæœ›æ•°æ®å°½é‡ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œé™¤äº†è¿™ç§æ–¹æ³•ï¼Œå¸¸è§çš„è¿˜æœ‰å¯¹æ•°æ®å–logã€å€’æ•°ç­‰ã€‚ é•¿å°¾æˆªæ–­ å¯¹æ­¤è‡ªå·±æŒæ€€ç–‘æ€åº¦ï¼Œç½‘ä¸Šä¹Ÿæ²¡æ‰¾åˆ°ç›¸å…³ä»£ç ã€‚å¦‚æœæ•°æ®çš„ç‰¹å¾å°‘æ—¶ï¼Œæˆªæ–­ä¸€éƒ¨åˆ†æ•°æ®è¿˜èƒ½æ¥å—ï¼Œå¦‚æœç¬¦åˆé•¿å°¾çš„ç‰¹åˆ™å¾ˆå¤šå†è¿›è¡Œé•¿å°¾é˜¶æ®µä¼¼ä¹æ˜¾å¾—ä¸åˆé€‚ã€‚ ç‰¹å¾å½’ä¸€åŒ–/æ ‡å‡†åŒ– sklearnæœ‰ä¸“é—¨çš„æ•°æ®å½’ä¸€åŒ–å’Œæ ‡æ³¨åŒ–çš„åŒ…ï¼Œå½“ç„¶è‡ªå·±å†™ä¹Ÿä¸éš¾ å½’ä¸€åŒ– def max_min(x): return (x-np.min(x))/(np.max(x)-np.min(x)) from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() x = scaler.fit_transform(x) æ ‡å‡†åŒ– def standard(x): return (x-np.mean(x))/(np.var(x)) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() x = scaler.fit_transform(x) å¹‚ç‡åˆ†å¸ƒ è¿™ç§æƒ…å†µæˆ‘è¿˜æ²¡é‡åˆ°è¿‡ï¼Œåœ¨æ­¤è®°å½•ä¸‹ log(1+x1+median)log(\\frac{1+x}{1+median})log(1+median1+xâ€‹) æ•°æ®åˆ†æ¡¶ é€šä¿—ç†è§£ä¸‹ï¼Œå°±æ˜¯å°†æ•°æ®åˆ†æ®µã€‚ä¸¾ä¸ªä¾‹å­å¹´é¾„æ®µçš„åˆ†æ¡¶ ç­‰é¢‘åˆ†æ¡¶ å¾…å®Œå–„ ç­‰è·åˆ†æ¡¶ bin=[i*10 for i in range(31)] data['power_bin']=pd.cut(data['power'],bin,labels=False) data[['power_bin','power']].head() Best-KSåˆ†æ¡¶ å¾…å®Œå–„ å¡æ–¹åˆ†æ¡¶ å¾…å®Œå–„ ç¼ºå¤±å€¼å¤„ç† å¯¹äºæ ‘æ¨¡å‹ï¼Œæ¨¡å‹æœ¬èº«å·²ç»æœ‰äº†å¯¹äºç¼ºå¤±å€¼çš„å¤„ç†æœºåˆ¶ï¼Œè¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥ä¸å¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚ å…¶ä»–çš„å¸¸è§æ–¹æ³•æ˜¯å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ï¼Œå¸¸è§çš„æœ‰å¹³å‡æ•°ã€ä¸­ä½æ•°ã€ä¼—æ•°ç­‰ç­‰ã€‚ åˆ†ç®±ï¼Œå¯¹å«æœ‰ç¼ºå¤±å€¼çš„æ•°æ®è¿›è¡Œåˆ†ç®±ï¼Œç¼ºå¤±çš„æ•°æ®å•ç‹¬ä¸ºä¸€ç±»ï¼Œä½†æ˜¯å¦‚æœè¿˜æƒ³åˆ©ç”¨åŸæ¥æœªç¼ºå¤±çš„æ•°æ®å€¼ï¼Œåˆ†ç®±å¹¶æœªå¯¹ç¼ºå¤±å€¼æ•°æ®è¿›è¡Œå¡«å……ã€‚ ç‰¹å¾æ„é€  æ„é€ ç»Ÿè®¡é‡ å¦‚å„ä¸ªç±»åˆ«çš„æ•°æ®æ±‚å‡å€¼ã€æ–¹å·®ã€æå€¼ã€ä¸­ä½æ•°ç­‰ç­‰ train_gb=train.groupby('brand') all_info={} for kind,kind_data in train_gb: info={} kind_data=kind_data[kind_data['price']&gt;0] info['brand_amount']=len(kind_data) info['brand_price_max']=kind_data.price.max() info['brand_price_median']=kind_data.price.median() info['brand_price_min']=kind_data.price.min() info['brand_price_sum']=kind_data.price.sum() info['brand_price_std']=kind_data.price.std() info['brand_price_average']=round(kind_data.price.sum()/(len(kind_data)+1),2) all_info[kind]=info brand_fe=pd.DataFrame(all_info).T.reset_index().rename(columns={'index':'brand'}) data=data.merge(brand_fe,how='left',on='brand') æ—¶é—´ç‰¹å¾ æ±‚ç›¸å¯¹æ—¶é—´ã€èŠ‚å‡æ—¥ç­‰ï¼Œåœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæ±½è½¦çš„ä½¿ç”¨æ—¶é—´å¾ˆæ˜æ˜¾å°±ä¼šå¯¹é”€å”®ä»·æ ¼äº§ç”Ÿå½±å“ã€‚ ata['used_time']=(pd.to_datetime(data['creatDate'],format='%Y%m%d',errors='coerce')- pd.to_datetime(data['regDate'],format='%Y%m%d',errors='coerce')).dt.days åœ°ç†ç‰¹å¾ å‰ææ˜¯ä½ å¾—çŸ¥é“æ•°æ®ä¸­æ‰€åŒ…å«çš„åœ°ç†ä¿¡æ¯ï¼Œ data['city']=data['regionCode'].apply(lambda x:str(x))[:-3] é™¤äº†ä»¥ä¸Šçš„ç‰¹å¾æ„é€ æ–¹å¼å¤–ï¼Œè¿˜æœ‰ç‰¹å¾ç»„åˆã€å¯¹æ•°æ®çš„éçº¿æ€§å˜æ¢ï¼ˆlogã€æ ¹å€¼ã€å¹³æ–¹ï¼‰ç­‰ç­‰ã€‚ç§ä»¥ä¸ºæ„é€ å¯è§£é‡Šçš„ç‰¹å¾ç»„åˆè¿˜æ¯”è¾ƒæœ‰è¯´æœåŠ›ï¼Œèƒ¡ä¹±ç»„åˆçš„ç‰¹å¾å¯èƒ½å°±æœ‰ç‚¹ç¢°è¿æ°”äº†ã€‚ ç‰¹å¾ç­›é€‰ è¿‡æ»¤å¼ é€šè¿‡ç›¸å…³ç³»æ•°é€‰æ‹©å‘ã€äº’ä¿¡æ¯æ³•ã€å¡æ–¹æ£€éªŒã€Reliefã€æ–¹å·®é€‰æ‹©æ³•é€‰æ‹©ç‰¹å¾ï¼Œç›¸å…³ç³»æ•°æ³•æ¯”è¾ƒå¥½ç†è§£ï¼Œè¶Šç›¸å…³çš„ç‰¹å¾å½“ç„¶æ›´å€¼å¾—é€‰æ‹©ã€‚ data_numeric=data[['power','kilometer','brand_amount','brand_price_average','brand_price_max','brand_price_median']] correlation=data_numeric.corr() f,ax=plt.subplots(figsize=(7,7)) plt.title('Correlation of Numeric Features with Price',y=1,size=16) sns.heatmap(correlation,square=True,vmax=0.8) u1s1,çƒ­åŠ›å›¾å¯ä»¥ç›´æ¥çœ‹å‡ºå“ªäº›ç‰¹å¾æ¯”è¾ƒç›¸å…³ï¼Œä¸è¿‡è¿™ä¸ªå›¾ä¼¼ä¹å°‘äº†priceè¿™ä¸ªå˜é‡ [ åŒ…è£¹å¼ æŠŠå­¦ä¹ å™¨çš„æ€§èƒ½ä½œä¸ºç‰¹å¾å­é›†çš„è¯„ä»·å‡†åˆ™ï¼Œç±»ä¼¼äºauto-mlï¼Ÿæˆ–è€…æ˜¯logisticå›å½’çš„å˜é‡ç³»æ•°ï¼Œç³»æ•°è¶Šå¤§è¶Šç›¸å…³ã€‚åˆ©ç”¨åŒ…å¸®æˆ‘ä»¬è‡ªåŠ©å¯»æ‰¾ç‰¹å¾ from mlxtend.feature_selection import SequentialFeatureSelector as SFS from sklearn.linear_model import LinearRegression sfs=SFS(LinearRegression(), k_features=10, forward=True, floating=False, scoring='r2', cv=0) x=data.drop(['price'],axis=1) x=x.fillna(0) y=y.fillna(0) y=data['price'] sfs.fit(x,y) sfs.k_feature_names_ from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs import matplotlib.pyplot as plt fig1=plot_sfs(sfs.get_metric_dict(),kind='std_dev') plt.grid() plt.show() åµŒå…¥å¼ ç»“åˆè¿‡æ»¤å¼å’ŒåŒ…è£¹å¼ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚å…¶å®å°±æ˜¯æŠŠæ•°æ®è¾“å…¥ç®—æ³•ä¸­ï¼Œè®©ç®—æ³•è‡ªå·±å–é€‰æ‹©ç‰¹å¾å‘—ã€‚ é™ç»´ é™ç»´è‡ªå·±è¿˜æ˜¯æœ‰ç‚¹äº†è§£çš„ï¼Œå¯¹äºé«˜ç»´åº¦æ•°æ®å¯ä»¥é™åˆ°ä½ç»´åº¦ã€‚åœ¨æ•°æ®å¯è§†åŒ–ä¹Ÿå¸¸ç”¨åˆ°ã€‚å¸¸ç”¨çš„é™ç»´ç®—æ³•æœ‰PCAã€t-SNEã€LDAã€LSAç­‰ç­‰ã€‚ è¿™æ¬¡çš„å†…å®¹å¥½å¤šå•Šï¼Œæ„Ÿè§‰ç»™çš„ç¬”è®°ä¹Ÿä¸æ˜¯å¾ˆå…¨ï¼Œè¿˜éœ€è¦è‡ªå·±å†å¤šåšæ€»ç»“ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"ç‰¹å¾å·¥ç¨‹","slug":"æ•°æ®åˆ†æ/ç‰¹å¾å·¥ç¨‹","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"01-äºŒæ‰‹è½¦ä»·æ ¼é”€å”®ä»·æ ¼é¢„æµ‹ä¹‹EDA","slug":"01-äºŒæ‰‹è½¦ä»·æ ¼é”€å”®ä»·æ ¼é¢„æµ‹ä¹‹EDA","date":"2020-03-24T08:59:22.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/03/24/01-äºŒæ‰‹è½¦ä»·æ ¼é”€å”®ä»·æ ¼é¢„æµ‹ä¹‹EDA/","link":"","permalink":"https://mz2sj.github.io/2020/03/24/01-%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%94%80%E5%94%AE%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%E4%B9%8BEDA/","excerpt":"","text":"å†™åœ¨å‰é¢ è¿™æ¬¡å‚åŠ äº†DataWhaleç»„ç»‡çš„äºŒæ‰‹è½¦é”€å”®ä»·å€¼é¢„æµ‹ç»„é˜Ÿå­¦ä¹ ï¼Œå¼€å§‹ç¬¬ä¸€éƒ¨åˆ†çš„å­¦ä¹ â€”â€”æ•°æ®åˆ†æEDAã€‚ä¸»è¦ä»æ•°æ®åˆ†ææµç¨‹å’Œè½¯ä»¶åŒ…å·¥å…·ä¸¤æ–¹é¢æ¥ä»‹ç»è‡ªå·±çš„æ”¶è·ã€‚ æ•°æ®åˆ†ææµç¨‹ æ€»ç»“äº†ä¸€ä¸‹ï¼ŒEDAä¸»è¦åšçš„å·¥ä½œå°±æ˜¯å‘ç°æ•°æ®ç¼ºå¤±å€¼å’ŒæŸ¥çœ‹æ•°æ®åˆ†å¸ƒæƒ…å†µä»¥åŠé¢„æµ‹ç›®æ ‡ä¸å…¶ä»–å› å˜é‡çš„ç›¸å…³æ€§åˆ†æã€‚ ç¼ºå¤±å€¼åˆ†æ å¯¹äºè¡¨æ ¼æ•°æ®ï¼Œä½¿ç”¨DataFrameçš„info()å‡½æ•°å°±å¯ä»¥ç›´è§‚çš„çœ‹åˆ°æ•°æ®çš„ç¼ºå¤±æƒ…å†µï¼Œä½†æ˜¯è¿™ä¸ªåªé’ˆå¯¹æ•°å€¼å‹ï¼ˆnumericï¼‰æ•°æ®ï¼Œç¼ºå¤±æ•°æ®ä¼šä»¥nullå€¼çš„å½¢å¼å‘ˆç°ï¼Œç›´æ¥è°ƒç”¨DataFrameçš„isnull()å‡½æ•°é…åˆsum()å°±å¯ä»¥å¯¹ç¼ºå¤±å€¼æ•°æ®è¿›è¡Œç»Ÿè®¡ã€‚æ¯”è¾ƒå‘çš„æ˜¯objectç±»å‹æ•°æ®ï¼Œæœ‰äº›ç¼ºå¤±å€¼æ•°æ®å¹¶ä¸ä¼šç”¨nullå€¼è¡¨ç¤ºï¼Œåƒä¸‹é¢è¿™ç§ï¼š â€œ-â€æ˜æ˜¾ä»£è¡¨ç¼ºå¤±å€¼æ•°æ®ï¼Œä½†è¢«å­—ç¬¦ä¸²ä»£æ›¿äº†ã€‚å¦‚ä½•å‘ç°è¿™ç§ç¼ºå¤±æƒ…å†µï¼Œéœ€è¦æˆ‘ä»¬å¯¹ç›¸åº”çš„æ•°æ®è°ƒç”¨value_counts()å‡½æ•°ï¼Œæˆ–è€…æ›´ç›´è§‚ç‚¹è§‚å¯Ÿçœ‹ä¸€éƒ¨åˆ†æ•°æ®çœ‹æœ‰æ— å¼‚å¸¸æƒ…å†µã€‚æ¯”å¦‚ç±»åˆ«æ•°æ®ä¸­ï¼Œæœ‰çš„ç±»åˆ«åªå‡ºç°äº†ä¸€æ¬¡ï¼Œå’Œå…¶ä»–ç±»åˆ«å‹æ ¹ä¸æ˜¯ä¸€ä¸ªæ•°é‡çº§ï¼Œé‚£ä¹ˆå¾ˆæœ‰å¯èƒ½æ˜¯å¼‚å¸¸å€¼ï¼Œå¯ä»¥è€ƒè™‘å°†å…¶åˆ å»ã€‚ æ•°æ®åˆ†å¸ƒ é¦–å…ˆæ˜¯é¢„æµ‹å€¼çš„æ•°æ®åˆ†å¸ƒæƒ…å†µï¼Œå…¶æ¬¡æ˜¯å…¶ä»–æ•°æ®ï¼ˆæˆ–è€…è¯´æ˜¯è‡ªå˜é‡ï¼‰çš„åˆ†å¸ƒæƒ…å†µã€‚å¯¹äºæ•°å€¼å‹æ•°æ®æˆ‘ä»¬å¯ä»¥ç”¨æ­£å¤ªåˆ†å¸ƒã€å¯¹æ•°æ­£æ€åˆ†å¸ƒè¿›è¡Œæ‹Ÿåˆï¼Œå…¶ä»–åˆ†å¸ƒè‡ªå·±è¿˜ä¸å¤ªäº†è§£ã€‚å¯¹äºç±»åˆ«ç‰¹å¾ï¼Œæˆ‘ä»¬è¦æŸ¥çœ‹å„ç§ç±»åˆ«å¯¹åº”çš„æ•°é‡ï¼Œå¯ä»¥ç”¨æŸ±çŠ¶å›¾æ¥å¯è§†åŒ–ã€‚ä¹Ÿå¯ä»¥æŸ¥çœ‹ä¸åŒç±»åˆ«çš„æ•°æ®å¯¹åº”çš„ç›®æ ‡é¢„æµ‹å€¼ï¼ˆpriceï¼‰çš„åˆ†å¸ƒæƒ…å†µï¼Œå¯ä»¥ç”¨æ¥åˆ†æçš„å›¾è¡¨æœ‰ç®±å‹å›¾ã€å°æç´å›¾ã€‚ç®±å‹å›¾å¯ä»¥å¸®åŠ©äº†è§£æ•°æ®çš„ç‰¹å¾èŒƒå›´ï¼Œä¹Ÿå¯ç”¨æ¥æŸ¥çœ‹ç¦»ç¾¤ç‚¹ã€‚å°æç´å›¾åœ¨ç®±å‹å›¾çš„åŸºç¡€ä¸Šæ˜¾ç¤ºäº†ç›®æ ‡æ•°æ®å€¼çš„åˆ†å¸ƒæƒ…å†µã€‚ ç›¸å…³æ€§åˆ†æ ä¸»è¦æ˜¯é¢„æµ‹å€¼å’Œè‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æï¼Œè¿˜æœ‰å„ä¸ªè‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æã€‚å¸¸ç”¨æ•£ç‚¹å›¾æ¥ç»˜åˆ¶ï¼Œçœ‹æ•°æ®ä¹‹é—´æ˜¯ä¸æ˜¯æœ‰ç›¸å…³æ€§å…³ç³»ã€‚å¦‚æœé¢„æµ‹å€¼å’ŒæŸä¸ªè‡ªå˜é‡æœ‰å¾ˆå¼ºçš„çº¿æ€§å…³ç³»ï¼Œé‚£ä¹ˆæ¯«æ— ç–‘é—®è¿™ä¸ªå˜é‡å¯¹æˆ‘ä»¬çš„é¢„æµ‹æ˜¯ååˆ†æœ‰ç”¨çš„ã€‚å¦‚æœæœ‰ä¸¤ä¸ªè‡ªå˜é‡å®ƒä»¬ä¹‹é—´å…·æœ‰å¾ˆæ˜æ˜¾çš„çº¿æ€§å…³ç³»ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åªä¿ç•™å…¶ä¸­ä¸€ä¸ªã€‚ç›¸å…³æ€§åˆ†æä¸­å¸¸ç”¨åˆ°çš„å›¾è¡¨æœ‰çƒ­åŠ›å›¾ã€æ•£ç‚¹å›¾ç­‰ç­‰ã€‚ è½¯ä»¶å·¥å…·æŠ€èƒ½åŒ… ä»¥å‰åªçŸ¥é“ç”¨pandasè¯»æ•°æ®ï¼Œmatplotlibç»˜å›¾ï¼Œè¿™æ¬¡å­¦åˆ°äº†ä¸€äº›æ–°çŸ¥è¯†ï¼Œåœ¨æ­¤è®°å½•ã€‚ missingno è§åçŸ¥æ„ï¼Œè¿™ä¸ªåŒ…æ˜¯ç”¨æ¥å¤„ç†ç¼ºå¤±å€¼çš„ã€‚ missingno.matrix(dataframe) missingno.bar(dataframe) å¯ä»¥ç›´è§‚çš„è®©æˆ‘ä»¬çœ‹å‡ºå“ªäº›æ•°æ®æœ‰ç¼ºå¤±å€¼ pandas DataFrame.skew() DataFrame.kurt() æŸ¥çœ‹æ•°å€¼å‹æ•°æ®çš„ååº¦å’Œå³°åº¦ DataFrame.corr() è®¡ç®—æ•°æ®çš„ç›¸å…³æ€§ Series.astype('category') Series.cat.add_category(list) astypeå°†æŸä¸ªSeiriesè½¬æˆcategoryå‹æ•°æ® add_categoryä¸ºcategoryå‹æ•°æ®æ·»åŠ æ–°çš„ç±»åˆ« pd.melt(dataframe,id_vars,value_vars,var_name,value_name) å°†åˆ—åè½¬ä¸ºåˆ—æ•°æ®ï¼Œid_varsæ˜¯ä¿ç•™çš„åˆ—ï¼Œvalue_varsæ˜¯è¦è½¬æ¢çš„åˆ—ï¼Œvar_nameã€value_nameå¯¹åº”ä¿ç•™å’Œè½¬æ¢åçš„åˆ—åï¼Œç›—ç”¨ä¸€å¼ å›¾è¡¨ç¤º seaborn æœ‰ä¸­æ–‡çš„æ–‡æ¡£ï¼ŒæŸ¥é˜…èµ·æ¥ååˆ†æ–¹ä¾¿ ä»¥å‰ç»˜å›¾ä½¿ç”¨çš„éƒ½æ˜¯matplotlibï¼Œä»Šå¤©è§è¯†åˆ°äº†seabornçš„å¼ºå¤§ï¼Œå¹¶ä¸”ç»˜åˆ¶å‡ºæ¥çš„å›¾ä¹Ÿååˆ†å¥½çœ‹ seaborn.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, color=None, vertical=False, norm_hist=False, axlabel=None, label=None, ax=None) çµæ´»ç»˜åˆ¶å•å˜é‡è§‚æµ‹å€¼åˆ†å¸ƒå›¾ ,fitå¯ä»¥é…åˆscipyä½¿ç”¨ï¼ŒæŸ¥çœ‹æ•°æ®æ˜¯å¦ç¬¦åˆæŸç§æ•°å­¦åˆ†å¸ƒ seaborn.heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels='auto', yticklabels='auto', mask=None, ax=None, **kwargs) å¯ä»¥å°†pandaså¤„ç†çš„corræ•°æ®æ”¾è¿›å»ï¼Œç»˜åˆ¶å‡ºçƒ­åŠ›å›¾ï¼Œé€‰æ‹©ä¸åŒçš„ä¸»é¢˜å°±å¾ˆèˆ’æœ seaborn.pairplot(data, hue=None, hue_order=None, palette=None, vars=None, x_vars=None, y_vars=None, kind='scatter', diag_kind='auto', markers=None, height=2.5, aspect=1, dropna=True, plot_kws=None, diag_kws=None, grid_kws=None, size=None) diag_kindé€‰æ‹©å¯¹è§’çº¿å›¾çš„ç±»å‹ï¼Œå¯é€‰ {â€˜autoâ€™, â€˜histâ€™, â€˜kdeâ€™} ä¸‰ç§ç±»å‹ seaborn.regplot(x, y, data=None, x_estimator=None, x_bins=None, x_ci='ci', scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, dropna=True, x_jitter=None, y_jitter=None, label=None, color=None, marker='o', scatter_kws=None, line_kws=None, ax=None) æ³¨æ„å’Œä¹‹å‰çš„ä¸åŒï¼Œç”±äºæ˜¯ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å›å½’ï¼Œæ‰€ä»¥x,yå€¼å¾—æ˜¯å˜é‡åï¼Œdataæ‰æ˜¯DataFrameæ•°æ® seaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, notch=False, ax=None, **kwargs) åŒæ ·xã€yæ˜¯å­—ç¬¦ä¸²ï¼Œdataæ˜¯æ•°æ® seaborn.violinplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, bw='scott', cut=2, scale='area', scale_hue=True, gridsize=100, width=0.8, inner='box', split=False, dodge=True, orient=None, linewidth=None, color=None, palette=None, saturation=0.75, ax=None, **kwargs) å°æç´å›¾çš„åŠŸèƒ½ä¸ç®±å‹å›¾ç±»ä¼¼ã€‚ å®ƒæ˜¾ç¤ºäº†ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰åˆ†ç±»å˜é‡å¤šä¸ªå±æ€§ä¸Šçš„å®šé‡æ•°æ®çš„åˆ†å¸ƒï¼Œä»è€Œå¯ä»¥æ¯”è¾ƒè¿™äº›åˆ†å¸ƒã€‚ä¸ç®±å½¢å›¾ä¸åŒï¼Œå…¶ä¸­æ‰€æœ‰ç»˜å›¾å•å…ƒéƒ½ä¸å®é™…æ•°æ®ç‚¹å¯¹åº”ï¼Œå°æç´å›¾æè¿°äº†åŸºç¡€æ•°æ®åˆ†å¸ƒçš„æ ¸å¯†åº¦ä¼°è®¡ã€‚ seaborn.FacetGrid(data, row=None, col=None, hue=None, col_wrap=None, sharex=True, sharey=True, height=3, aspect=1, palette=None, row_order=None, col_order=None, hue_order=None, hue_kws=None, dropna=True, legend_out=True, despine=True, margin_titles=False, xlim=None, ylim=None, subplot_kws=None, gridspec_kws=None, size=None) æ¥äº†éš¾ç‚¹äº†ï¼Œä»¥å‰çš„åªè¦ä»£æ•°æ®å°±å¯ä»¥ï¼Œè¿™ä¸ªç½‘æ ¼ç¨å¾®å¤æ‚ç‚¹ col_wrap æŒ‡å®šç½‘ç»œçš„åˆ—æ•°ï¼ŒcolæŒ‡å®šåˆ—ç»´åº¦ï¼ŒrowæŒ‡å®šè¡Œç»´åº¦ï¼ŒhueæŒ‡å®šç¬¬ä¸‰ä¸ªç»´åº¦ã€‚åœ¨æŒ‡å®šç»´åº¦ä¸Šï¼Œä¼šç»˜åˆ¶å¯¹åº”ç±»åˆ«çš„æ•°æ®ã€‚ ä¹‹åå¯ä»¥è°ƒç”¨FacetGridå¯¹è±¡çš„mapæ–¹æ³•ã€‚åƒä¸‹é¢çš„å°æ —å­ g = sns.FacetGrid(tips, col=\"time\") g.map(plt.hist, \"tip\"); colå¯¹åº”çš„ç»´åº¦æ˜¯timeï¼Œè€Œtimeæœ‰Lunchå’ŒDinnerä¸¤ç±»æ•°æ®ï¼Œå°±ä¼šåˆ†åˆ«åœ¨ä¸¤åˆ—ç»˜åˆ¶å¯¹åº”çš„æ•°å¥ã€‚mapæ–¹æ³•ä¼ å…¥è¦ç»˜åˆ¶çš„å›¾ç±»å‹å‡½æ•°plt.hist,å¹¶ä¼ é€’å‚æ•°åç§°ã€‚ g = sns.FacetGrid(tips, col=\"sex\", hue=\"smoker\") g.map(plt.scatter, \"total_bill\", \"tip\", alpha=.7) g.add_legend(); è¿™æ˜¯ä¼ å…¥ç¬¬ä¸‰ç»´åº¦çš„ä¾‹å­ å¥½å•¦ï¼Œè¿™æ¬¡å°±éƒ½è¿™é‡Œäº†ï¼Œå¿«åˆ°deadlineäº†ï¼Œä¸Šä¼ ä¸‹æˆ‘çš„åšå®¢ã€‚","categories":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Regression","slug":"æ•°æ®åˆ†æ/Regression","permalink":"https://mz2sj.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/Regression/"}],"tags":[{"name":"æ•°æ®åˆ†æ","slug":"æ•°æ®åˆ†æ","permalink":"https://mz2sj.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"EDA","slug":"EDA","permalink":"https://mz2sj.github.io/tags/EDA/"},{"name":"å›å½’","slug":"å›å½’","permalink":"https://mz2sj.github.io/tags/%E5%9B%9E%E5%BD%92/"},{"name":"æ¯”èµ›","slug":"æ¯”èµ›","permalink":"https://mz2sj.github.io/tags/%E6%AF%94%E8%B5%9B/"}]},{"title":"04-spelling-correction","slug":"04-spelling-correction","date":"2020-02-16T05:42:05.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/02/16/04-spelling-correction/","link":"","permalink":"https://mz2sj.github.io/2020/02/16/04-spelling-correction/","excerpt":"","text":"ä»Šå¤©è¦ä»‹ç»çš„æ˜¯æ‹¼å†™æ£€æŸ¥ï¼Œä¸»è¦åŸºäºè¯­è¨€æ¨¡å‹ã€‚ Spelling Tasks æ‹¼å†™æ£€æŸ¥ä»»åŠ¡ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ–¹é¢ï¼Œä¸€æ–¹é¢æ˜¯æ‹¼å†™é”™è¯¯ä¾¦æµ‹ï¼Œå¦ä¸€æ–¹é¢æ˜¯æ‹¼å†™é”™è¯¯ä¿®æ­£ã€‚ æ‹¼å†™é”™è¯¯åˆ†ä¸ºä¸¤ç§ï¼š 1.Non-word Errorsï¼šæ¯”å¦‚æ¼å†™å¯¼è‡´çš„é”™è¯¯ï¼Œgraffe-&gt;giraffeã€‚è¿™ç±»é”™è¯¯äº§ç”Ÿçš„è¯ä¸å­˜åœ¨ã€‚å¯ä»¥é€šè¿‡è¯å…¸æ¥è¿›è¡Œä¾¦æµ‹ã€‚æ‰¾å‡ºä¸è¿™äº›è¯ç›¸è¿‘çš„è¯ï¼Œæœ‰ä¸¤ç§æ–¹æ³•ï¼Œæœ€å°ç¼–è¾‘è·¯å¾„æ³•ï¼Œä¿¡é“æ³•ï¼ˆHighest noisy channel probabilityï¼‰ã€‚ 2.Real-word Errorsï¼šä¸€ç§æ˜¯æ‰“å°é”™è¯¯ï¼Œå¦‚ three-&gt;thereé¡ºåºé¢ å€’ã€‚å¦ä¸€ç§æ˜¯è®¤çŸ¥é”™è¯¯ï¼Œå¦‚piece-&gt;peace,too-&gt;twoï¼Œå‘éŸ³ç›¸åŒå¯¼è‡´çš„é”™è¯¯ã€‚è¿™ç±»é”™è¯¯äº§ç”Ÿçš„è¯æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œè¦å…ˆæ‰¾åˆ°ä¸è¿™äº›è¯å‘éŸ³æ‹¼å†™ç›¸è¿‘çš„è¯å½¢æˆå€™é€‰é›†ï¼Œåœ¨é€šè¿‡ä¿¡é“æ¨¡å‹å’Œåˆ†ç±»å™¨é€‰æ‹©æœ€å¥½çš„å€™é€‰è¯ã€‚ The Nosiy Channel Model of Spelling The intuition of Noisy Model is that the words we see may be wrong words were produced through Noisy Channelã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ‹¼å†™é”™è¯¯çš„è¯ï¼Œå¯ä»¥å»ºç«‹ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹æ‰¾åˆ°æ­£ç¡®çš„è¯ã€‚ w^=argmaxâ¡wâˆˆVP(wâˆ£x)=argmaxâ¡wâˆˆVP(xâˆ£w)P(w)P(x)=argmaxâ¡wâˆˆVP(xâˆ£w)P(w)\\begin{aligned} \\hat{w} &amp;=\\underset{w \\in V}{\\operatorname{argmax}} P(w | x) \\\\ &amp;=\\underset{w \\in V}{\\operatorname{argmax}} \\frac{P(x | w) P(w)}{P(x)} \\\\ &amp;=\\underset{w \\in V}{\\operatorname{argmax}} P(x | w) P(w) \\end{aligned} w^â€‹=wâˆˆVargmaxâ€‹P(wâˆ£x)=wâˆˆVargmaxâ€‹P(x)P(xâˆ£w)P(w)â€‹=wâˆˆVargmaxâ€‹P(xâˆ£w)P(w)â€‹ Væ˜¯è¯è¡¨ Non-word spellig error example æ¯”å¦‚acressè¿™ä¸ªè¯æˆ‘ä»¬è¦æ‰¾åˆ°ä»–çš„æ‹¼å†™æ­£ç¡®çš„è¯ é¦–å…ˆè¦äº§ç”Ÿå€™é€‰é›†ï¼Œå€™é€‰é›†çš„äº§ç”Ÿåˆä¸¤ç§ï¼Œä¸€ç§æ˜¯åŸºäºæ‹¼å†™ä¸Šçš„ç›¸è¿‘ï¼Œå¦ä¸€ç§æ˜¯åŸºäºå‘éŸ³ä¸Šçš„ç›¸è¿‘ã€‚ æœ€å°ç¼–è¾‘è·ç¦»åˆæœ‰å››ç§ç±»å‹ï¼šæ’å…¥ï¼Œåˆ é™¤ï¼Œæ›¿æ¢ï¼Œç›¸é‚»ä½ç½®è¯çš„æ›¿æ¢ã€‚æ’å…¥åˆ é™¤ ç¼–è¾‘è·ç¦»ä¸º1ï¼Œæ›¿æ¢ç¼–è¾‘è·ç¦»ä¸º2 å¤§éƒ¨åˆ†æ‹¼å†™é”™è¯¯çš„ç¼–è¾‘è·ç¦»éƒ½åœ¨1ä»¥å†…ï¼Œå‡ ä¹æ‰€æœ‰æ‹¼å†™é”™è¯¯éƒ½åŒ…å«åœ¨ç¼–è¾‘è·ç¦»2. ä¼šç”¨åˆ°ä¸¤ç§æ¨¡å‹ï¼Œä¸€ç§æ˜¯Language Modelï¼Œå¦ä¸€ç§æ˜¯Channel Model Probabilityï¼Œå³æ­£ç¡®çš„è¯äº§ç”ŸæŸç§æ‹¼å†™é”™è¯¯çš„æ¦‚ç‡ã€‚ P(xâˆ£w)=P(x | w)=P(xâˆ£w)= probability of the edit wwwæ˜¯æ­£ç¡®è¯ xxxæ˜¯æ‹¼å†™é”™è¯¯çš„è¯ deletion/insertion/substitution/transposition) æ‹¼å†™é”™è¯¯çš„å››ç§æƒ…å†µå¯ä»¥äº§ç”Ÿå¯¹åº”çš„æ··æ·†çŸ©é˜µ del[x,y]: count(xy typed as x) ins[x,y]: count(x typed as xy) sub[x,y]: count(x typed as y) trans[x,y]: count(xy typed as yx) åŸºäºè¯­æ–™è¿›è¡Œç»Ÿè®¡ï¼Œæ¯”å¦‚æœ‰å¤šå°‘æ¬¡xyè¢«æ‹¼æˆäº†xå•Šï¼Œxè¢«æ‹¼æˆäº†xy åŸºäºä¸Šå›¾çš„æ··æ·†çŸ©é˜µæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—å¯¹åº”é”™è¯¯çš„æ¦‚ç‡ P(xâˆ£w)={delâ¡[wiâˆ’1,wi]countâ¡[wiâˆ’1,wi], if deletion insâ¡[wiâˆ’1,xi]countâ¡[wiâˆ’1], if insertion subâ¡[xi,wi]countâ¡[wi], if substitution transâ¡[wi,wi+1]countâ¡[wi,wi+1], if transposition P(x | w)=\\left\\{\\begin{array}{cl} {\\frac{\\operatorname{del}\\left[w_{i-1}, w_{i}\\right]}{\\operatorname{count}\\left[w_{i-1}, w_{i}\\right]},} &amp; {\\text { if deletion }} \\\\ {\\frac{\\operatorname{ins}\\left[w_{i-1}, x_{i}\\right]}{\\operatorname{count}\\left[w_{i-1}\\right]},} &amp; {\\text { if insertion }} \\\\ {\\frac{\\operatorname{sub}\\left[x_{i}, w_{i}\\right]}{\\operatorname{count}\\left[w_{i}\\right]},} &amp; {\\text { if substitution }} \\\\ {\\frac{\\operatorname{trans}\\left[w_{i}, w_{i+1}\\right]}{\\operatorname{count}\\left[w_{i}, w_{i+1}\\right]},} &amp; {\\text { if transposition }} \\end{array}\\right. P(xâˆ£w)=â©âªâªâªâªâ¨âªâªâªâªâ§â€‹count[wiâˆ’1â€‹,wiâ€‹]del[wiâˆ’1â€‹,wiâ€‹]â€‹,count[wiâˆ’1â€‹]ins[wiâˆ’1â€‹,xiâ€‹]â€‹,count[wiâ€‹]sub[xiâ€‹,wiâ€‹]â€‹,count[wiâ€‹,wi+1â€‹]trans[wiâ€‹,wi+1â€‹]â€‹,â€‹ if deletion if insertion if substitution if transposition â€‹ è¿™äº›æ¦‚ç‡å¯ä»¥ç”¨æ¥å¯¹åº”å€™é€‰é›†å„ä¸ªå•è¯çš„æ¦‚ç‡ï¼Œé”™è¯¯è¯å¯¹åº”çš„å¯èƒ½çš„çœŸå®è¯çš„æ¦‚ç‡ å†åŠ ä¸Šlanguage model æˆ‘ä»¬å°±å¯ä»¥æ‰¾åˆ°æœ€å¤§æ¦‚ç‡çš„è¯ï¼Œå½“ç„¶å•¦è¿™åªæ˜¯åŸºäºå½“å‰è¯unigram å†çœ‹bigram â€œa stellar and versatile acress whose combination of sass and glamourâ€¦â€ â€¢P(actress|versatile)=.000021 P(whose|actress) = .0010 â€¢P(across|versatile) =.000021 P(whose|across) = .000006 åŸºäºbigramè®¡ç®—å‡ºå¯¹åº”å‰åä¸¤ä¸ªè¯ç»„æˆåºåˆ—çš„æ¦‚ç‡ â€¢P(â€œversatile actress whoseâ€) = .000021*.0010 = 210 x10-10 â€¢P(â€œversatile across whoseâ€) = .000021*.000006 = 1 x10-10 å†ä¹˜ä¸Šå¯¹åº”çš„ç¼–è¾‘è·ç¦»æ¦‚ç‡ï¼Œå°±èƒ½å¾—å‡ºå€™é€‰é›†å„ä¸ªè¯çš„æ¦‚ç‡ Real-word spelling errors å°±æ˜¯è¿™ä¸ªè¯åœ¨å­—å…¸ä¸­æ˜¯å­˜åœ¨çš„ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯æˆ‘ä»¬è¦çš„é‚£ä¸ªè¯ã€‚ â€¢The design an construction of the systemâ€¦ â€¢Can they lave him my messages? ç¬¬ä¸€æ­¥ä¹Ÿæ˜¯äº§ç”Ÿå€™é€‰é›†ï¼Œfor each word in sentence å› ä¸ºè¿™äº›è¯æ˜¯çœŸå®å­˜åœ¨ä¸å­—å…¸ä¸­çš„ï¼Œæ‰€ä»¥è¯è‡ªèº«ä¹Ÿæ˜¯åœ¨å€™é€‰é›†é‡Œï¼Œå…¶ä»–çš„è¿˜åŒ…æ‹¬å•ä¸€ç¼–è¾‘è·ç¦»è¯ï¼Œå‘éŸ³ç›¸è¿‘çš„è¯ç­‰ã€‚ çœŸå®å­˜åœ¨è¯æ‹¼å†™æ£€æŸ¥æµç¨‹æ¯”è¾ƒæ˜æ˜¾çš„ç‰¹ç‚¹å°±æ˜¯å®ƒæ˜¯åŸºäºæ•´ä¸ªå¥å­ï¼Œå¯¹æ¯ä¸ªè¯éƒ½äº§ç”Ÿå€™é€‰é›†ï¼Œå°†æ‰€æœ‰è¯è¿›è¡Œç»„åˆï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªåºåˆ—è·¯å¾„çš„è¯ â€¢Given a sentence w1,w2,w3,â€¦,wn â€¢Generate a set of candidates for each word wi â€¢Candidate(w1) = {w1, wâ€™1 , wâ€™â€™1 , wâ€™â€™â€™1 ,â€¦} â€¢Candidate(w2) = {w2, wâ€™2 , wâ€™â€™2 , wâ€™â€™â€™2 ,â€¦} â€¢Candidate(wn) = {wn, wâ€™n , wâ€™â€™n , wâ€™â€™â€™n ,â€¦} â€¢Choose the sequence W that maximizes P(W) ä¸‹é¢è¿™å¼ å›¾åº”è¯¥å°±æ¯”è¾ƒå¥½ç†è§£äº† å¾ˆæ˜æ˜¾ï¼Œtwo of theåº”è¯¥æ˜¯è®¡ç®—åæ¦‚ç‡æœ€å¤§çš„è·¯å¾„ã€‚ ä¹Ÿå¯ä»¥ç®€åŒ–è®¡ç®—ï¼Œæ¯æ¬¡åªæ›¿æ¢è·¯å¾„ä¸­çš„ä¸€ä¸ªè¯ï¼Œä¸€èˆ¬æ¥è¯´æ‹¼å†™é”™è¯¯åªä¼šæ‹¼é”™ä¸€ä¸ªå§ã€‚ å’Œnon-word errorç›¸æ¯”,real-word errorå¤šäº†ä¸€ä¸ªæ¦‚ç‡è®¡ç®—ï¼Œno error $$\\mathrm{P}(\\mathrm{w} | \\mathrm{w})$$ é‚£ä¹ˆè¿™ä¸ªè¯æœ¬èº«å°±æ˜¯æ­£ç¡®çš„æ¦‚ç‡æ˜¯å¤šå°‘å‘¢ï¼Ÿéœ€è¦æ ¹æ®å…·ä½“åº”ç”¨è€Œå®šã€‚å‡è®¾ä½ è¦åšçš„åº”ç”¨ï¼Œåä¸ªè¯å°±æœ‰ä¸€ä¸ªæ˜¯é”™è¯¯çš„ï¼Œé‚£ä¹ˆå°±æ˜¯0.90ï¼Œ20ä¸ªè¯é‡Œé¢æœ‰1ä¸ªæ˜¯é”™è¯¯çš„å°±æ˜¯0.95. çœ‹ä¸€ä¸‹ï¼Œç°é¢çš„ä¾‹å­ åœ¨unigramæ¨¡å‹ä¸­ï¼Œthewå¾ˆæœ‰å¯èƒ½å°±æ˜¯theæ‹¼é”™çš„ï¼Œå¦‚æœå‡è®¾è¯­æ–™çš„æ‹¼å†™é”™è¯¯æ¦‚ç‡æ˜¯0.9çš„æƒ…å†µä¸‹ State-of-the-art Systems ä¸å†ç®€å•çš„æŠŠå…ˆéªŒæ¦‚ç‡ä¹˜ä¸Šchannel modeläº§ç”Ÿçš„é”™è¯¯æ¦‚ç‡ï¼Œå°†ç‹¬ç«‹æ€§å‡è®¾è½¬ä¸ºæ¦‚ç‡ä¸ç›¸åŒ w^=argmaxâ¡wâˆˆVP(xâˆ£w)P(w)Î»\\hat{w}=\\underset{w \\in V}{\\operatorname{argmax}} P(x | w) P(w)^{\\lambda} w^=wâˆˆVargmaxâ€‹P(xâˆ£w)P(w)Î» é’ˆå¯¹non-word errorï¼Œäº§ç”Ÿäº†ä¸Šé¢çš„å…¬å¼ï¼ŒÎ»\\lambdaÎ»å¯ä»¥é€šè¿‡development test set ä¸­å­¦ä¹ å‡ºæ¥ã€‚ Phonetic error model ç®€å•çš„æ¥è¯´å°±æ˜¯åŠ ä¸Šå‘éŸ³æ¥ä¿®æ­£æ‹¼å†™é”™è¯¯ï¼ŒåŸæ¥çš„æ‹¼å†™é”™è¯¯éƒ½æ˜¯åŸºäºå­—æ¯çš„æ‹¼å†™é”™è¯¯ï¼Œç°åœ¨æŠŠå‘éŸ³å› ç´ ä¹Ÿè€ƒè™‘åˆ°æ‹¼å†™é”™è¯¯ã€‚æ¯”å¦‚ä¸‹é¢çš„è§„åˆ™ï¼š â€¢â€œDrop duplicate adjacent letters, except for C.â€ â€¢â€œIf the word begins with â€˜KNâ€™, â€˜GNâ€™, â€˜PNâ€™, â€˜AEâ€™, â€˜WRâ€™, drop the first letter.â€ â€œDrop â€˜Bâ€™ if after â€˜Mâ€™ and if it is at the end of the å»æ‰è¿™äº›å­—æ¯åï¼Œæ ¹æ®å‘éŸ³æ¥è¿›è¡Œspelling correctionï¼Ÿï¼Ÿï¼Ÿ æ¥ç€æ‰¾åˆ°å’Œæ‹¼å†™é”™è¯¯è¯å‘éŸ³ç›¸å·®1-2çš„æœ€å°ç¼–è¾‘é”™è¯¯å€™é€‰è¯ è®¡ç®—æ¦‚ç‡æ—¶å¯¹ä¸åŒçš„ç¼–è¾‘é”™è¯¯å€™é€‰ç»™ä¸ä¸åŒçš„æƒé‡ã€‚ Improvement to channel model æ›´ä¸°å¯Œçš„ç¼–è¾‘è·ç¦»ï¼Œä¸ä»…ä»…æ˜¯åˆ é™¤æ’å…¥æ›¿æ¢ç­‰ â€¢entâ€“&gt;ant â€¢phâ€“&gt;f â€¢leâ€“&gt;al å°†å‘éŸ³é”™è¯¯è€ƒè™‘åˆ°channelé‡Œé¢ channel model é™¤äº†åŸºäºè¯­æ–™ç»Ÿè®¡å‡ºæ’å…¥åˆ é™¤æ›¿æ¢äº§ç”Ÿçš„channel modelï¼Œè¿˜å¯ä»¥è€ƒè™‘å…¶ä»–å› ç´  â€¢The source letter â€¢The target letter â€¢Surrounding letters â€¢The position in the word â€¢Nearby keys on the keyboard â€¢Homology on the keyboard â€¢Pronunciations â€¢Likely morpheme transformations classifier-based methods for real-word spelling correction é™¤äº†åŸºäºlanguage model ã€channel modelçš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨ç‰¹å¾è¾“å…¥åˆ†ç±»å™¨æ¥è¿›è¡Œæ‹¼å†™é”™è¯¯æ£€æŸ¥ã€‚ æ¯”å¦‚ä¸‹é¢çš„ä¾‹å­ whether/weather åˆ¤æ–­è¿™ä¸ªæ‹¼å†™é”™è¯¯åˆ°åº•æ˜¯å“ªä¸€ä¸ªï¼Œæˆ‘ä»¬è€ƒè™‘ä¸‹é¢è¿™äº›ç‰¹å¾ï¼š é™„è¿‘10ä¸ªè¯å†…æ˜¯å¦æœ‰cloudyï¼Œæœ‰çš„è¯é‚£ä¹ˆå¾ˆå¯èƒ½å°±æ˜¯weatherå¤©æ°” åé¢ æ˜¯ä¸æ˜¯æ¥çš„ to VERBï¼Œæ˜¯çš„è¯é‚£ä¹ˆå¾ˆæœ‰å¯èƒ½å°±æ˜¯whether æ˜¯å¦ åé¢æ¥çš„æ˜¯ä¸æ˜¯ or notï¼Œæ˜¯çš„è¯å¾ˆæœ‰å¯èƒ½å°±æ˜¯or not æ˜¯å¦","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"03-language-modeling","slug":"03-language-modeling","date":"2020-02-14T09:46:04.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/14/03-language-modeling/","link":"","permalink":"https://mz2sj.github.io/2020/02/14/03-language-modeling/","excerpt":"","text":"ä»Šå¤©è¦å­¦ä¹ çš„æ˜¯è¯­è¨€æ¨¡å‹ï¼Œå¥½å¤šä¸œè¥¿å‘€ï¼ æ¦‚å¿µ è€å¸ˆä»‹ç»äº†ä¸€å¤§å †ï¼Œæˆ‘çš„ç†è§£è¯­è¨€æ¨¡å‹å°±æ˜¯å¯¹è¯­è¨€çš„åˆç†æ€§è¿›è¡Œå»ºæ¨¡ã€‚è¿™ç§å»ºæ¨¡å¯ä»¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­çš„å¯èƒ½æ€§ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€å¥è¯åŸºäºä¹‹å‰çš„è¯é¢„æµ‹å‡ºä¸‹ä¸€ä¸ªè¯çš„å¯èƒ½æ€§ã€‚æ•°å­¦å…¬å¼è¡¨è¾¾å¦‚ä¸‹ï¼š P(W)=P(w1,W2,W3,W4,W5â€¦Wn)\\mathrm{P}(\\mathrm{W})=\\mathrm{P}\\left(\\mathrm{w}_{1}, \\mathrm{W}_{2}, \\mathrm{W}_{3}, \\mathrm{W}_{4}, \\mathrm{W}_{5} \\dots \\mathrm{W}_{\\mathrm{n}}\\right)P(W)=P(w1â€‹,W2â€‹,W3â€‹,W4â€‹,W5â€‹â€¦Wnâ€‹) ä¸€ä¸ªå¥å­å‡ºç°çš„æ¦‚ç‡ P(wnâˆ£w1,W2â€¦Wnâˆ’1)\\mathrm{P}\\left(\\mathrm{w}_{\\mathrm{n}} | \\mathrm{w}_{1}, \\mathrm{W}_{2} \\dots \\mathrm{W}_{\\mathrm{n}-1}\\right)P(wnâ€‹âˆ£w1â€‹,W2â€‹â€¦Wnâˆ’1â€‹) åŸºäºä¹‹å‰è¯é¢„æµ‹ä¸‹ä¸€è¯ è¯­è¨€æ¨¡å‹çš„è®¡ç®— é‚£ä¹ˆè¯­è¨€æ¨¡å‹è¯¥å¦‚ä½•è®¡ç®—å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡é“¾å¼æ³•åˆ™æ¥è®¡ç®—ä¸€å¥è¯æ‰€æœ‰å•è¯çš„è”åˆæ¦‚ç‡ P(w1w2â‹¯wn)=âˆiP(wiâˆ£w1w2â‹¯wiâˆ’1)P\\left(w_{1} w_{2} \\cdots w_{n}\\right)=\\prod_{i} P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) P(w1â€‹w2â€‹â‹¯wnâ€‹)=iâˆâ€‹P(wiâ€‹âˆ£w1â€‹w2â€‹â‹¯wiâˆ’1â€‹) ä¸¾ä¸ªä¾‹å­ P(â€²â€²its water is so transparent&quot; â€²â€²)=\\mathrm{P}\\left(^{\\prime \\prime} \\mathrm{its} \\text { water is so transparent&quot; }^{\\prime \\prime}\\right)=P(â€²â€²its water is so transparent&quot; â€²â€²)=P(its)Ã—P( water âˆ£ its )Ã—P( is lits water )\\quad \\mathrm{P}(\\mathrm{its}) \\times \\mathrm{P}(\\text { water } | \\text { its }) \\times \\mathrm{P}(\\text { is lits water })P(its)Ã—P( water âˆ£ its )Ã—P( is lits water )Ã—P( so lits water is )Ã—\\quad \\times \\mathrm{P}(\\text { so lits water is }) \\timesÃ—P( so lits water is )Ã—P(transparent) its water is so))) è¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªå­—éƒ½æ˜¯ä¾èµ–äºä¹‹å‰æ‰€æœ‰çš„å­—ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ä»ç¬¬ä¸€ä¸ªå­—ä¸€ç›´ä¼ é€’åˆ°æœ€åä¸€ä¸ªå­—ï¼Œå½¢æˆå®Œæ•´çš„ä¸€å¥è¯çš„æ¦‚ç‡ã€‚ ä½†æ˜¯éšç€å¥å­é•¿åº¦å¢é•¿ï¼Œæ¡ä»¶æ¦‚ç‡çš„è®¡ç®—å˜å¾—ä¸ç°å® P( the âˆ£ its water is so transparent that )=Count(itswaterissotransparentthatthe)/Count(itswaterissotransparentthat)P(\\text { the } | \\text { its water is so transparent that })= Count (its water is so transparent that the) /Count (its water is so transparent that) P( the âˆ£ its water is so transparent that )=Count(itswaterissotransparentthatthe)/Count(itswaterissotransparentthat) æˆ‘ä»¬é€šè¿‡å¯¹åºåˆ—è®¡æ•°æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ï¼Œå½“å¥å­è¿‡é•¿æ—¶ï¼Œå¥å­åºåˆ—æ•°é‡å¢é•¿ï¼Œæœ‰äº›å¥å­åºåˆ—æˆ‘ä»¬æ ¹æœ¬å°±æ²¡è§è¿‡ï¼Œä¼šå¯¼è‡´åˆ†æ¯éƒ¨åˆ†æ¦‚ç‡ä¸º0ï¼Œå¯¼è‡´æ— æ³•è®¡ç®—ã€‚ç”±æ­¤å¼•å…¥äº†é©¬å°”ç§‘å¤«å‡è®¾ï¼Œå³ä¸€ä¸ªäº‹åŠ¡çš„çŠ¶æ€åªä¸ä¹‹å‰çš„çŠ¶æ€æœ‰å…³ã€‚ æ•´ä¸ªåºåˆ—çš„æ¦‚ç‡å¯ä»¥ç”¨å¦‚ä¸‹å…¬å¼ä¼°è®¡ P(w1w2â‹¯wn)â‰ˆâˆP(wiâˆ£wiâˆ’kâ‹¯wiâˆ’1)P\\left(w_{1} w_{2} \\cdots w_{n}\\right) \\approx \\prod P\\left(w_{i} | w_{i-k} \\cdots w_{i-1}\\right) P(w1â€‹w2â€‹â‹¯wnâ€‹)â‰ˆâˆP(wiâ€‹âˆ£wiâˆ’kâ€‹â‹¯wiâˆ’1â€‹) å•ä¸ªå•è¯çš„æ¡ä»¶æ¦‚ç‡ä¼°è®¡ P(wiâˆ£w1w2â‹¯wiâˆ’1)â‰ˆP(wiâˆ£wiâˆ’kâ‹¯wiâˆ’1)P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) \\approx P\\left(w_{i} | w_{i-k} \\cdots w_{i-1}\\right) P(wiâ€‹âˆ£w1â€‹w2â€‹â‹¯wiâˆ’1â€‹)â‰ˆP(wiâ€‹âˆ£wiâˆ’kâ€‹â‹¯wiâˆ’1â€‹) å½“å½“å‰è¯çš„æ¦‚ç‡åªä¸å½“å‰è¯æœ‰å…³ï¼Œåˆ™æ˜¯Unigram Model P(w1w2â‹¯wn)â‰ˆâˆiP(wi)P\\left(w_{1} w_{2} \\cdots w_{n}\\right) \\approx \\prod_{i} P\\left(w_{i}\\right) P(w1â€‹w2â€‹â‹¯wnâ€‹)â‰ˆiâˆâ€‹P(wiâ€‹) å½“å½“å‰è¯æ¦‚ç‡ä¸å‰ä¸€ä¸ªè¯æœ‰å…³æ—¶ï¼Œåˆ™æ˜¯Bigram Model P(wiâˆ£w1w2â‹¯wiâˆ’1)â‰ˆP(wiâˆ£wiâˆ’1)P\\left(w_{i} | w_{1} w_{2} \\cdots w_{i-1}\\right) \\approx P\\left(w_{i} | w_{i-1}\\right) P(wiâ€‹âˆ£w1â€‹w2â€‹â‹¯wiâˆ’1â€‹)â‰ˆP(wiâ€‹âˆ£wiâˆ’1â€‹) ç”±æ­¤å¯ä»¥ç»§ç»­æ¨å‡º3-gramã€4-gram Modelç­‰n-gram modelæ¨¡å‹ã€‚ n-gramæ¨¡å‹åŸºäºæ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼Œå¿½ç•¥äº†è¯­è¨€ä¹‹é—´çš„ä¾èµ–æ€§ï¼Œä½†æˆ‘ä»¬ä»ä½¿ç”¨å®ƒã€‚ é¢„æµ‹N-gram æ¦‚ç‡ ä¸¾ä¸ªbigramä¾‹å­ P(wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)c(wiâˆ’1)P\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)}{c\\left(w_{i-1}\\right)} P(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)c(wiâˆ’1â€‹,wiâ€‹)â€‹ åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¼šåŠ ä¸Šå¥å­çš„å¼€å§‹&lt;s&gt;å’Œ&lt;/s&gt;ç»“å°¾ç¬¦ã€‚ &lt;s&gt; I am Sam &lt;/s&gt; &lt;s&gt; Sam I am &lt;/s&gt; &lt;s&gt; I do not like green eggs and ham &lt;/s&gt; P(Iâˆ£&lt;s&gt;)=23=.67P( Sam âˆ£&lt;s)=13=.33P(amâˆ£I)=23=.67P(&lt;/s&gt;âˆ£ Sam )=12=0.5P( Sam âˆ£am)=12=.5P(doâˆ£I)=13=.33\\begin{array}{ll} {P(\\mathrm{I} |&lt;\\mathrm{s}&gt;)=\\frac{2}{3}=.67} &amp; {P(\\text { Sam } |&lt;\\mathrm{s})=\\frac{1}{3}=.33 \\quad P(\\mathrm{am} | \\mathrm{I})=\\frac{2}{3}=.67} \\\\ {P(&lt;/ \\mathrm{s}&gt;| \\text { Sam })=\\frac{1}{2}=0.5} &amp; {P(\\text { Sam } | \\mathrm{am})=\\frac{1}{2}=.5 \\quad P(\\mathrm{do} | \\mathrm{I})=\\frac{1}{3}=.33} \\end{array} P(Iâˆ£&lt;s&gt;)=32â€‹=.67P(&lt;/s&gt;âˆ£ Sam )=21â€‹=0.5â€‹P( Sam âˆ£&lt;s)=31â€‹=.33P(amâˆ£I)=32â€‹=.67P( Sam âˆ£am)=21â€‹=.5P(doâˆ£I)=31â€‹=.33â€‹ åŸºäºè¯­æ–™æˆ‘ä»¬å¯ä»¥å½¢æˆå¯¹åº”bigramè¡¨ è¿™ç§æ¦‚ç‡è®¡ç®—å‡ºæ¥çš„æ¦‚ç‡ä¼šå¾ˆå°ï¼Œå¯èƒ½ä¼šå¸¦æ¥underflowé—®é¢˜ï¼Œç”±æ­¤å¼•å‡ºlogå‡½æ•°å°†è¿ä¹˜è½¬åŒ–ä¸ºç´¯åŠ  logâ¡(p1Ã—p2Ã—p3Ã—p4)=logâ¡p1+logâ¡p2+logâ¡p3+logâ¡p4\\log \\left(p_{1} \\times p_{2} \\times p_{3} \\times p_{4}\\right)=\\log p_{1}+\\log p_{2}+\\log p_{3}+\\log p_{4} log(p1â€‹Ã—p2â€‹Ã—p3â€‹Ã—p4â€‹)=logp1â€‹+logp2â€‹+logp3â€‹+logp4â€‹ è¯„ä»·è¯­è¨€æ¨¡å‹ perplexity é™¤äº†å¯ä»¥æ ¹æ®ä»»åŠ¡è¯„ä»·è¯­è¨€æ¨¡å‹å¤–ï¼Œæœ€ç›´æ¥çš„è¯­è¨€æ¨¡å‹è¯„ä»·æ–¹æ³•å°±æ˜¯perplexity ä¸€ä¸ªå¥½çš„è¯­è¨€æ¨¡å‹åº”è¯¥èƒ½ç»™çœŸå®çš„å¥å­æ›´é«˜çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥æ®æ­¤æ¥è¯„ä»·è¯­è¨€æ¨¡å‹ã€‚å°†æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ¦‚ç‡çš„å€’æ•°næ¬¡å¹‚åä½œä¸ºperplexityï¼Œbigramså…¬å¼å¦‚ä¸‹ PPâ¡(W)=âˆi=1N1P(wiâˆ£wiâˆ’1)N\\operatorname{PP}(W)=\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P\\left(w_{i} | w_{i-1}\\right)}} PP(W)=Ni=1âˆNâ€‹P(wiâ€‹âˆ£wiâˆ’1â€‹)1â€‹â€‹ å›°æƒ‘åº¦è¶Šä½ä»£è¡¨æ¨¡å‹è¶Šå¥½ Smoothing å¹³æ»‘ å½“è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¾ˆç›¸ä¼¼çš„æ—¶å€™ï¼Œn-gramsæ¨¡å‹å¯ä»¥å–å¾—å¾ˆå¥½æ•ˆæœã€‚ä½†æ˜¯ç°å®ä¸­ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†å¸¸æœ‰å·®åˆ«ï¼Œå¯¼è‡´åœ¨æµ‹è¯•é›†ä¸­å‡ºç°è®¸å¤šåœ¨è®­ç»ƒé›†ä¸­æ²¡è§è¿‡çš„ä¸œè¥¿ã€‚æ²¡è§è¿‡çš„ä¸œè¥¿æ¦‚ç‡æ˜¯0ï¼Œé‚£ä¹ˆæµ‹è¯•é›†ä¸­åŸæœ¬æ˜¯æ­£ç¡®çš„å¥å­å¯èƒ½è¢«é¢„æµ‹ä¸ºé”™è¯¯ï¼Œè€Œä¸”æ¦‚ç‡ä¸º0ï¼Œå¯¼è‡´æ— æ³•è®¡ç®—å›°æƒ‘åº¦ã€‚ åŠ«å¯Œæµè´« å°†ä¸€äº›æ¦‚ç‡è¾ƒé«˜çš„gramåˆ†é…ç»™ä¸€äº›æœªå‡ºç°è¿‡çš„gram Add-one estimation å¯¹æ‰€æœ‰è¯éƒ½åŠ ä¸€ï¼Œé‚£ä¹ˆåˆ†æ¯å°±è¦æœºä¸Šè¯è¡¨å¤§å°V PAdd-1(wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)+1c(wiâˆ’1)+VP_{\\text {Add-1}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+1}{c\\left(w_{i-1}\\right)+V} PAdd-1â€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)+Vc(wiâˆ’1â€‹,wiâ€‹)+1â€‹ Add-oneæ–¹æ³•æ˜¾å¾—ååˆ†ç²—ç³™ï¼Œæ‰€ä»¥å¹¶æ²¡æœ‰ç”¨åœ¨N-gramsä¸­ï¼Œä½†åœ¨æ–‡æœ¬åˆ†ç±»å’Œ0çš„æ•°ç›®ä¸æ˜¯å¾ˆå¤šçš„ä»»åŠ¡ä¸­ä¼šç”¨åˆ°ã€‚ ä¹Ÿå¯ä»¥ä¸åŠ 1ï¼ŒåŠ kï¼Œç”±æ­¤å¼•å‡ºä¸‹åˆ—æ¨¡å‹ PAdd âˆ’k(wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)+kc(wiâˆ’1)+kVPAdd-k(wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)+m(1V)c(wiâˆ’1)+m\\begin{aligned} &amp;P_{\\text {Add }-k}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+k}{c\\left(w_{i-1}\\right)+k V}\\\\ &amp;P_{\\text {Add-k}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m\\left(\\frac{1}{V}\\right)}{c\\left(w_{i-1}\\right)+m} \\end{aligned} â€‹PAdd âˆ’kâ€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)+kVc(wiâˆ’1â€‹,wiâ€‹)+kâ€‹PAdd-kâ€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)+mc(wiâˆ’1â€‹,wiâ€‹)+m(V1â€‹)â€‹â€‹ ä¿è¯åˆ†å­åŠ ä¸Šçš„å’Œå’Œåˆ†æ¯åŠ ä¸Šçš„ç›¸ç­‰å³å¯ï¼Œä¹Ÿå¯åŠ ä¸Šå½“å‰è¯çš„æ¦‚ç‡ PAdd-k(wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)+m(1V)c(wiâˆ’1)+mPUnigramPrior (wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)+mP(wi)c(wiâˆ’1)+m\\begin{aligned} &amp;P_{\\text {Add-k}}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m\\left(\\frac{1}{V}\\right)}{c\\left(w_{i-1}\\right)+m}\\\\ &amp;P_{\\text {UnigramPrior }}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)+m P\\left(w_{i}\\right)}{c\\left(w_{i-1}\\right)+m} \\end{aligned} â€‹PAdd-kâ€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)+mc(wiâˆ’1â€‹,wiâ€‹)+m(V1â€‹)â€‹PUnigramPrior â€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)+mc(wiâˆ’1â€‹,wiâ€‹)+mP(wiâ€‹)â€‹â€‹ æ¨¡å‹çš„èåˆ Backoff å›é€€ è¯•ä¸€è¯•é‚£ç§è¯­è¨€æ¨¡å‹å¥½ï¼Œè°å¥½ç”¨è°ã€‚ä¾‹å¦‚è®¡ç®—trigramæ—¶ï¼Œé¢„æ–™ä¸­æ²¡æœ‰è¿™ä¸ªtrigramï¼Œå°±ç”¨bigramæ¥ä»£æ›¿ï¼Œbigramæ²¡æœ‰å°±ç”¨unigramä»£æ›¿ã€‚ Interpolation æ’å€¼ å°† unigramï¼Œbigramï¼Œtrigramæ··ç”¨ï¼Œé€šå¸¸ä¼šè¡¨ç°çš„æ›´å¥½ã€‚ ç®€å•çš„interpolation P^(wnâˆ£wnâˆ’1wnâˆ’2)=Î»1P(wnâˆ£wnâˆ’1wnâˆ’2)+Î»2P(wnâˆ£wnâˆ’1)+Î»3P(wn)\\begin{aligned} \\hat{P}\\left(w_{n} | w_{n-1} w_{n-2}\\right)=&amp; \\lambda_{1} P\\left(w_{n} | w_{n-1} w_{n-2}\\right) \\\\ &amp;+\\lambda_{2} P\\left(w_{n} | w_{n-1}\\right) \\\\ &amp;+\\lambda_{3} P\\left(w_{n}\\right) \\end{aligned} P^(wnâ€‹âˆ£wnâˆ’1â€‹wnâˆ’2â€‹)=â€‹Î»1â€‹P(wnâ€‹âˆ£wnâˆ’1â€‹wnâˆ’2â€‹)+Î»2â€‹P(wnâ€‹âˆ£wnâˆ’1â€‹)+Î»3â€‹P(wnâ€‹)â€‹ âˆ‘iÎ»i=1\\sum_{i} \\lambda_{i}=1 iâˆ‘â€‹Î»iâ€‹=1 åŸºäºä¸Šæ–‡çš„interpolation P^(wnâˆ£wnâˆ’2wnâˆ’1)=Î»1(wnâˆ’2nâˆ’1)P(wnâˆ£wnâˆ’2wnâˆ’1)+Î»2(wnâˆ’2nâˆ’1)P(wnâˆ£wnâˆ’1)+Î»3(wnâˆ’2nâˆ’1)P(wn)\\begin{aligned} \\hat{P}\\left(w_{n} | w_{n-2} w_{n-1}\\right)=&amp; \\lambda_{1}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n} | w_{n-2} w_{n-1}\\right) \\\\ &amp;+\\lambda_{2}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n} | w_{n-1}\\right) \\\\ &amp;+\\lambda_{3}\\left(w_{n-2}^{n-1}\\right) P\\left(w_{n}\\right) \\end{aligned} P^(wnâ€‹âˆ£wnâˆ’2â€‹wnâˆ’1â€‹)=â€‹Î»1â€‹(wnâˆ’2nâˆ’1â€‹)P(wnâ€‹âˆ£wnâˆ’2â€‹wnâˆ’1â€‹)+Î»2â€‹(wnâˆ’2nâˆ’1â€‹)P(wnâ€‹âˆ£wnâˆ’1â€‹)+Î»3â€‹(wnâˆ’2nâˆ’1â€‹)P(wnâ€‹)â€‹ æˆ‘ä»¬è¦æ‰¾åˆ°èƒ½ä½¿æ¦‚ç‡æœ€å¤§çš„lambda logâ¡P(w1â€¦wnâˆ£M(Î»1â€¦Î»k))=âˆ‘ilogâ¡PM(Î»1â€¦Î»k)(wiâˆ£wiâˆ’1)\\log P\\left(w_{1} \\ldots w_{n} | M\\left(\\lambda_{1} \\ldots \\lambda_{k}\\right)\\right)=\\sum_{i} \\log P_{M\\left(\\lambda_{1} \\ldots \\lambda_{k}\\right)}\\left(w_{i} | w_{i-1}\\right) logP(w1â€‹â€¦wnâ€‹âˆ£M(Î»1â€‹â€¦Î»kâ€‹))=iâˆ‘â€‹logPM(Î»1â€‹â€¦Î»kâ€‹)â€‹(wiâ€‹âˆ£wiâˆ’1â€‹) å¯¹äºå¤§è§„æ¨¡çš„n-gramæ¨¡å‹ï¼Œå¦‚æœæŸä¸ªè¯åºåˆ—ä¸ªæ•°ä¸º0ï¼Œå¯¼è‡´n-gramä¸º0ï¼Œåˆ™å¯ä»¥é™ä½n-gramé˜¶æ•°ï¼Œè®¡ç®—ä¹‹å‰çš„n-gramæ¦‚ç‡ã€‚ S(wiâˆ£wiâˆ’k+1iâˆ’1)={countâ¡(wiâˆ’k+1i)countâ¡(wiâˆ’k+1iâˆ’1) if countâ¡(wiâˆ’k+1i)&gt;00.4S(wiâˆ£wiâˆ’k+2iâˆ’1) otherwise S\\left(w_{i} | w_{i-k+1}^{i-1}\\right)=\\left\\{\\begin{array}{cc} {\\frac{\\operatorname{count}\\left(w_{i-k+1}^{i}\\right)}{\\operatorname{count}\\left(w_{i-k+1}^{i-1}\\right)}} &amp; {\\text { if } \\operatorname{count}\\left(w_{i-k+1}^{i}\\right)&gt;0} \\\\ {0.4 S\\left(w_{i} | w_{i-k+2}^{i-1}\\right)} &amp; {\\text { otherwise }} \\end{array}\\right. S(wiâ€‹âˆ£wiâˆ’k+1iâˆ’1â€‹)=â©â¨â§â€‹count(wiâˆ’k+1iâˆ’1â€‹)count(wiâˆ’k+1iâ€‹)â€‹0.4S(wiâ€‹âˆ£wiâˆ’k+2iâˆ’1â€‹)â€‹ if count(wiâˆ’k+1iâ€‹)&gt;0 otherwise â€‹ é«˜çº§å¹³æ»‘ Absolute Discounting Interpolation PAbsoluteDiscounting (wiâˆ£wiâˆ’1)=c(wiâˆ’1,wi)âˆ’dc(ui)+Î»(wdiâˆ’1)P(w)P_{\\text {AbsoluteDiscounting }}\\left(w_{i} | w_{i-1}\\right)=\\frac{c\\left(w_{i-1}, w_{i}\\right)-d}{c\\left(u_{i}\\right)}+\\lambda\\left(\\stackrel{d}{w}_{i-1}\\right) P(w) PAbsoluteDiscounting â€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(uiâ€‹)c(wiâˆ’1â€‹,wiâ€‹)âˆ’dâ€‹+Î»(wdiâˆ’1â€‹)P(w) å°†æˆ‘ä»¬é¢‘ç‡å‡ºç°è¾ƒé«˜çš„è¯å‡å»ä¸€éƒ¨åˆ†ï¼Œåˆ†ç»™è¾ƒä½çš„ä¸€éƒ¨åˆ†ã€‚dé€šå¸¸å–0.75æˆ–è€…æ¯”è¾ƒå°çš„æ•° Kneser-Ney Smotting I used to eat Chinese food with ______ instead of knife and fork. å½“æˆ‘ä»¬åœ¨ä¸€å¥è¯ä¸­é¢„æµ‹æœªçŸ¥è¯ï¼Œæˆ‘ä»¬é™¤äº†è¦è€ƒè™‘è¯çš„unigramï¼Œè¿˜è¦è€ƒè™‘è¿™ä¸ªè¯çš„bigramï¼Œå®ƒæ˜¯å¦é€‚åˆå’Œåˆ«çš„è¯ç»„æˆè¯ï¼Œå³ä¸€ç§è¿ç»­æ€§ã€‚ PCONTTNUATION (w)=âˆ£{wiâˆ’1:c(wiâˆ’1,w)&gt;0}âˆ£âˆ£{(wjâˆ’1,wj):c(wjâˆ’1,wj)&gt;0}P_{\\text {CONTTNUATION }}(w)=\\frac{\\left|\\left\\{w_{i-1}: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right|}{|\\left\\{\\left(w_{j-1}, w_{j}\\right): c\\left(w_{j-1}, w_{j}\\right)&gt;0\\right\\}} PCONTTNUATION â€‹(w)=âˆ£{(wjâˆ’1â€‹,wjâ€‹):c(wjâˆ’1â€‹,wjâ€‹)&gt;0}âˆ£{wiâˆ’1â€‹:c(wiâˆ’1â€‹,w)&gt;0}âˆ£â€‹ ä¸Šé¢æ—¶wè¿™ä¸ªè¯åœ¨è¯­æ–™ä¸­äº§ç”Ÿçš„bigramæ•°é‡ï¼Œä¸‹é¢æ˜¯è¯­æ–™ä¸­æ‰€æœ‰è¯çš„bigramæ•°é‡ PCONTTNUATON (w)=âˆ£{wiâˆ’1:c(wiâˆ’1,w)&gt;0}âˆ£âˆ‘wâ€²âˆ£{wiâˆ’1â€²:c(wiâˆ’1â€²,wâ€²)&gt;0}âˆ£P_{\\text {CONTTNUATON }}(w)=\\frac{\\left|\\left\\{w_{i-1}: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right|}{\\sum_{w^{\\prime}}\\left|\\left\\{w_{i-1}^{\\prime}: c\\left(w_{i-1}^{\\prime}, w^{\\prime}\\right)&gt;0\\right\\}\\right|} PCONTTNUATON â€‹(w)=âˆ‘wâ€²â€‹âˆ£âˆ£â€‹{wiâˆ’1â€²â€‹:c(wiâˆ’1â€²â€‹,wâ€²)&gt;0}âˆ£âˆ£â€‹âˆ£{wiâˆ’1â€‹:c(wiâˆ’1â€‹,w)&gt;0}âˆ£â€‹ å°†PCONTINUATION (w)P_{\\text {CONTINUATION }}(w)PCONTINUATION â€‹(w)å¼•å…¥ä¹‹å‰å®šä¹‰çš„abosolute discounting PKN(wiâˆ£wiâˆ’1)=maxâ¡(c(wiâˆ’1,wi)âˆ’d,0)c(wiâˆ’1)+Î»(wiâˆ’1)PCONTINUATION(wi)P_{K N}\\left(w_{i} | w_{i-1}\\right)=\\frac{\\max \\left(c\\left(w_{i-1}, w_{i}\\right)-d, 0\\right)}{c\\left(w_{i-1}\\right)}+\\lambda\\left(w_{i-1}\\right) P_{\\text {CONTINUATION}}\\left(w_{i}\\right) PKNâ€‹(wiâ€‹âˆ£wiâˆ’1â€‹)=c(wiâˆ’1â€‹)max(c(wiâˆ’1â€‹,wiâ€‹)âˆ’d,0)â€‹+Î»(wiâˆ’1â€‹)PCONTINUATIONâ€‹(wiâ€‹) maxæ˜¯ä¸ºäº†ä½¿åˆ†å­éƒ½å¤§äº0 Î»(wiâˆ’1)=dc(wiâˆ’1)âˆ£{w:c(wiâˆ’1,w)&gt;0}âˆ£\\lambda\\left(w_{i-1}\\right)=\\frac{d}{c\\left(w_{i-1}\\right)}\\left|\\left\\{w: c\\left(w_{i-1}, w\\right)&gt;0\\right\\}\\right| Î»(wiâˆ’1â€‹)=c(wiâˆ’1â€‹)dâ€‹âˆ£{w:c(wiâˆ’1â€‹,w)&gt;0}âˆ£ Î»\\lambdaÎ»å–å€¼ï¼Œåªæœ‰å½“c(wiâˆ’1,wi)c\\left(w_{i-1}, w_{i}\\right)c(wiâˆ’1â€‹,wiâ€‹)&gt;0æ—¶æ‰ä¼šå‡å»dï¼Œè¿™é‡Œåˆä¹˜ä¸Šå‡å»dçš„æ¬¡æ•°ã€‚ å¹³æ»‘ç®—æ³•å¤ªå¤æ‚å•¦ï¼Œä»Šå¤©å…ˆåˆ°è¿™å„¿ï¼Œæˆ‘è¿˜è¦å†ç†è§£ç†è§£ã€‚","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"02 basic text processing","slug":"02-basic-text-processing","date":"2020-02-12T12:45:27.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/12/02-basic-text-processing/","link":"","permalink":"https://mz2sj.github.io/2020/02/12/02-basic-text-processing/","excerpt":"","text":"ä»Šå¤©å­¦ä¹ æ­£åˆ™è¡¨è¾¾å¼çš„ä½¿ç”¨ï¼Œåé¢è¿˜æœ‰ä¸€ä¸ªå°ä½œä¸šè¦åšï¼Œå¼€å§‹å•¦ï¼ æ­£åˆ™è¡¨è¾¾å¼ Disjunctions æå– æˆ‘ä¹Ÿä¸çŸ¥é“ä¸­æ–‡è¯¥æ€ä¹ˆè¯´ï¼Œæš‚å®šæå–å§ã€‚disjunctionç”¨ [] è¡¨ç¤ºï¼Œè¡¨ç¤ºé€‰å–æ‹¬å·ä¸­è¡¨ç¤ºçš„ä¸€ä¸ªå­—ç¬¦ã€‚ä¸‹é¢ä¸¾ä¸ªä¾‹å­ Pattern Matches [wW]oodchunk Woodchunk,woodchunk [123456789] any digit(ç”±äºåŒ¹é…æ˜¯è´ªå¿ƒçš„ï¼Œæ‰€ä»¥åªè¦æ˜¯æ•°å­—éƒ½ä¼šåŒ¹é…) åƒæ•°å­—æ¯æ¬¡éƒ½è¦æŠŠåä¸ªé˜¿æ‹‰ä¼¯æ•°å­—éƒ½å†™å‡ºæ¥å¤ªéº»çƒ¦äº†ï¼Œæ‰€ä»¥æœ‰äº†rangeç”¨æ³• Pattern Matches [A-Z] An uppeer case letter [a-z] A lowwer case letter [0-9] A single digit Negation in Disjunction æå–å–å ç¿»è¯‘å¿½ç•¥ï¼Œ ^ caratå°±æ˜¯å–åçš„æ„æ€ï¼Œåœ¨disjunctionä¸­ç»å¸¸ä¼šç”¨åˆ° ^ åªæœ‰åœ¨æå– [] ä¸­ä½œä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦æ‰è¡¨ç¤ºå–åï¼Œä¹Ÿå°±æ˜¯è¯´**^** ä¸ [] å¸¸å¸¸è¿ç”¨ Pattern Matcher Examples [^A-Z] Not an upper case letter Hello World [^Ss] Neither S nor s Say [e] Neither e nor ^ ^eergg a^b The pattern a carat b,as a signal tobe matched a^b More Disjunction å–æˆ– ç”¨ **|**æ¥è¡¨ç¤ºï¼Œè¡¨ç¤ºå–æˆ–çš„æ„æ€ï¼Œæˆ–è€…ç†è§£æˆlinuxçš„ç®¡é“ï¼Ÿåœ¨å‰é¢çš„åŸºç¡€ä¸Šç»§ç»­è¿›è¡Œè¿ç®—ã€‚ Pattern Matches hello|world åŒ¹é…helloæˆ–è€…world a|b|c ç­‰åŒäº[abc] [Hh]ello|[Ww]orld åŒ¹é…å¤§å°å†™å¼€å¤´çš„helloæˆ–è€…world å€¼å¾—æ³¨æ„çš„æ˜¯ | åˆ¤æ–­çš„æ˜¯ç¬¦å·ä¹‹å‰æˆ–ä¹‹åçš„è¿ç»­å­—ç¬¦æ˜¯å¦åŒ¹é…ï¼Œè€Œä¸æ˜¯åªçœ‹å‰é¢è‹¥å¹²ä¸ªå­—ç¬¦åŒ¹é…æƒ…å†µã€‚ ï¼Ÿ * +. Patter Matches Instances colou?r optional previous char,?å·å‰é¢çš„å­—ç¬¦å¯æœ‰å¯æ—  color colour oo*h! 0 or more of previous char,0æ¬¡æˆ–è€…æ›´å¤šæ¬¡ oh!,ooh!,oooh! o+h! 1 or more of previous char,1æ¬¡æˆ–è€…æ›´å¤šæ¬¡ oh!,ooh! baa+ baa,baaa beg.n 1 char,å•ä¸ªå­—ç¬¦ï¼Œå¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ beggnï¼Œbeg1nï¼Œbeg2n Anchors ^$ é”š è¯´ç™½äº†ï¼Œå°±æ˜¯è¡¨æ˜ä½ç½®ã€‚^ åçš„å­—ç¬¦è¡¨ç¤ºè¯¥å­—ç¬¦åœ¨åºåˆ—å¼€å§‹å¤„ï¼Œ$ å‰çš„å­—ç¬¦è¡¨ç¤ºè¯¥å­—ç¬¦åœ¨ç»“å°¾å¤„ã€‚ Pattern Matches ^[A-Z] Palo Alto åªåŒ¹é…å‰é¢çš„ä¸€ä¸ªå¤§å†™P ^[^A-Za-z] **1 â€œ**Helloâ€ å®éªŒè¯æ˜åªèƒ½åŒ¹é…ä¸€ä¸ªå­—ç¬¦å¹¶ä¸èƒ½åŒ¹é…å¼•å·ï¼Œpptä¸Šæœ‰è¯¯ \\.$ The end. ä»¥.ç»“å°¾ .$ åŒ¹é…ç»“å°¾çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ The end**ï¼Ÿ** The end**ï¼** æ–‡æœ¬åºåˆ—åŒ– Text Normalization æ–‡æœ¬è§„èŒƒåŒ– å¼•å‡ºå‡ ä¸ªæ¦‚å¿µ lemma è¯æ ¹ï¼šcat å’Œ catsçš„è¯æ ¹æ˜¯ç›¸åŒçš„ wordform è¯å½¢ï¼šcat å’Œ catsçš„è¯å½¢å¼ä¸åŒçš„ å¯¹äºè¯çš„ç±»å‹å’Œæ•°ç›®ä¹Ÿæœ‰è®²ç©¶ï¼Œæ¯”å¦‚ä¸€å¥ç®€å•çš„è‹±è¯­ they lay back on the San Fancisco grass and looked at the stars and their å…³äºè¯çš„æ•°ç›®ï¼ŒSan Fanciscoæ˜¯ç®—ä¸€ä¸ªè¯è¿˜æ˜¯ä¸¤ä¸ªè¯å‘¢ å…³äºè¯çš„ç±»å‹ï¼Œtheyå’Œtheirç®—ä¸ç®—åŒä¸€ç±»è¯å‘¢ï¼Ÿ éƒ½æ˜¯é—®é¢˜ æˆ‘ä»¬é€šäº§ç”¨Nè¡¨ç¤ºtokençš„æ•°ç›®ï¼Œç”¨Vè¡¨ç¤ºè¯å…¸è¯çš„å¤§å°ï¼Œä¹Ÿå°±æ˜¯è¯ç±»å‹æ•°ç›® ä¸åŒçš„è¯­è¨€åœ¨tokençš„è¿‡ç¨‹ä¸­ä¼šé‡åˆ°ä¸åŒçš„é—®é¢˜ï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€ä¾‹ä¸¾äº†ã€‚ è¯¾ç¨‹ä¸­è¿˜ä¸“é—¨ä»‹ç»äº†ä¸­æ–‡åˆ†è¯æ–¹æ³•ï¼Œæœ€å¤§æ­£å‘åŒ¹é…æ³•ã€‚æœ€å¤§æ­£å‘åŒ¹é…æ³•çš„æ€æƒ³å¾ˆç®€å•ï¼Œå°±æ˜¯æ‹¿è¯å…¸ä¸­æœ€é•¿çš„è¯å»éå†è¯­å¥ï¼ŒåŒ¹é…æˆåŠŸåˆ™åˆ†è¯ï¼Œä¸æˆåŠŸåˆ™æ¢é•¿åº¦æ›´çŸ­çš„è¯ç»§ç»­åŒ¹é…ï¼Œç›´è‡³åŒ¹é…ç»“æŸã€‚ ä¸‹é¢ä»‹ç»é›†ä¸­å¸¸è§çš„æ–‡æœ¬è§„èŒƒåŒ–æ–¹æ³•ã€‚ Word Normalization Case folding å¤§å°å†™è½¬æ¢ï¼Œæ–‡æœ¬å¤„ç†ä¸­æˆ‘ä»¬é€šå¸¸ä¼šå°†å¤§å†™è½¬åŒ–ä¸ºå°å†™ï¼Œä½†æ˜¯è¿™ä¹Ÿä¼šå¸¦æ¥ä¸€äº›é—®é¢˜ã€‚ä¾‹å¦‚æœºå™¨ç¿»è¯‘ä¸­ï¼Œå¤§å†™USå’Œå°å†™uså®Œå…¨ä¸æ˜¯ä¸€ä¸ªæ„æ€ã€‚ä½†æ˜¯åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼Œæˆ‘ä»¬ä¹ æƒ¯å°å†™ï¼Œå¦‚æœå‡ºç°äº†å¤§å†™ä¹Ÿä¼šåŒæ—¶æ£€ç´¢å°å†™çš„ç»“æœã€‚ Lemmatization è¯æ€§è¿˜åŸï¼Œæ ¹æ®è¯å…¸æ‰¾åˆ°è¯åˆå§‹çš„è¯å½¢å¼ï¼Œæ¯”å¦‚å•å¤æ•°ï¼Œæ‰€æœ‰æ ¼ï¼Œæ—¶æ€çš„è¿˜åŸ amï¼Œareï¼Œis --&gt; be Morphology å½¢æ€è¿˜åŸï¼Œå°†è¯åˆ†ä¸ºè¯å¹²å’Œè¯ç¼€ï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ä¸­å°±å¸¸ä¼šç”¨åˆ°Stemmingã€‚ä¿ç•™è¯çš„è¯å¹²ç”¨æ¥æ£€ç´¢ï¼Œ ä¾‹å¦‚compressedå’Œcompressionéƒ½èƒ½é€šè¿‡å®ƒä»¬çš„è¯å¹²compressæ£€ç´¢åˆ°ã€‚ è‹±è¯­ä¸­ä¼šæœ‰ä¸“é—¨çš„è§„åˆ™ç”¨æ¥è·å–è¯çš„è¯å¹²ï¼Œæ¯”å¦‚ä¸‹é¢è¿™æ¡ã€‚ ï¼ˆ*v*)ing å»æ‰ingåçš„è¯å¹²ï¼Œè¯å¹²ä¸­å«æœ‰å…ƒéŸ³ walking --&gt;walk aæ˜¯å…ƒéŸ³ sing --&gt;sing sä¸æ˜¯å…ƒéŸ³ï¼Œæ‰€ä»¥è¯å¹²ä¸æ˜¯s ç§€ä¸€æ³¢æ­£åˆ™è¡¨è¾¾å¼ï¼Œ.*[aoeiu].*ing$ï¼Œå°±æ˜¯ä¸Šé¢æˆ‘ä»¬è¯´çš„åŒ¹é…æ‰€æœ‰ç±»ä¼¼äºwalkingè¿™æ ·çš„è¯ Sentence Segmentation and Decision Trees å¥å­çš„åˆ’åˆ†æœ‰æ—¶å€™ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è‹±è¯­ä¸­çš„å¥å·.æœ‰æ—¶ä¼šè¢«å½“ä½œå…¶ä»–ç”¨é€”ï¼Œå¹¶ä¸æ˜¯çœŸæ­£è¡¨ç¤ºä¸€å¥è¯ç»“æŸï¼Œä¾‹å¦‚%0.2ä¸­çš„.å°±ä¸æ˜¯è¡¨ç¤ºå¥å­ç»“æŸ,æ‰€ä»¥æˆ‘ä»¬è¦å»ºç«‹ä¸€ä¸ªåˆ†ç±»å™¨æ¥åˆ¤æ–­å“ªé‡Œæ˜¯å¥å­çš„å¼€å§‹å“ªé‡Œå¥å­ç»“æŸäº†. ä¾‹å¦‚å†³ç­–æ ‘æ¥åˆ¤æ–­å¥å­æ˜¯å¦ç»“æŸ. æˆ‘ä»¬éœ€è¦æ‰‹å·¥é€‰å–è®¸å¤šç‰¹å¾è¾“å…¥å†³ç­–æ ‘è®©å†³ç­–æ ‘åˆ¤æ–­","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"},{"name":"æ­£åˆ™è¡¨è¾¾å¼","slug":"æ­£åˆ™è¡¨è¾¾å¼","permalink":"https://mz2sj.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"01 introduction to nlp","slug":"01-introduction-to-nlp","date":"2020-02-12T08:10:18.000Z","updated":"2023-01-08T06:30:22.489Z","comments":true,"path":"2020/02/12/01-introduction-to-nlp/","link":"","permalink":"https://mz2sj.github.io/2020/02/12/01-introduction-to-nlp/","excerpt":"","text":"å†™åœ¨å‰é¢ ç ”ç©¶ç”Ÿè¯»äº†åŠå¹´äº†ï¼Œè¶Šæ¥è¶Šè¿·èŒ«äº†ï¼Œæ²¡æœ‰è®¤è®¤çœŸçœŸåšå¥½ä¸€ä»¶äº‹ã€‚ä»Šå¤©ï¼Œç«‹å¸–ä¸ºè¯ï¼Œæˆ‘ä¸€å®šè¦å°†è¿™é—¨è¯¾è®¤çœŸä¿®å®Œï¼Œå‹¤åšç¬”è®°ï¼Œå®Œæˆä½œä¸šã€‚è¯ä¸å¤šè¯´ï¼Œå¼€å§‹å§ï¼é™„ä¸Šè¯¾ç¨‹é“¾æ¥ã€‚ nlpçš„åº”ç”¨é¢†åŸŸ Question Answering é—®ç­” Information Extraction ä¿¡æ¯æŠ½å– Sentiment Analysis æƒ…æ„Ÿåˆ†æ æœºå™¨ç¿»è¯‘ è‡ªç„¶è¯­è¨€åœ¨åƒåœ¾é‚®ä»¶ä¾¦æµ‹ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ä¸Šå·²ç»å–å¾—äº†è¾ƒå¥½çš„è¡¨ç°ï¼Œåœ¨æƒ…æ„Ÿåˆ†ç±»ã€æŒ‡ä»£æ¶ˆè§£ã€å¥æ³•åˆ†æã€æœºå™¨ç¿»è¯‘ã€ä¿¡æ¯æŠ½å–ä¹Ÿæœ‰æ‰€è¿›æ­¥ï¼Œå°¤å…¶æ˜¯ç°åœ¨çš„ç¥ç»æœºå™¨ç¿»è¯‘ï¼Œæ¯”ä¹‹å‰çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘å–å¾—äº†é•¿è¶³çš„è¿›æ­¥ï¼Œä¸è¿‡è¿™é—¨è¯¾å¼€è®¾æ—¶é—´å·²ç»æ¯”è¾ƒæ—©äº†ï¼Œé‚£æ—¶å€™æ·±åº¦å­¦ä¹ è¿˜æ²¡æœ‰å¤§è§„æ¨¡åº”ç”¨åˆ°nlpä¸­ã€‚ä½†åœ¨é—®ç­”ã€æ‘˜è¦ã€å¯¹è¯ç³»ç»Ÿã€æ”¹è¿°ï¼ˆParaphraseï¼‰ç­‰æ–¹é¢è¡¨ç°è¿˜æœ‰å¾…æé«˜ã€‚ æ­§ä¹‰è®©nlpå˜å¾—æ›´åŠ å›°éš¾ ä¸¾ä¸ªä¾‹å­ï¼Œè¿™æ˜¯çº½çº¦æ—¶æŠ¥çš„ä¸€æ¡æ–°é—»å¤´æ¡ Fed raises interest rates æ—¢å¯ä»¥ç†è§£ä¸ºFedè¿™ä¸ªäººå°†åˆ©ç‡interest rates æé«˜äº†ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºFedè¿™ä¸ªäººå¯¹ratesè¿™ä¸ªä¸œè¥¿èµ·äº†å…´è¶£raise interestã€‚ å½“ç»™å‡ºæ›´å¤šä¿¡æ¯æ—¶ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ¶ˆé™¤è¿™ç§æ¨¡ç³Šæ€§ã€‚ Fed raises interest rates 0.5% è¿™æ—¶å€™æˆ‘ä»¬å°±çŸ¥é“äº†æ–°é—»çœŸå®æƒ³è¡¨è¾¾çš„æ„æ€æ˜¯åˆ©ç‡æé«˜äº†%0.5ï¼Œè€Œä¸æ˜¯Fedæ¥äº†å…´è¶£. å…¶ä»–çš„å›°éš¾ éæ ‡å‡†è‹±è¯­ æ¯”å¦‚æ¨ç‰¹ä¸Šçš„ç”¨è¯­å¯èƒ½å°±å¾ˆä¸è§„èŒƒ åˆ‡åˆ†é—®é¢˜ ä¸€ä¸ªè¯ä¸åŒçš„åˆ‡åˆ†ä¼šå¸¦æ¥ä¸åŒçš„é—®é¢˜ ä¿šè¯­ ç±»ä¼¼äºæˆ‘ä»¬çš„æˆè¯­,ä¸èƒ½é€šè¿‡å­—é¢æ„æ€æ¥åˆ¤æ–­ä»–çš„æ„æ€ æ–°è¯ ç±»ä¼¼äºç»™åŠ›è¿™ç§æ–°è¯æ±‡ å¤–éƒ¨çŸ¥è¯† çŸ¥è¯†çš„ä¸ç¡®å®šæ€§ å®ä½“å æœ‰äº›è¯å¯èƒ½æ˜¯ä¸“æœ‰åè¯,ä¸èƒ½æŒ‰å­—é¢æ„æ€ç†è§£ å†™åœ¨åé¢ ç¬¬ä¸€ç¯‡å·®ä¸å¤šå°±åˆ°è¿™å„¿äº†,å®Œäº‹å¼€å¤´éš¾,åŠ æ²¹å†™ä¸‹å»å“¦!","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/categories/nlp/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://mz2sj.github.io/tags/nlp/"}]},{"title":"ä½¿ç”¨torchtextå¤„ç†ä¸­æ–‡æ–‡æœ¬","slug":"ä½¿ç”¨torchtextå¤„ç†ä¸­æ–‡æ–‡æœ¬","date":"2020-01-08T11:45:08.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/08/ä½¿ç”¨torchtextå¤„ç†ä¸­æ–‡æ–‡æœ¬/","link":"","permalink":"https://mz2sj.github.io/2020/01/08/%E4%BD%BF%E7%94%A8torchtext%E5%A4%84%E7%90%86%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC/","excerpt":"","text":"torchtextæ˜¯pytorchä¸­ä¸“é—¨ç”¨æ¥å¤„ç†æ–‡æœ¬çš„åŒ…ï¼Œä½¿ç”¨torchtextå¯ä»¥çµæ´»çš„å¸®æˆ‘ä»¬ç”Ÿæˆbatchå’Œè¯å‘é‡ã€‚ç½‘ä¸Šä»‹ç»çš„å¤§å¤šæ˜¯åŸºäºè‹±æ–‡çš„ç‰ˆæœ¬ï¼Œè‡ªå·±åœ¨ä¸­æ–‡æ–‡æœ¬ä¸Šè¿›è¡Œäº†å°è¯•ï¼Œå°†ä¹‹å‰çš„è‹±æ–‡æ–‡æœ¬åˆ†ç±»æ”¹æˆä¸­æ–‡æ–‡æœ¬åˆ†ç±»ã€‚ä¸‹é¢å¼€å§‹ä»‹ç»ã€‚ æ•°æ®æ ¼å¼ text label åˆè¦å†™åšå®¢äº†ï¼Œå¥½å¼€å¿ƒå•Š 0 ç¨‹åºå‡ºbugäº†ï¼Œå”‰ 1 æ•°æ®æ ¼å¼å¯ä»¥æ˜¯csvã€tsvã€jsonæ–‡ä»¶ï¼Œè‡ªå·±æ¯”è¾ƒå–œæ¬¢å¤„ç†csvã€tsvæ–‡ä»¶ä¸¤è€…æ˜¯ä¸€æ ·çš„ï¼Œjsonæ–‡ä»¶è¿˜æ²¡æ€ä¹ˆæ¥è§¦ã€‚æ•°æ®çš„å†…å®¹å¦‚ä¸Šæ‰€ç¤ºï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ—å’Œæ ‡ç­¾åˆ—ï¼Œä¸€èˆ¬ä¼šåˆ†ä¸ºtrainã€devã€testä¸‰ä¸ªé›†åˆã€‚æœ¬æ–‡ç”¨åˆ°train.tsvã€dev.tsvã€test.tsvã€‚ Field æˆ‘ä¹Ÿä¸çŸ¥é“è¿™ä¸ªè¯¥å«å•¥ï¼Œé’ˆå¯¹æˆ‘ä»¬è¦å¤„ç†çš„ä¸åŒæ•°æ®åˆ†åˆ«åˆ›å»ºFieldå¯¹è±¡ã€‚ import torchtext.data as data import jieba def tokenizer(x): return list(jieba.cut(x)) text_field=data.Field(tokenize=tokenizer,fix_length=60) label_field=data.LabelField(dtype=torch.long) é’ˆå¯¹æˆ‘ä»¬éœ€è¦å¤„ç†çš„textåˆ—å’Œlabelåˆ—ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªFieldå¯¹è±¡ï¼Œåˆ†åˆ«ç”¨æ¥å¤„ç†å¯¹åº”åˆ—ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹textè¿›è¡Œåˆ†è¯ï¼Œæ‰€ä»¥éœ€è¦æŒ‡å®šåˆ†è¯å™¨ï¼Œæˆ‘è¿™é‡Œç”¨çš„ç»“å·´ã€‚åˆ†è¯å™¨çš„è¿”å›ç»“æœéœ€è¦æ˜¯å…³äºè¯è¯­çš„åˆ—è¡¨ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚æŒ‡å®šã€‚fix_lengthæŒ‡å®šå•ä¸ªåºåˆ—çš„é•¿åº¦ï¼Œtorchtextä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬è¿›è¡Œpaddingå’Œè£å‰ªæ“ä½œã€‚labelåˆ—æ˜¯åˆ†ç±»çš„ç±»åˆ«åˆ—ï¼Œtorchtextç›¸åº”æä¾›äº†LabelFieldï¼Œéœ€è¦æŒ‡å®šæ•°æ®ç±»å‹ã€‚çœ‹åˆ«äººçš„ä»£ç ï¼Œä½¿ç”¨Fieldä¹Ÿèƒ½ç”¨ï¼Œè‡ªå·±è¿˜æ²¡å®éªŒè¿‡ã€‚ Fieldä¸­è¿˜æœ‰å…¶ä»–å‚æ•° def generate_bigrams(x): n_grams = set(zip(*[x[i:] for i in range(2)])) for n_gram in n_grams: x.append(' '.join(n_gram)) return x text_field=data.Field(tokenize=tokenizer,fix_length=60,include_lengths=True, preprocessing=generate_bigrams) include_lengths,è¿”å›çš„minibatchä¸­åŒ…å«paddedåçš„textå’Œåºåˆ—é•¿åº¦ï¼Œåºåˆ—é•¿åº¦å¯¹äºæˆ‘ä»¬å¤„ç†å˜é•¿åºåˆ—æœ‰ç”¨ã€‚è¿™æ ·æ“ä½œåï¼Œåœ¨åé¢çš„æ¯ä¸ªbatchä¸­ï¼Œbatch.textè¿”å›çš„å°±æ˜¯ä¸€ä¸ªï¼ˆtextï¼Œtext_lengthï¼‰çš„å…ƒç»„ã€‚ preprocessing,æ·»åŠ é¢„å¤„ç†å‡½æ•°ï¼Œtokenizeåçš„ç»“æœè¿”å›çš„æ˜¯åˆ—è¡¨ç±»å‹æ•°æ®ï¼Œå°†è¿™ä¸ªåˆ—è¡¨æ•°æ®å†é€šè¿‡preprocessingæŒ‡å®šçš„å‡½æ•°è¿›è¡Œå¤„ç†ã€‚ TabularDataset é’ˆå¯¹æˆ‘ä»¬éœ€è¦å¤„ç†çš„è¡¨æ ¼æ•°æ®ï¼Œtorchæä¾›äº†TabularDatasetå¸®åŠ©æˆ‘ä»¬æŒ‰è¡Œè¿›è¡Œåˆ’åˆ†ï¼ŒæŒ‰ç…§è¡¨æ ¼å¯¹åº”åˆ—çš„é¡ºåºæŒ‡å®šFieldå¯¹è±¡æ„æˆå…ƒç»„fields=[('text', text_field)ï¼Œ('label', label_field), ] ,labelå’Œtextæ˜¯åˆ«åï¼Œå¯ä»¥ä»»æ„ï¼Œæˆ‘ä»¬åé¢å¯ä»¥é€šè¿‡è¿™ä¸ªåˆ«åè·å–batchçš„æ•°æ®ã€‚é™¤æ­¤ä¹‹å¤–è¿˜è¦æŒ‡å®šæ•°æ®çš„è·¯å¾„ï¼Œæ ¼å¼ï¼Œä»¥åŠæ˜¯å¦è·³è¿‡è¡¨å¤´ç­‰è¶…å‚æ•°ã€‚ train,dev,test=data.TabularDataset.splits(path,train='trian.tsv',validation='dev.tsv',test='test.tsv',skip_header=True,fields=[('label',label_field),('text',text_field)],format='tsv') ç”Ÿæˆçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ æ„å»ºæ˜ å°„å’Œå¯¼å…¥è¯å‘é‡ å‰é¢å°†æ•°æ®æŒ‰è¡Œè¿›è¡Œåˆ’åˆ†äº†ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰å»ºç«‹è¯å’Œç´¢å¼•çš„å¯¹åº”å…³ç³»ï¼Œä¹Ÿæ²¡æœ‰å»ºç«‹ç´¢å¼•å’Œè¯å‘é‡çš„å¯¹åº”å…³ç³»ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯å»ºç«‹è¯å’Œç´¢å¼•å’Œå‘é‡çš„å…³ç³» import torchtext.vocab as vocab vectors=vocab.Vectors('embedding_path','cachepath') text_field.build_vocab(train,dev,test,vectors,unk_init=torch.Tensor.normal_) label_field.build_vocab(train,dev,test) torchtext.vocabæä¾›äº†ä¸€ä¸ªç¼“å­˜è¯å‘é‡çš„æ–¹æ³•ï¼Œåœ¨ç¬¬ä¸€æ¬¡è¯»å–è¯å‘é‡æ—¶å°†è¯å‘é‡ç¼“å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œä¸‹æ¬¡å†è¯»å–è¯å‘é‡æ—¶ç›´æ¥ä»ç¼“å­˜ç›®å½•ä¸­è¯»å–ï¼Œè¿™æ ·å°±ä¸ç”¨æ¯æ¬¡åŠ è½½è¯å‘é‡äº†ï¼ŒåŠ å¿«äº†è¯»å–é€Ÿåº¦ã€‚æœªçŸ¥è¯å’Œpaddingè¯è¯è¯­å’Œç´¢å¼•å¯¹åº”å…³ç³»ä¸ºï¼š{:0,:1}ã€‚å…¶ç´¢å¼•å¯ä»¥é€šè¿‡å¦‚ä¸‹ æ–¹å¼è·å–ï¼š PAD_IDX=text_field.vocab.stoi[text_field.pad_token] UNK_IDX=text_field.vocab.stoi[text_field.unk_token] æœªçŸ¥è¯å’Œpaddingè¯é€šå¸¸å¯¹æˆ‘ä»¬çš„ä»»åŠ¡æ²¡æœ‰å¸®åŠ©ï¼Œä½†unk_init=torch.Tensor.normal_å´æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–äº†ï¼Œè·å–äº†è¿™ä¸¤ä¸ªè¯çš„ç´¢å¼•å¯ä»¥å¸®åŠ©æˆ‘ä»¬åœ¨åé¢çš„embeddingå±‚å°†å…¶è¯å‘é‡é‡ç½®ä¸º0 å»ºç«‹textä¸­è¯å’Œç´¢å¼•ã€è¯å‘é‡çš„æ˜ å°„å…³ç³»æ—¶ï¼Œéœ€è¦è¾“å…¥è¦å¤„ç†çš„æ•°æ®ã€è¯å‘é‡ï¼ŒæŒ‡å®šæœªè¯†åˆ«è¯çš„åˆå§‹åŒ–æ–¹å¼ã€‚ å»ºç«‹labelä¸­ç±»åˆ«å’Œç´¢å¼•çš„å…³ç³»å°±æ¯”è¾ƒç®€å•äº†ï¼Œè¾“å…¥ä¹‹å‰ç”Ÿæˆçš„æ•°æ®å°±å¥½äº†ã€‚çœ‹ä¸€çœ¼ç”Ÿæˆçš„è¯è¡¨å’Œè¯å‘é‡ ç”Ÿæˆbatch å‰é¢æˆ‘ä»¬å·²ç»å°†æ¯è¡Œæ•°æ®è¿›è¡Œäº†åˆ†è¯ï¼Œå»ºç«‹äº†è¯è¡¨ã€ç´¢å¼•ç­‰å¯¹åº”å…³ç³»ï¼Œæ¥ä¸‹æ¥å°±æ˜¯ç”Ÿæˆbatchå•¦ train_iter,dev_iter,test_iter=data.BucketIterator.splits( (train,dev,test),batch_sizes=(64,len(dev),len(test)), sort_key=lambda x:len(x.text),sort_within_batch,repeat=False, shuffle=True,device=device ) é¦–å…ˆæŒ‡å®šè¦å¤„ç†çš„æ•°æ®ï¼ˆtrainï¼Œdevï¼Œtestï¼‰ï¼Œæ¥ç€æŒ‡å®šå„ä¸ªæ•°æ®å¯¹åº”è¦ç”Ÿæˆ çš„batchçš„batch_sizeå¤§å°ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å¤šä¸ªæ•°æ®é›†çš„batch_size,å‚æ•°æ˜¯batch_sizes=ï¼ˆ64ï¼Œlenï¼ˆdev),len(test)ï¼‰,å¯ä»¥çœ‹åˆ°è¿™é‡Œå¯¹devå’Œtestæˆ‘ä»¬çš„batchsizeå¤§å°å°±æ˜¯ä»–ä»¬æ‰€æœ‰æ•°æ®ï¼Œæ‰€ä»¥è¿™äº›æ•°æ®åªä¼šç”Ÿæˆä¸€ä¸ªbatchã€‚é€šå¸¸å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®è¦æŒ‰æ•°æ®é•¿åº¦è¿›è¡Œæ’åºï¼Œæˆ‘ä»¬è¦æŒ‡å®šæ’åºçš„é”®sort_keyï¼Œè¿™é‡Œä½¿ç”¨äº†åŒ¿åå‡½æ•°ï¼Œtextå°±æ˜¯æˆ‘ä»¬ä¹‹å‰æŒ‡å®šçš„åˆ«åã€‚è¿˜åŒ…æ‹¬å…¶ä»–ä¸€äº›å‚æ•°ï¼Œä¸ä¸€ä¸€ä»‹ç»ã€‚è‡³æ­¤æˆ‘ä»¬å°±ç”Ÿæˆäº†textå’Œlabelçš„batché›†åˆï¼Œä¸¤è€…æ˜¯åˆåœ¨ä¸€èµ·çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡batchçš„åˆ«åè·å¾—å„è‡ªå€¼ã€‚sort_within_batch,åœ¨ä¸€ä¸ªbatchå†…å¯¹æ•°æ®è¿›è¡Œæ’åºã€‚ embedding å¦‚ä½•è·å¾—è¯è¡¨å¯¹åº”çš„embeddingå‘¢ï¼Ÿ pretrained_embeddings=text_field.vocab.vectors å¯ä»¥å’Œtorchçš„embeddingæ¨¡å—ç»“åˆä½¿ç”¨ embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False) embedded=embedding(batch.text) embeddingçš„è¾“å…¥éœ€è¦æ˜¯ä¹‹å‰å¤„ç†è¿‡çš„batchæ•°æ®ï¼Œtorchtextä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬å¤„ç†å¥½å¯¹åº”å…³ç³»ã€‚ é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å¤„ç†è¾¹é•¿åºåˆ—å‘¢ï¼Ÿé€šå¸¸ä¸ºäº†æ•°æ®çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬å¯¹è¾ƒçŸ­çš„æ•°æ®ä¼šè¿›è¡Œpaddingï¼Œè®©æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚å½“ä½¿ç”¨RNNç­‰ç½‘ç»œæ—¶ï¼Œç»è¿‡paddingçš„æ•°æ®äº§ç”Ÿçš„hidden stateã€outputså¯¹äºæˆ‘ä»¬å¹¶æ— å¸®åŠ©ï¼Œæˆ‘ä»¬ä¸»éœ€è¦çœŸæ˜¯åºåˆ—é•¿äº§ç”Ÿçš„hidden stateå’Œoutputså³å¯ã€‚torchtextæä¾›äº†pack_padded_sequenceå’Œpad_packed_sequenceä¸¤ä¸ªæ–¹æ³•ï¼Œå¯¹embeddedçš„ç»“æœè¿›è¡Œå¤„ç†åŠ å…¥åºåˆ—é•¿åº¦ã€‚ embedded=self.embedding(text) #æ·»åŠ äº†paddingçš„embeddingç»“æœ packed_embedded=nn.utils.pack_padded_embedding(embedded,text_lengths) packed_output,(hidden,cell)=self.rnn(packed_output) #æ²¡æœ‰paddingçš„è¾“å‡º output,output_lengths=nn.utils.rnn.pad_packed_sequence(packed_output) #è¿”å›è¾“å‡ºï¼Œå’Œæ²¡æœ‰paddingçš„å¥å­é•¿åº¦ å¯¹äºunkå’Œpadè¯æˆ‘ä»¬å¯ä»¥å°†å…¶è¯å‘é‡åˆå§‹åŒ–ä¸º0å‘é‡ model.embedding.weight.data[UNK_IDX]=torch.zeros(EMBEDDING_DIM) model.embedding.weight.data[PAD_IDX]=torch.zeros(EMBEDDING_DIM) å…¶å®åœ¨è°ƒç”¨field.build_coabæ—¶ä¼šè‡ªåŠ¨å°†padå’Œunkåˆå§‹åŒ–ä¸º0 è¿˜å¯ä»¥æ¢ç§æ–¹å¼æŒ‡å®šembedding embedding=nn.Embedding(vocab_size,embedding_dim,pad_idx=pad_idx) å…ˆæŒ‡å®šå¥½embeddingçš„å½¢çŠ¶ï¼Œå†åœ¨æ¨¡å‹æ­å»ºå¥½åæŒ‡å®šembedding embedding=model.embedding.weight.data.copy_(pretrained_embeddings) æ”¶å·¥~","categories":[],"tags":[{"name":"torchtext","slug":"torchtext","permalink":"https://mz2sj.github.io/tags/torchtext/"}]},{"title":"TextCNN","slug":"TextCNN","date":"2020-01-07T11:50:28.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/07/TextCNN/","link":"","permalink":"https://mz2sj.github.io/2020/01/07/TextCNN/","excerpt":"","text":"å­¦äº†ç‚¹pytorchï¼Œæœ€è¿‘åœ¨çŸ¥ä¹åˆ·åˆ°äº†ä¸€ä¸ªrepoä¸»è¦æ˜¯ä»‹ç»æ–‡æœ¬åˆ†ç±»çš„ï¼Œæ„Ÿè§‰è¿˜ä¸é”™ï¼Œæ‰“ç®—ç”¨æ¥å…¥é—¨ï¼Œè‡ªå·±å°†ä¼šæ¯”è¾ƒç»†è‡´çš„å®ç°ä»£ç ï¼Œåˆ†æä»£ç çš„ç»“æ„ã€‚å¤©ä¸‹å¤§äº‹å¿…åšäºæ˜“ï¼Œå¤©ä¸‹å¤§äº‹å¿…åšäºç»†ã€‚å¯’å‡æ¥äº†ï¼Œè‡ªå·±åŸºç¡€è¾ƒå·®ï¼Œåªèƒ½å¤šèŠ±ç‚¹æ—¶é—´äº†ã€‚ åŸºç¡€ç½‘ç»œ ä½œè€…è‡ªå·±æ²¡æœ‰ç›´æ¥è°ƒç”¨torchçš„apiï¼Œè€Œæ˜¯åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œå®ç°ã€‚ä¸€äº›æ¨¡å—ç»§æ‰¿è‡ªnn.Moduleå±‚ï¼Œå¹¶ä¸”å®ç°äº†åˆå§‹åŒ–å’Œå‰å‘è®¡ç®—ã€‚å˜¿å‘€ï¼Œç­‰æˆ‘ç†Ÿç»ƒäº†ï¼Œæˆ‘ä¹Ÿè¦è‡ªå·±åšä¸€ä¸ªå·¥å…·ç®±ã€‚ä¸å¤šè¯´ä¸Šä»£ç  Conv1d import torch.nn.Functional as F class Conv1d(nn.Module): def __init__(self,in_channels,out_channels,filter_sizes): super(Conv1d,self).__init__() self.convs=nn.ModuleList([ nn.Conv1d(in_channels,out_channels,kernel_size=fs) for fs in filter_sizes ]) self.init_params() def init_params(self): for m in self.convs: nn.init.xavier_uniform_(m.weight.data) nn.init.constant_(m.bias.data,0.1) def forward(self,x): return [F.relu(conv(x))for conv in self.convs] è®©æˆ‘ä»¬æ¥ç»†ç»†å“å‘³è¿™æ®µä»£ç ï¼Œç»§æ‰¿è‡ª nn.Module è¿™ä¸ç”¨è¯´äº†ï¼Œè¡¨æ˜è¿™ä¸ªæ¨¡å—å¯ä»¥å’Œtorchè‡ªå¸¦çš„æ¨¡å—æ··ç”¨ã€‚__init__å‡½æ•°çš„å‚æ•°æœ‰è®²ç©¶ï¼Œé™¤äº†å¿…ä¼ çš„selfå¤–ï¼Œè¿˜ä¼ å…¥äº†ä¸€äº›æŒ‡å®šå·ç§¯å±‚å½¢çŠ¶çš„å‚æ•°ï¼Œä¾‹å¦‚é€šé“æ•°ï¼šin_channelsã€out_channels,è¿˜æœ‰å·ç§¯æ ¸é›†åˆã€‚è¿™é‡Œä¼ å…¥çš„æ˜¯å·ç§¯æ ¸é›†åˆï¼Œè€Œä¸æ˜¯å•å•ä¸€ä¸ªå·ç§¯æ ¸éœ€è¦æ³¨æ„ã€‚æ¥ä¸‹æ¥æ˜¯ç»§æ‰¿è‡ªçˆ¶ç±»çš„åˆå§‹åŒ–ï¼Œsuper()ç¬¬ä¸€ä¸ªå‚æ•°ä¸€èˆ¬ä¸ºå‡½æ•°åï¼Œç¬¬äºŒä¸ªæ˜¯self,åé¢å†æ¥__init__()ã€‚åœ¨__init__()å‡½æ•°ä¸­æ”¾ç½®è¦ç”¨åˆ°çš„æ¨¡å—æˆ–å‚æ•°ç­‰ï¼Œä¸æ¶‰åŠåˆ°å…·ä½“çš„æ“ä½œï¼Œåœ¨åˆå§‹åŒ–å±‚è°ƒç”¨äº†åé¢å®šä¹‰çš„å‚æ•° åˆå§‹åŒ–å‡½æ•°ã€‚ ç¬¬äºŒéƒ¨åˆ†æ˜¯åˆå§‹åŒ–å±‚ï¼Œinit_params()ã€‚åˆå§‹åŒ–å±‚æ¯”è¾ƒå¥½ç†è§£ï¼Œåˆå§‹åŒ–initå±‚çš„å‚æ•°å°±å¥½ã€‚å‰é¢è¦åˆå§‹åŒ–çš„åªæœ‰ModuleListä¸­çš„å·ç§¯å±‚ï¼Œå·ç§¯çš„å‡½æ•°æ”¹å¤©ä¸“é—¨æ€»ç»“ä¸€ä¸‹ã€‚å·ç§¯å±‚å‚æ•°åˆ†ä¸ºweightå’Œbiasä¸¤éƒ¨åˆ†,biaséƒ¨åˆ†åˆå§‹åŒ–ä¸º0ã€‚ ç¬¬ä¸‰éƒ¨åˆ†æ˜¯å‰å‘ä¼ æ’­ï¼Œforward()ã€‚å‚æ•°æœ‰è¾“å…¥xï¼Œç”±äºæœ‰å¤šä¸ªå·ç§¯æ ¸ï¼Œå°†å¤šä¸ªå·ç§¯æ ¸çš„å·ç§¯ç»“æœæ”¾åˆ°åˆ—è¡¨ä¸­ã€‚å†è°ƒç”¨F.relu()activationã€‚ä¸å¾—ä¸è¯´torch.nn.Functional è¶…å¥½ç”¨ã€‚ Linear Linearå±‚å’ŒConv1dç»“æ„ä¸Šå·®ä¸å¤šï¼Œä¸è¿‡Linearå±‚çš„åˆå§‹åŒ–å‚æ•°å°†å·ç§¯å±‚çš„channelæ¢æˆäº†featureï¼Œå¹¶ä¸”å°‘äº†å·ç§¯æ ¸ã€‚ä»£ç å¦‚ä¸‹ï¼š class Linear(nn.Module): def __init__(self,in_features,out_features): super(Linear,self).__init__() self.linear=nn.Linear(in_features=in_features,out_features=out_features) self.init_params() def init_parmas(self): nn.init.kaiming_normal_(self.linear.weight) nn.init.constant_(self,linear.bias,0) def forward(self,x): x=self.linear(x) return x TextCNN åˆ°äº†æœ€æœ€å…³é”®çš„ç¯èŠ‚äº†ï¼ŒTextCNNå±‚: class TextCNN(nn.Module): def __init__(self,embedding_dim,n_filters,filter_sizes,output_dim,dropout, pretrained_embeddings): super(TextCNN,self).__init__() #å¯¼å…¥é¢„è®­ç»ƒçš„embedding frezeeæŒ‡å®šæƒé‡æ˜¯å¦æ›´æ–° self.embedding=nn.Embedding.from_pretrained(pretrained_embeddings,freeze=False) #å·ç§¯æ ¸å‚æ•° embeddingç»´æ•°ï¼Œå·ç§¯æ ¸ä¸ªæ•°ï¼Œå·ç§¯æ ¸å¤§å° self.convs=Cov1d(embedding_dim,n_filters,filter_sizes) #å‰å‘è¿ç®— è¾“å…¥ç»´åº¦ é¢„æµ‹è¾“å‡º self.fc=Linear(len(filter_sizes)*fliters,output_dim) self.dropout=nn.Dropout(dropout) def forward(self,x): #text:[sent_len,batch_size] ä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯è¡Œæ•°åˆ—æ•° text,_=x #ç»´åº¦æ¢ä½ [batch_size,sent_len] text=text.permute(1,0) # [batch_size,sent_len,embedding_len] embedded=self.embedding(text) #[batch_size,emb_dim,sent_len] embedded=embedded.permute(0,2,1) conved=self.convs(embedded) ##[batch_size,n_filters,sent_len-filter_sizes[n]+1] pooled=[F.max_pool1d(conv,conv.shape[2]) for conv in conved] cat=self.dropout(torch.cat(pooled,dim=1)) cat=cat.reshae((cat.shape[0],-1)) return self.fc(cat) TextCNNå±‚ä¸­çš„å¼ é‡ç»´åº¦å˜æ¢æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘æ˜¯ä¸€ä¸‹å­ååº”ä¸è¿‡æ¥çš„ï¼Œåé¢è‡ªå·±ä¼šdebugå½»åº•å¼„æ¸…æ¥šæ¯ä¸€æ­¥æ˜¯æ€ä¹ˆå·ç§¯çš„ï¼Œç»´åº¦æ€ä¹ˆå˜æ¢çš„ã€‚ argparse ä»£ç è¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒå€¼å¾—å€Ÿé‰´çš„ç‚¹å°±æ˜¯ä½¿ç”¨äº†argparseæ¨¡å—å¯¹å„ç§å‚æ•°è¿›è¡Œç®¡ç†ï¼Œä¸æ˜¯ä¸€ä¸ªä¸ªæ‰‹åŠ¨å»å®šä¹‰å˜é‡ï¼Œæ›´åŠ æ¸…æ™°ã€‚ import argparse def get_args(data_dir, cache_dir, embedding_folder, model_dir, log_dir): parser = argparse.ArgumentParser(description='SST') # data_util parser.add_argument('--model_name', default='TextCNN', type=str, help='å‚æ•°æ‰€å±æ¨¡å‹å') parser.add_argument('--seed', default=47, type=int, help='éšæœºç§å­') parser.add_argument('--data_path', default=data_dir, type=str, help='SST2æ•°æ®é›†ä½ç½®') parser.add_argument('--cache_path', default=cache_dir, type=str, help='æ•°æ®é›†åœ°å€') parser.add_argument('--sequence_length', default=60, type=int, help='å¥å­é•¿åº¦') # è¾“å‡ºæ–‡ä»¶å parser.add_argument('--model_dir', default=model_dir + 'TextCNN/', type=str, help='è¾“å‡ºæ¨¡å‹åœ°å€') parser.add_argument('--log_dir', default=log_dir + 'TextCNN/', type=str, help='æ—¥å¿—æ–‡ä»¶åœ°å€') parser.add_argument('--do_train', action='store_true', help='Whether to run training') parser.add_argument('--print_step', default=100, type=int, help='å¤šå°‘æ­¥å­˜å‚¨ä¸€æ¬¡æ¨¡å‹') # ä¼˜åŒ–å‚æ•° parser.add_argument('--batch_size', default=64, type=int) parser.add_argument('--epoch_num', default=5, type=int) parser.add_argument('--dropout', default=0.4, type=float) # æ¨¡å‹å‚æ•° parser.add_argument('--output_dim', default=2, type=int) # TextCNNå‚æ•° parser.add_argument('--filter_num', default=200, type=int, help='filteræ•°é‡') parser.add_argument('--filter_sizes', default='1 2 3 4 5', type=str, help='filterçš„size') # word embdding parser.add_argument('--glove_word_file', default=embedding_folder + 'glove.6B.50d.txt', type=str, help='path of embedding file') parser.add_argument('--glove_words_size', default=int(2.2e6), type=int, help='Corpus size for Glove') parser.add_argument('--glove_word_dim', default=50, type=int, help='word embedding size(default:300)') config = parser.parse_args([]) return config æ³¨æ„çš„ä¸€ç‚¹å°±æ˜¯parse_args()åœ¨notebookä¸­éœ€è¦åŠ å…¥[]ä½œä¸ºå‚æ•°ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚ device æŒ‡å®šè®¾å¤‡deviceï¼Œè¿™ä¸ªå¾ˆå®¹æ˜“ç†è§£ def get_device(): device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') n_gpu=torch.cuda.device_count() if torch.cuda.is_available(): print('device is cuda,# cuda is:',n_gpu) else: print('device is cpu,not recommend') return device,n_gpu ç»Ÿè®¡gpuæ•°ä»£ç ï¼štorch.cuda.device_count() æŒ‡å®šdeviceä»£ç ï¼štorch.device('cuda:0') åŠ è½½æ•°æ®é›† å¯¹apiè¿˜ä¸å¤ªç†Ÿæ‚‰ï¼Œè¿™é‡Œä¹Ÿæ¯”è¾ƒå¤æ‚ã€‚ def load_sst2(path,text_field,label_field,batch_size,device,embedding_file,cache_dir): train,dev,test=data.TabularDataset.splits( path=path,train='train.tsv',validationj='dev.tsv', test='test.tsv',format='tsv',skip_header=True, fields=[('text',text_field),('label',label_field)] ) print('the size of train:{},dev:{},test:{}'.format( len(train.examples), len(dev.examples), len(test.examples) )) #å°†è¯å‘é‡æ”¾åˆ°ç¼“å­˜ç›®å½•ä¸­ï¼Œä¸‹æ¬¡ç›´æ¥ä»æ¢ä»ç›®å½•ä¸­åŠ è½½ vectors=vocab.Vectors(embedding_file,cache_dir) #ä½¿ç”¨ä¹‹å‰æ„å»ºçš„è¯å‘é‡æ˜ å°„æ–‡æœ¬åºåˆ—ï¼Œå°†å­åºåˆ—è½¬åŒ–ä¸ºæ•°å­—åºåˆ— text_field.build_vocab( train,dev,test,max_size=25000, vectors=vectors,unk_init=torch.Tensor.normal_ ) label_field.build_vocab(train,dev,test) train_iter,dev_iter,test_iter=data.BucketIterator.splits( (train,dev,test),batch_sizes=(batch_size,len(dev),len(test)), sort_key=lambda x:len(x.text),sort_within_batch=True,repeat=False, shuffle=True,device=device ) return train_iter,dev_iter,test_iter å¾—æ€»ç»“ä¸€ä¸‹å¥—è·¯ï¼Œçœ‹èµ·æ¥æ‡µæ‡µçš„ è®­ç»ƒ def train(epoch_num,model,train_dataloader,dev_dataloader,optimizer,criterion, label_list,out_model_file,log_dir,print_step,data_type='word'): #é¦–å…ˆè½¬ä¸ºè®­ç»ƒæ¨¡å¼ model.train() writer=SummaryWriter(log_dir=log_dir+time.strftime('%H%M%S',time.gmtime())) global_step=0 best_dev_loss=float('inf') for epoch in range(int(epoch_num)): print(f'--------------Epoch:{epoch+1:02}--------------') epoch_loss=0 train_step=0 all_preds=np.array([],dtype=int) all_labels=np.array([],dtype=int) for step,batch in enumerate(tqdm(train_dataloader,desc='Iteration')): #é¦–å…ˆæ¢¯åº¦å½’é›¶ optimzier.zero_grad() logits=model(batch.text) #è®¡ç®—æŸå¤± loss=criterion(logits.view(-1,len(label_list)),batch.label) labels=batch.label.detach().cpu().numpy() preds=np.argmax(logits.detach().cpu().numpy(),axis=1) #è®¡ç®—æ¢¯åº¦ loss.backward() optimizer.step() global_step+=1 epoch_loss+=loss.item() train_steps+=1 all_preds=np.append(all_preds,preds) all_labels=np.append(all_labels,labels) #æ‰“å° if global_step%print_step==0: train_loss=epoch_loss/train_steps train_acc,train_report=classification_metric( all_preds,all_labels,label_list) dev_loss,dev_acc,dev_report=evaluate( model,dev_dataloader,criterion,label_list,data_type ) c=global_step/print_step writer.add_scalar('loss/train',train_loss,c) writer.add_scalar('loss/dev',dev_loss,c) writer.add_scalar('acc/train',train_acc,c) writer.add_scalar('acc/dev',dev_acc,c) for label in label_list: writer.add_scalar(label+':'+'f1/train', train_report[label]['f1-score'],c) writer.add_scalar(label+':'+'f1/dev', dev_report[label]['f1-score'],c) print_list=['macro avg','weighted avg'] for label in print_list: writer.add_scalar(label+':'+'f1/train', train_report[label]['f1-score'],c) writer.add_scalar(label+':'+'f1/dev', dev_report[label]['f1-score'],c) if dev_loss&lt;best_dev_loss: best_dev_loss=dev_loss torch.save(model.state_dict(),out_model_file) model.train() writer.close() è®­ç»ƒå‡½æ•°æœ‰å‡ ä¸ªç‚¹æ€»ç»“ï¼Œä¼ å…¥å‚æ•°æœ‰ï¼šè¿­ä»£æ¬¡æ•°ã€modelã€è®­ç»ƒé›†ã€éªŒè¯é›†ï¼Œoptimzerã€æŸå¤±å‡½æ•°ã€æ‰“å°çš„epochæ•°ï¼Œç­‰ç­‰ã€‚ evaluate ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹è®­ç»ƒç»“æœè¿›è¡Œæµ‹è¯• def evaluate(model,iterator,criterion,label_list,data_type='word'): model.eval() epoch_loss=0 all_preds=np.array([],dtype=int) all_labels=np.array([],dtype=int) with torch.no_grad(): for batch in iterator: if data_type=='word': with torch.no_grad(): logits=model(batch.text) elif data_type=='highway': with torch.no_grad(): logits=model(batch.text_word,batch.text_char) loss=criterion(logits.view(-1,len(label_list)),batch.label) labels=batch.label.detach().cpu().numpy() preds=np.argmax(logits.detach().cpu().numpy(),axis=1) all_preds=np.append(all_preds,preds) all_labels=np.append(all_labels,labels) epoch_loss+=loss.item() acc,report=classification_metric(all_preds,all_labels,label_list) return epoch_loss/len(iterator),acc,report æµ‹è¯•è¿‡ç¨‹ä¸­è®°å¾—è¦åˆ‡æ¢åˆ°evalæ¨¡å¼ï¼Œå¹¶ä¸”å…³é—­æ¢¯åº¦è¿½è¸ªwith torch.no_grad()ã€‚ mainå‡½æ•° def main(config): if not os.path.exists(config.model_dir): os.makedirs(config.model_dir) if not os.path.exists(config.log_dir): os.makedirs(config.log_dir) print('\\t \\t \\t the model name is {}'.format(config.model_name)) device, n_gpu = get_device() # æŒ‡å®štorch éšæœºç§å­ torch.manual_seed(config.seed) # æŒ‡å®šnumpy éšæœºç§å­ np.random.seed(config.seed) torch.manual_seed(config.seed) if n_gpu &gt; 0: # æŒ‡å®šcudaéšæœºç§å­ torch.cuda.manual_seed_all(config.seed) # CuDNNçš„å·ç§¯æ“ä½œå°±æ˜¯æ¯æ¬¡ä¸€æ ·,å®éªŒå¯é‡å¤ torch.backends.cudnn.deterministic = True # sst2 æ•°æ®å‡†å¤‡ text_field = data.Field(tokenize='toktok', lower=True, include_lengths=True, fix_length=config.sequence_length) label_field = data.LabelField(dtype=torch.long) train_iterator, dev_iterator, test_iterator = load_sst2(config.data_path, text_field, label_field, config.batch_size, device, config.glove_word_file, config.cache_path) # è¯å‘é‡å‡†å¤‡ è·å–è¯å‘é‡æŒ‰indexé¡ºåºæ’åº pretrained_embeddings = text_field.vocab.vectors model_file = config.model_dir + 'model1.pt' # æ¨¡å‹å‡†å¤‡ å·ç§¯æ ¸å¤§å° filter_sizes = [int(val) for val in config.filter_sizes.split()] model = TextCNN(config.glove_word_dim, config.filter_num, filter_sizes, config.output_dim, config.dropout, pretrained_embeddings) optimizer = torch.optim.Adam(model.parameters()) criterion = nn.CrossEntropyLoss() model = model.to(device) criterion = criterion.to(device) if not config.do_train: train(config.epoch_num, model, train_iterator, dev_iterator, optimizer, criterion, ['0', '1'], model_file, config.log_dir,config.print_step, 'word') model.load_state_dict(torch.load(model_file)) test_loss, test_acc, test_report = evaluate(model, test_iterator, criterion, ['0', '1'], 'word') print('-----------Test----------') print('\\t Loss:{}|Acc:{}|Macro avg F1{}|Weighted avg F1{}'.format( test_loss, test_acc, test_report['macro avg']['f1-score'], test_report['weighted avg']['f1-score'] )) å¤ªå¤šå•¦ï¼Œæ‡’å¾—å†è®°å½•é¸Ÿ~ä¸‹é¢åˆ†æä¸€ä¸‹å·ç§¯çš„è¿‡ç¨‹å§ã€‚ å·ç§¯çš„è¿‡ç¨‹ ä»£ç ä¸­ç”¨äº†5ä¸ªå·ç§¯æ ¸ï¼Œå·ç§¯æ ¸çš„å¤§å°åˆ†åˆ«æ˜¯[1,2,3,4,5]ã€‚ä¸€ç»´å·ç§¯å±‚çš„è¾“å…¥æ˜¯è¯å‘é‡çš„æ‹¼æ¥ï¼Œæœ¬ä»£ç ä¸­ä½¿ç”¨çš„è¯å‘é‡æ˜¯50ç»´ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰60ä¸ªè¯ï¼Œå› æ­¤å·ç§¯çš„è¾“å…¥æ˜¯50x60çš„çŸ©é˜µXXXã€‚ä¸€ç»´å·ç§¯æ ¸çš„è¾“å…¥é€šé“æ•°å°±æ˜¯XXXçš„å®½50ï¼Œè¾“å‡ºé€šé“æ•°æ˜¯å·ç§¯æ ¸çš„ä¸ªæ•°200ï¼Œåˆ™å·ç§¯æ ¸[1,2,3,4,5]å¯¹åº”çš„è¾“å‡ºåˆ†åˆ«ä¸º[200*x60,200x59,200x58,200x57,200x56]ã€‚å†åˆ†åˆ«å¯¹è¡Œè¿›è¡Œæœ€å¤§æ± åŒ–ï¼Œå¾—åˆ°çš„è¾“å‡ºä¸º5ä¸ª[200x1]çš„å‘é‡ï¼Œå°†è¿™5ä¸ªå‘é‡æ‹¼æ¥èµ·æ¥æˆ[1000x1]çš„å‘é‡ï¼Œè¾“å…¥å‰é¦ˆç¥ç»ç½‘ç»œ[1000x2]ï¼Œå³å¯è¿›è¡ŒäºŒåˆ†ç±»ã€‚è€å®è¯´ï¼Œä¸€ç»´å·ç§¯çš„è¿‡ç¨‹è‡ªå·±æ²¡å¤ªçœ‹æ‡‚ã€‚ä»£ç ä¸­å°†å·ç§¯æ ¸çš„ä¸ªæ•°ä½œä¸ºä¸€ç»´å·ç§¯çš„è¾“å‡ºé€šé“æ•°ï¼Œembeddingç»´åº¦ä½œä¸ºå·ç§¯çš„è¾“å…¥é€šé“æ•°ã€‚ä¸€ç›´ä»¥æ¥çœ‹åˆ°çš„éƒ½æ˜¯äºŒç»´å·ç§¯ï¼Œä¸€ç»´å·ç§¯æœ‰ç‚¹æ‡µï¼Œä¸‹é¢ç”»ä¸ªè‰å›¾ç†è§£ä¸‹å§ã€‚","categories":[],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://mz2sj.github.io/tags/CNN/"},{"name":"æ–‡æœ¬åˆ†ç±»","slug":"æ–‡æœ¬åˆ†ç±»","permalink":"https://mz2sj.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"}]},{"title":"colabä½¿ç”¨","slug":"colabä½¿ç”¨","date":"2020-01-07T05:26:04.000Z","updated":"2023-01-08T06:30:22.504Z","comments":true,"path":"2020/01/07/colabä½¿ç”¨/","link":"","permalink":"https://mz2sj.github.io/2020/01/07/colab%E4%BD%BF%E7%94%A8/","excerpt":"","text":"æœ€è¿‘å­¦ä¹ pytorchçš„ä½¿ç”¨ï¼Œè‡ªå·±çš„ç”µè„‘æ²¡æœ‰æ˜¾å¡ï¼Œä¸€äº›å®éªŒåšä¸äº†ï¼Œæƒ³èµ·ä¹‹å‰æœ¬ç§‘è€å¸ˆå®‰åˆ©çš„colabï¼Œæ„Ÿè°¢è°·æ­Œæä¾›çš„å…è´¹èµ„æºã€‚ è®¿é—®è°·æ­Œ colabåŸºäºè°·æ­Œäº‘ç›˜ï¼Œæ—¢ç„¶æ˜¯è°·æ­Œçš„æœåŠ¡ï¼Œéš¾å…å°±è¦ç¿»å¢™äº†ã€‚è‡ªå·±ç”¨çš„shadowsocksï¼Œä¸€å¹´100å—å·¦å³ï¼Œå¤§éƒ¨åˆ†æ—¶é—´éƒ½æŒºç¨³å®šçš„ã€‚å¼€å¯å°é£æœºï¼Œä½ å°±å¯ä»¥çœ‹åˆ°å¤–é¢çš„ä¸–ç•Œå•¦ï¼ äº‘ç›˜æŒ‚è½½ å…³äºcolabçš„åŸºç¡€ä½¿ç”¨è¿™é‡Œå°±ä¸ä»‹ç»äº†ï¼Œå¯ä»¥è‡ªè¡Œç™¾åº¦ã€‚åœ¨ä½¿ç”¨colabçš„è¿‡ç¨‹ä¸­ï¼Œç»å¸¸éœ€è¦æŒ‚è½½äº‘ç›˜æ–‡ä»¶ï¼Œå‚è€ƒäº†è®¸å¤šäººçš„æ–¹æ³•åï¼Œä»‹ç»ä¸€ä¸ªæ— å‘ç‰ˆã€‚é¦–å…ˆï¼Œåœ¨colabå·¦ä¾§çš„ä»£ç æç¤ºéƒ¨åˆ†é€‰æ‹©æŒ‚è½½colabä¼šè‡ªåŠ¨ç”Ÿæˆäº‘ç›˜æŒ‚è½½ä»£ç ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š from google.colab import drive drive.mount('/gdrive') è¿è¡Œç¬¬ä¸€ä¸ªå•å…ƒæ ¼çš„ä»£ç ï¼Œç¬¬äºŒä¸ªæ˜¯æ‰“å¼€ç›¸å…³æ–‡ä»¶çš„ä»£ç å¿½ç•¥ã€‚è¿è¡Œåä¼šç»™å‡ºä¸€ä¸ªè¶…é“¾æ¥ï¼Œç‚¹å‡»è¿›å»éªŒè¯å¾—åˆ°éªŒè¯ç è¾“å…¥å³å¯æŒ‚è½½æˆåŠŸï¼Œè¿™æ—¶å€™æˆ‘ä»¬æŒ‚è½½çš„ä½ç½®æ˜¯åœ¨/gdrive/gdrive/gdrive ç›®å½•ä¸‹ã€‚é‚£ä¹ˆå¦‚ä½•åˆ‡æ¢åˆ°æˆ‘ä»¬æƒ³è¦çš„ç›®å½•å‘¢ï¼Ÿå†æ¬¡æ‰“å¼€å·¦ä¾§çš„ä»£ç æç¤ºéƒ¨åˆ†ï¼Œè¿›å…¥æ–‡ä»¶æ ï¼Œç‚¹å‡» ......... è¿›å…¥ä¸Šå±‚ç›®å½•ï¼Œè¿›å…¥/gdrive/gdrive/gdriveç›®å½•ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬äº‘ç›˜ä¸‹çš„ç›®å½•æ–‡ä»¶ï¼Œå³å‡»å¤åˆ¶æ–‡ä»¶è·¯å¾„ã€‚ å†é€šè¿‡é­”æ³•å‡½æ•°%cd è·¯å¾„å°±%cd filepath å°±å¯ä»¥åˆ‡æ¢åˆ°æŒ‡å®šçš„è·¯å¾„ï¼Œæ­¤æ—¶colabçš„å·¥ä½œè·¯å¾„å°±æ˜¯ä½ æŒ‡å®šçš„è·¯å¾„ï¼Œä½ å°±å¯ä»¥åƒåœ¨æœ¬åœ°ä¸€æ ·æ“ä½œcolabçš„æ–‡ä»¶ã€‚æ“ä½œéå½“å‰æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶ï¼ŒæŒ‡å®šå¥½æ–‡ä»¶çš„è·¯å¾„å³å¯ã€‚ %cd /gdrive/My Drive/Colab Notebooks/æµ‹è¯•ç”¨çš„ æ— é™å®¹é‡ colabé»˜è®¤çš„å­˜å‚¨å®¹é‡æ˜¯15Gï¼Œå¯èƒ½æ”¾å‡ ä¸ªå¤§æ–‡ä»¶å°±å æ»¡äº†ï¼Œå‡çº§çš„è¯åˆä¸æ”¯æŒæ”¯ä»˜å®ï¼Œè€Œä¸”ä¸€å¹´ä¹Ÿè¦100å¤šå—ã€‚è´«ç©·ä½¿æˆ‘æƒ³èµ·äº†æ­ªé—¨é‚ªé“ã€‚åœ¨æ·˜å®ä¸Šæä¾›äº†colabæ‰©å®¹æœåŠ¡ï¼Œ20å—å°±å¤Ÿäº†ï¼Œå…¶åŸç†æ˜¯å°†ä½ åŠ å…¥ä¸€äº›å­¦æ ¡çš„å…±äº«ç›˜ç¾¤ç»„ï¼Œä½ å¯ä»¥åœ¨ä¸Šé¢æ— é™åˆ¶ä¸Šä¼ æ–‡ä»¶ã€‚å¯èƒ½ä¼šå­˜åœ¨ä¸€äº›å®‰å…¨é—®é¢˜ï¼Œä½†å¹³æ—¶è·‘è·‘ç¨‹åºæ˜¯å®Œå…¨æ²¡é—®é¢˜çš„ã€‚åŠ å…¥è´¡çŒ®ç›˜åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è‡ªå·±çš„gdriveæ–‡ä»¶å¤¹ä¸‹å¤šäº†ä¸€ä¸ªShared drivesã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸Šé¢è¿›è¡Œå’ŒåŸç¡¬ç›˜ç›¸åŒçš„æ“ä½œï¼Œå¹¶ä¸”æ— çº¿ç©ºé—´ã€‚å‰å®³å•¦ğŸ‘ çºªå¿µè‡ªå·±çš„ç¬¬ä¸€ç¯‡åšå®¢ æ—¢ç„¶å¼€äº†è¿™ä¸ªåšå®¢å°±è¦å¥½å¥½å†™å“¦ï¼Œè®°å½•è‡ªå·±çš„å­¦ä¹ ã€ç”Ÿæ´»ï¼ŒåŠªåŠ›æˆä¸ºæ›´å¥½çš„äººå•Šï¼ åŠ æ²¹ å†²","categories":[],"tags":[{"name":"colab Google Drive","slug":"colab-Google-Drive","permalink":"https://mz2sj.github.io/tags/colab-Google-Drive/"}]}]}